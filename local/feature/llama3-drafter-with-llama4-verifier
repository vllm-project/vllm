We need to support running the following eagle3 drafter with a llama4 verifier in vllm
- nm-testing/Llama4-Maverick-Eagle3-Speculators

Llama4 Verifier:
- "RedHatAI/Llama-4-Maverick-17B-128E-Instruct-quantized.w4a16"

The Drafter is an eagle3 head trained on top of llama4

It's config:

```json
{
  "architectures": [
    "Eagle3Speculator"
  ],
  "speculators_model_type": "eagle3",
  "speculators_version": "0.1.0.dev42",
  "draft_vocab_size": 202048,
  "norm_before_residual": true,
  "target_hidden_size": null,
  "transformer_layer_config": {
    "model_type": "llama",
    "vocab_size": 202048,
    "hidden_size": 5120,
    "intermediate_size": 32768,
    "num_hidden_layers": 1,
    "num_attention_heads": 40,
    "num_key_value_heads": 8,
    "head_dim": 128,
    "hidden_act": "silu",
    "max_position_embeddings": 1048576,
    "initializer_range": 0.02,
    "rms_norm_eps": 1e-05,
    "pretraining_tp": 1,
    "use_cache": true,
    "rope_theta": 500000.0,
    "rope_scaling": {
      "factor": 8.0,
      "high_freq_factor": 4.0,
      "low_freq_factor": 1.0,
      "original_max_position_embeddings": 8192,
      "rope_type": "llama3"
    },
    "attention_bias": false,
    "attention_dropout": 0.0,
    "mlp_bias": false,
    "tie_word_embeddings": false
  },
  "speculators_config": {
    "algorithm": "eagle3",
    "default_proposal_method": "greedy",
    "proposal_methods": [
      {
        "proposal_type": "greedy",
        "speculative_tokens": 3,
        "verifier_accept_k": 1,
        "accept_tolerance": 0.0
      }
    ],
    "verifier": {
      "name_or_path": "RedHatAI/Llama-4-Maverick-17B-128E-Instruct-quantized.w4a16",
      "architectures": [
        "Llama4ForConditionalGeneration"
      ]
    }
  },
  "torch_dtype": "bfloat16",
  "eagle_aux_hidden_state_layer_ids": [
    1,
    23,
    44
  ],
  "use_aux_hidden_state": true,
  "use_input_layernorm_in_first_layer": true,
  "use_mtp_layernorm": false,
  "eagle_config": {
    "eagle_aux_hidden_state_layer_ids": [
      1,
      23,
      44
    ],
    "use_aux_hidden_state": true,
    "use_input_layernorm_in_first_layer": true,
    "use_last_layernorm": true,
    "use_mtp_layernorm": false
  },
  "_comment": "Eagle3 head based on Llama3 architecture targeting Llama4 Maverick verifier",
  "_conversion_notes": {
    "source": "nvidia/Llama-4-Maverick-17B-128E-Eagle3",
    "architecture_notes": "Eagle3 head uses Llama3 rope_type, targets Llama4 verifier",
    "vocabulary_notes": "Large 202K vocabulary, same for draft and target",
    "auxiliary_layers": "Uses hidden states from verifier layers 1, 23, 44",
    "implementation_note": "May require Eagle3Speculator extensions for aux hidden states"
  }
}
```

A few new things special about this drafter:

- hidden states are taken from layers 1, 23, 44 instead of the default set in vllm

This is the first time we are running eagle3 speculative decoding method on multimodal models, (llama4)
so there might be bugs

We need to first add support to get these hidden states instead of default, then run the model with speculative decoding and fix any errors we come accross

Command to run:

```bash
CUDA_VISIBLE_DEVICES=6,7 vllm serve "nm-testing/Llama4-Maverick-Eagle3-Speculators" --tensor-parallel-size 2
```

venv to use: /home/rahul-tuli/vllm/.venv/bin/activate 
(This venv has vllm installed in editable mode)