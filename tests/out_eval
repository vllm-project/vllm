ENABLE_HMA_FLAG is set, appending ENABLE_HMA_FLAG=1 to each config
=== Running tests (default backend) ===
-> Running with ENABLE_HMA_FLAG=1 GPU_MEMORY_UTILIZATION=0.6 PREFILLER_TP_SIZE=2 DECODER_TP_SIZE=2 
Running accuracy tests with kv_buffer_device=cuda
HMA (Hybrid KV Cache Manager) enabled
================================
Testing model: Qwen/Qwen3-0.6B
================================
Starting prefill instance 0 on GPU 0,1, port 8100
Starting decode instance 0 on GPU 2,3, port 8200
Waiting for prefill instance on port 8100 to start...
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:314] 
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:314]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:314]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.2rc1.dev34+g62deffca0.d20260205
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:314]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen3-0.6B
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:314]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:314] 
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:15 [utils.py:250] non-default args: {'model_tag': 'Qwen/Qwen3-0.6B', 'api_server_count': 1, 'port': 8200, 'enforce_eager': True, 'tensor_parallel_size': 2, 'block_size': 128, 'gpu_memory_utilization': 0.6, 'disable_hybrid_kv_cache_manager': False, 'kv_transfer_config': KVTransferConfig(kv_connector='NixlConnector', engine_id='2390b1ed-9351-43eb-a05d-19c4de3adef5', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False, kv_load_failure_policy='recompute')}
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:314] 
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:314]        â–ˆ     â–ˆ     â–ˆâ–„   â–„â–ˆ
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:314]  â–„â–„ â–„â–ˆ â–ˆ     â–ˆ     â–ˆ â–€â–„â–€ â–ˆ  version 0.15.2rc1.dev34+g62deffca0.d20260205
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:314]   â–ˆâ–„â–ˆâ–€ â–ˆ     â–ˆ     â–ˆ     â–ˆ  model   Qwen/Qwen3-0.6B
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:314]    â–€â–€  â–€â–€â–€â–€â–€ â–€â–€â–€â–€â–€ â–€     â–€
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:314] 
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:15 [utils.py:250] non-default args: {'model_tag': 'Qwen/Qwen3-0.6B', 'api_server_count': 1, 'port': 8100, 'enforce_eager': True, 'tensor_parallel_size': 2, 'block_size': 128, 'gpu_memory_utilization': 0.6, 'disable_hybrid_kv_cache_manager': False, 'kv_transfer_config': KVTransferConfig(kv_connector='NixlConnector', engine_id='38dfb72a-cbc2-42f6-842c-66ac26a9e290', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False, kv_load_failure_policy='recompute')}
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:18 [model.py:529] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:18 [model.py:1544] Using max model len 40960
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:18 [model.py:529] Resolved architecture: Qwen3ForCausalLM
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:18 [model.py:1544] Using max model len 40960
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:18 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:18 [vllm.py:666] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2088919)[0;0m WARNING 02-06 10:41:18 [vllm.py:704] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=2088919)[0;0m INFO 02-06 10:41:18 [vllm.py:809] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:18 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:18 [vllm.py:666] Asynchronous scheduling is enabled.
[0;36m(APIServer pid=2088903)[0;0m WARNING 02-06 10:41:18 [vllm.py:704] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=2088903)[0;0m INFO 02-06 10:41:18 [vllm.py:809] Cudagraph is disabled under eager mode
[0;36m(EngineCore_DP0 pid=2089638)[0;0m INFO 02-06 10:41:28 [core.py:96] Initializing a V1 LLM engine (v0.15.2rc1.dev34+g62deffca0.d20260205) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-0.6B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=2089638)[0;0m WARNING 02-06 10:41:28 [multiproc_executor.py:910] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[0;36m(EngineCore_DP0 pid=2089634)[0;0m INFO 02-06 10:41:28 [core.py:96] Initializing a V1 LLM engine (v0.15.2rc1.dev34+g62deffca0.d20260205) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=Qwen/Qwen3-0.6B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'static_all_moe_layers': []}
[0;36m(EngineCore_DP0 pid=2089634)[0;0m WARNING 02-06 10:41:28 [multiproc_executor.py:910] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-06 10:41:40 [parallel_state.py:1234] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:42381 backend=nccl
INFO 02-06 10:41:40 [parallel_state.py:1234] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:35589 backend=nccl
INFO 02-06 10:41:40 [parallel_state.py:1234] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35589 backend=nccl
INFO 02-06 10:41:40 [parallel_state.py:1234] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:42381 backend=nccl
INFO 02-06 10:41:40 [pynccl.py:111] vLLM is using nccl==2.27.5
INFO 02-06 10:41:41 [pynccl.py:111] vLLM is using nccl==2.27.5
INFO 02-06 10:41:43 [parallel_state.py:1445] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
INFO 02-06 10:41:43 [parallel_state.py:1445] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
INFO 02-06 10:41:43 [parallel_state.py:1445] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
INFO 02-06 10:41:43 [parallel_state.py:1445] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:44 [gpu_model_runner.py:4119] Starting to load model Qwen/Qwen3-0.6B...
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:44 [gpu_model_runner.py:4119] Starting to load model Qwen/Qwen3-0.6B...
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:44 [cuda.py:367] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION'].
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:45 [cuda.py:367] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION'].
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:45 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:45 [default_loader.py:291] Loading weights took 0.17 seconds
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:45 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:46 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:46 [gpu_model_runner.py:4216] Model loading took 0.57 GiB memory and 1.603964 seconds
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:46 [weight_utils.py:567] No model.safetensors.index.json found in remote.
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:46 [default_loader.py:291] Loading weights took 0.20 seconds
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:47 [gpu_model_runner.py:4216] Model loading took 0.57 GiB memory and 2.198764 seconds
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [gpu_worker.py:360] Available KV cache memory: 40.84 GiB
[0;36m(EngineCore_DP0 pid=2089634)[0;0m INFO 02-06 10:41:49 [kv_cache_utils.py:1307] GPU KV cache size: 764,672 tokens
[0;36m(EngineCore_DP0 pid=2089634)[0;0m INFO 02-06 10:41:49 [kv_cache_utils.py:1312] Maximum concurrency for 40,960 tokens per request: 18.67x
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:105] Setting UCX_RCACHE_MAX_UNRELEASED to '1024' to avoid a rare memory leak in UCX when using NIXL.
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:105] Setting UCX_RCACHE_MAX_UNRELEASED to '1024' to avoid a rare memory leak in UCX when using NIXL.
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:118] NIXL is available
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:118] NIXL is available
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [factory.py:64] Creating v1 connector with name: NixlConnector and engine_id: 2390b1ed-9351-43eb-a05d-19c4de3adef5
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [factory.py:64] Creating v1 connector with name: NixlConnector and engine_id: 2390b1ed-9351-43eb-a05d-19c4de3adef5
[0;36m(Worker_TP0 pid=2090292)[0;0m WARNING 02-06 10:41:49 [base.py:166] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:928] Initializing NIXL wrapper
[0;36m(Worker_TP1 pid=2090293)[0;0m WARNING 02-06 10:41:49 [base.py:166] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:929] Initializing NIXL worker 2390b1ed-9351-43eb-a05d-19c4de3adef5
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:928] Initializing NIXL wrapper
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:929] Initializing NIXL worker 2390b1ed-9351-43eb-a05d-19c4de3adef5
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:49 [gpu_worker.py:360] Available KV cache memory: 40.84 GiB
[0;36m(EngineCore_DP0 pid=2089638)[0;0m INFO 02-06 10:41:49 [kv_cache_utils.py:1307] GPU KV cache size: 764,672 tokens
[0;36m(EngineCore_DP0 pid=2089638)[0;0m INFO 02-06 10:41:49 [kv_cache_utils.py:1312] Maximum concurrency for 40,960 tokens per request: 18.67x
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:105] Setting UCX_RCACHE_MAX_UNRELEASED to '1024' to avoid a rare memory leak in UCX when using NIXL.
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:105] Setting UCX_RCACHE_MAX_UNRELEASED to '1024' to avoid a rare memory leak in UCX when using NIXL.
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:118] NIXL is available
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:118] NIXL is available
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:49 [factory.py:64] Creating v1 connector with name: NixlConnector and engine_id: 38dfb72a-cbc2-42f6-842c-66ac26a9e290
[0;36m(Worker_TP0 pid=2090156)[0;0m WARNING 02-06 10:41:49 [base.py:166] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:49 [factory.py:64] Creating v1 connector with name: NixlConnector and engine_id: 38dfb72a-cbc2-42f6-842c-66ac26a9e290
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:928] Initializing NIXL wrapper
[0;36m(Worker_TP1 pid=2090157)[0;0m WARNING 02-06 10:41:49 [base.py:166] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:929] Initializing NIXL worker 38dfb72a-cbc2-42f6-842c-66ac26a9e290
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:928] Initializing NIXL wrapper
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:929] Initializing NIXL worker 38dfb72a-cbc2-42f6-842c-66ac26a9e290
[0;36m(Worker_TP1 pid=2090293)[0;0m 2026-02-06 10:41:49 NIXL INFO    _api.py:363 Backend UCX was instantiated
[0;36m(Worker_TP1 pid=2090293)[0;0m 2026-02-06 10:41:49 NIXL INFO    _api.py:253 Initialized NIXL agent: 70bd73a1-9d3e-40d3-8e54-28edb0363079
[0;36m(Worker_TP0 pid=2090292)[0;0m 2026-02-06 10:41:49 NIXL INFO    _api.py:363 Backend UCX was instantiated
[0;36m(Worker_TP0 pid=2090292)[0;0m 2026-02-06 10:41:49 NIXL INFO    _api.py:253 Initialized NIXL agent: 624f5a93-a261-4ef9-92a9-6d19ce6b47bc
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [utils.py:73] `VLLM_KV_CACHE_LAYOUT` environment variable detected. Setting KV cache layout to HND.
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:1082] Detected attention backend FLASH_ATTN
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [utils.py:73] `VLLM_KV_CACHE_LAYOUT` environment variable detected. Setting KV cache layout to HND.
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:1083] Detected kv cache layout HND
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:1082] Detected attention backend FLASH_ATTN
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:49 [nixl_connector.py:1083] Detected kv cache layout HND
[0;36m(Worker_TP1 pid=2090157)[0;0m 2026-02-06 10:41:50 NIXL INFO    _api.py:363 Backend UCX was instantiated
[0;36m(Worker_TP1 pid=2090157)[0;0m 2026-02-06 10:41:50 NIXL INFO    _api.py:253 Initialized NIXL agent: ab967c8d-ebd5-4dff-aaa1-64833ef5fb69
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:50 [utils.py:73] `VLLM_KV_CACHE_LAYOUT` environment variable detected. Setting KV cache layout to HND.
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1082] Detected attention backend FLASH_ATTN
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1083] Detected kv cache layout HND
[0;36m(Worker_TP0 pid=2090156)[0;0m 2026-02-06 10:41:50 NIXL INFO    _api.py:363 Backend UCX was instantiated
[0;36m(Worker_TP0 pid=2090156)[0;0m 2026-02-06 10:41:50 NIXL INFO    _api.py:253 Initialized NIXL agent: 4cd176c9-f171-422b-8536-79b8abdf23bf
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:50 [utils.py:73] `VLLM_KV_CACHE_LAYOUT` environment variable detected. Setting KV cache layout to HND.
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1082] Detected attention backend FLASH_ATTN
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1083] Detected kv cache layout HND
[0;36m(Worker_TP0 pid=2090292)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1399] Registering KV_Caches. use_mla: False, kv_buffer_device: cuda, use_host_buffer: False
[0;36m(Worker_TP1 pid=2090293)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1399] Registering KV_Caches. use_mla: False, kv_buffer_device: cuda, use_host_buffer: False
[0;36m(Worker_TP1 pid=2090157)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1399] Registering KV_Caches. use_mla: False, kv_buffer_device: cuda, use_host_buffer: False
[0;36m(Worker_TP0 pid=2090156)[0;0m INFO 02-06 10:41:50 [nixl_connector.py:1399] Registering KV_Caches. use_mla: False, kv_buffer_device: cuda, use_host_buffer: False
[0;36m(EngineCore_DP0 pid=2089638)[0;0m INFO 02-06 10:41:50 [core.py:272] init engine (profile, create kv cache, warmup model) took 3.72 seconds
[0;36m(EngineCore_DP0 pid=2089634)[0;0m INFO 02-06 10:41:50 [core.py:272] init engine (profile, create kv cache, warmup model) took 4.27 seconds
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/engine/core.py", line 957, in run_engine_core
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/engine/core.py", line 711, in __init__
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]     super().__init__(
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/engine/core.py", line 123, in __init__
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]     Scheduler = vllm_config.scheduler_config.get_scheduler_cls()
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/config/scheduler.py", line 156, in get_scheduler_cls
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]     from vllm.v1.core.sched.async_scheduler import AsyncScheduler
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/core/sched/async_scheduler.py", line 6, in <module>
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]     from vllm.v1.core.sched.scheduler import Scheduler
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/core/sched/scheduler.py", line 2070
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]     if
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966]       ^
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:52 [core.py:966] SyntaxError: invalid syntax
[0;36m(Worker_TP0 pid=2090156)[0;0m WARNING 02-06 10:41:52 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(Worker_TP1 pid=2090157)[0;0m WARNING 02-06 10:41:52 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966] EngineCore failed to start.
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/engine/core.py", line 957, in run_engine_core
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/engine/core.py", line 711, in __init__
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]     super().__init__(
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/engine/core.py", line 123, in __init__
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]     Scheduler = vllm_config.scheduler_config.get_scheduler_cls()
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/config/scheduler.py", line 156, in get_scheduler_cls
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]     from vllm.v1.core.sched.async_scheduler import AsyncScheduler
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/core/sched/async_scheduler.py", line 6, in <module>
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]     from vllm.v1.core.sched.scheduler import Scheduler
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]   File "/home/NickLucche/llmd/worktree/vllm/v1/core/sched/scheduler.py", line 2070
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]     if
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966]       ^
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:52 [core.py:966] SyntaxError: invalid syntax
[0;36m(Worker_TP1 pid=2090293)[0;0m WARNING 02-06 10:41:52 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(Worker_TP0 pid=2090292)[0;0m WARNING 02-06 10:41:52 [multiproc_executor.py:786] WorkerProc was terminated
[0;36m(EngineCore_DP0 pid=2089638)[0;0m ERROR 02-06 10:41:54 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[0;36m(EngineCore_DP0 pid=2089634)[0;0m ERROR 02-06 10:41:54 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
