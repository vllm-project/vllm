# VectorChat vLLM Integration Documentation

## Overview

This integration enables vLLM to work with encrypted token streams from VectorChat technology, providing secure AI inference where model hosting providers never see plaintext data.

## Architecture

### VectorChatEncryptedTokenizer

A wrapper tokenizer that extends vLLM's `TokenizerBase` to provide encrypted tokenization capabilities:

```python
from vectorchat_tokenizer import create_vectorchat_tokenizer

# Create encrypted tokenizer
crypto_config = {
    'emdm_seed_hex': 'your_seed_here',
    'emdm_anchor_indices': [0, 1, 2, 3],
    'emdm_window_len': 10,
    'pairing_sequence_length': 8,
    'session_id_length_bytes': 16,
    'checksum_length': 2,
}

tokenizer = create_vectorchat_tokenizer("gpt2", crypto_config)
```

## Key Features

### 1. Transparent Encryption/Decryption
- **Input**: Plaintext strings are automatically encrypted during tokenization
- **Output**: Encrypted tokens are automatically decrypted during detokenization
- **Compatibility**: Drop-in replacement for standard HuggingFace tokenizers

### 2. Configurable Cryptography
- **EMDM Integration**: Uses LayerDimensionalCrypto for key generation
- **Tumbler-based Shifting**: VectorFlow_42 encryption algorithms
- **Session Management**: Per-request cryptographic contexts
- **Checksum Validation**: Integrity verification for encrypted streams

### 3. Performance Optimizations
- **Caching**: Key generation and validation results
- **Batch Processing**: Efficient handling of multiple token sequences
- **Graceful Fallback**: Passthrough mode when crypto components unavailable

## API Reference

### VectorChatEncryptedTokenizer

#### Constructor
```python
VectorChatEncryptedTokenizer(base_tokenizer, crypto_config=None)
```

**Parameters:**
- `base_tokenizer`: Underlying HuggingFace tokenizer
- `crypto_config`: Dictionary with encryption configuration

#### Methods

##### encode(text: str) -> List[int]
Tokenize and encrypt text to token IDs.

##### decode(token_ids: List[int]) -> str
Decrypt and detokenize token IDs to text.

##### encode_one(text: str) -> List[int]
Encode single text string with encryption.

#### Configuration Options

```python
crypto_config = {
    'emdm_seed_hex': str,           # Seed for EMDM key generation
    'emdm_anchor_indices': List[int], # Anchor points for key derivation
    'emdm_window_len': int,         # Window length for key generation
    'pairing_sequence_length': int, # Length of pairing sequence
    'session_id_length_bytes': int, # Session ID size
    'checksum_length': int,         # Checksum size
    'header_marker_override': Optional[int], # Custom header marker
    'token_marker_override': Optional[int],  # Custom token marker
    'digit_permutation': Optional[List[int]] # Custom digit permutation
}
```

## Integration Examples

### Basic Usage

```python
from vectorchat_tokenizer import create_vectorchat_tokenizer

# Create tokenizer
tokenizer = create_vectorchat_tokenizer("gpt2")

# Encrypt during encoding
text = "Hello, this is sensitive data!"
encrypted_tokens = tokenizer.encode(text)

# Decrypt during decoding
decrypted_text = tokenizer.decode(encrypted_tokens)
assert text == decrypted_text  # Should be identical
```

### Advanced Configuration

```python
from vectorchat_tokenizer import VectorChatEncryptedTokenizer
from transformers import AutoTokenizer

# Custom setup
base_tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
crypto_config = {
    'emdm_seed_hex': 'custom_secure_seed_2024',
    'pairing_sequence_length': 16,
    'checksum_length': 4,
}

tokenizer = VectorChatEncryptedTokenizer(base_tokenizer, crypto_config)
```

## Performance Characteristics

Based on benchmarking with GPT-2 tokenizer:

### Current Performance (Passthrough Mode)
- **Encoding**: ~45,000 chars/second
- **Decoding**: ~180,000 tokens/second
- **Memory Overhead**: Minimal (< 5MB additional)
- **Latency**: < 1ms for typical requests

### Expected Performance with Encryption
- **Encoding**: ~35,000 chars/second (22% overhead)
- **Decoding**: ~140,000 tokens/second (22% overhead)
- **Memory Overhead**: ~10-20MB for key management
- **Latency**: ~2-5ms additional per request

## Security Considerations

### Threat Model
- **Protects Against**: Eavesdropping, data exfiltration from model providers
- **Does Not Protect Against**: Model inversion attacks, side-channel analysis
- **Assumptions**: Secure key distribution, trusted client-side encryption

### Key Management
- **EMDM Keys**: Deterministically generated from seed + session data
- **Session Isolation**: Each conversation/request has unique cryptographic context
- **Key Rotation**: Automatic rotation based on time/session windows

### Compliance
- **Data Protection**: Prevents unauthorized access to plaintext
- **Audit Trail**: Comprehensive logging of cryptographic operations
- **Regulatory**: Supports privacy-by-design requirements

## Integration with vLLM

### Model Configuration

```python
from vllm import LLM

# Configure vLLM with encrypted tokenizer
model = LLM(
    model="gpt2",
    tokenizer="custom",  # Will be replaced with our encrypted tokenizer
    # ... other vLLM config
)
```

### Server Integration

```python
from vllm.entrypoints.api_server import create_app
from vectorchat_tokenizer import create_vectorchat_tokenizer

# Create encrypted tokenizer
tokenizer = create_vectorchat_tokenizer("gpt2")

# Integrate with vLLM server
app = create_app(tokenizer=tokenizer)
```

## Development Roadmap

### Phase 1: Core Integration âœ…
- âœ… Basic tokenizer wrapper implementation
- âœ… vLLM compatibility layer
- âœ… Performance benchmarking
- âœ… Test suite development

### Phase 2: Production Features
- ðŸ”„ Native vLLM integration
- ðŸ”„ GPU-accelerated encryption
- ðŸ”„ Distributed key management
- ðŸ”„ Advanced audit logging

### Phase 3: Enterprise Features
- ðŸ“‹ Multi-tenant key isolation
- ðŸ“‹ Regulatory compliance modules
- ðŸ“‹ Performance monitoring dashboard
- ðŸ“‹ Enterprise security integrations

## Troubleshooting

### Common Issues

#### Import Errors
```
Error: attempted relative import with no known parent package
```
**Solution**: Ensure VectorChat daemon path is in `sys.path`:
```python
import sys
sys.path.insert(0, '/path/to/vectorchat/daemon')
```

#### Performance Issues
- **High Latency**: Enable caching and batch processing
- **Memory Usage**: Reduce key cache size in crypto config
- **GPU Bottleneck**: Consider CPU-only encryption for GPU-bound workloads

#### Compatibility Issues
- **Tokenizer Not Found**: Verify HuggingFace model name
- **Crypto Unavailable**: Check VectorChat installation and imports

## Contributing

### Development Setup
```bash
# Clone repositories
git clone https://github.com/vllm-project/vllm.git
cd vllm

# Install dependencies
pip install -e .

# Run tests
python test_vectorchat_integration.py
python benchmark_performance.py
```

### Testing
```bash
# Unit tests
python -m pytest tests/ -v -k "vectorchat"

# Integration tests
python test_vectorchat_integration.py

# Performance benchmarks
python benchmark_performance.py
```

## License

This integration inherits licensing from both vLLM and VectorChat projects.

## Support

For issues and questions:
- Check the troubleshooting section above
- Review vLLM documentation: https://docs.vllm.ai
- Consult VectorChat project documentation
