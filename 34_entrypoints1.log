--- XPU Test Session Started: Sat Feb 21 03:11:05 UTC 2026 ---
TARGET: tests/entrypoints/ --ignore=tests/entrypoints/llm --ignore=tests/entrypoints/rpc --ignore=tests/entrypoints/sleep --ignore=tests/entrypoints/instrumentator --ignore=tests/entrypoints/openai --ignore=tests/entrypoints/offline_mode --ignore=tests/entrypoints/test_chat_utils.py --ignore=tests/entrypoints/pooling
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.2, pluggy-1.6.0 -- /opt/venv/bin/python3
cachedir: .pytest_cache
rootdir: /workspace
configfile: pyproject.toml
plugins: timeout-2.4.0, hydra-core-1.3.2, asyncio-1.3.0, anyio-4.12.1
timeout: 300.0s
timeout method: signal
timeout func_only: False
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fiture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 92 items

tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_customer_script_functions_auto_loaded PASSED [  1%]
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_customer_decorator_usage PASSED [  2%]
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_handler_priority_order PASSED [  3%]
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_environment_variable_script_loading PASSED [  4%]
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_framework_default_handlers PASSED [  5%]
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_handler_env_var_override FAILED [  6%]
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_env_var_priority_over_decorator_and_script FAILED [  7%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_load_adapter_happy_path ERROR [  8%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_unload_adapter_happy_path ERROR [  9%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_load_adapter_not_found ERROR [ 10%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_load_adapter_invalid_files ERROR [ 11%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_unload_nonexistent_adapter ERROR [ 13%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_invocations_with_adapter ERROR [ 14%]
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_multiple_adapters_load_unload ERROR [ 15%]
tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_customer_middleware_with_vllm_server FAILED [ 16%]
tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_middleware_with_ping_endpoint FAILED [ 17%]
tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_middleware_env_var_override FAILED [ 18%]
tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_create_session_badrequest ERROR [ 19%]
tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_close_session_badrequest[nonexistent_session_id-session_id_change0-request_body_change0-session not found] ERROR [ 20%]
tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_close_session_badrequest[malformed_close_request-session_id_change1-request_body_change1-None] ERROR [ 21%]
tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_close_session_invalidrequest ERROR [ 22%]
tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_session ERROR [ 23%]
tests/entrypoints/test_api_server_process_manager.py::test_api_server_process_manager_init[True] PASSED [ 25%]
tests/entrypoints/test_api_server_process_manager.py::test_api_server_process_manager_init[False] PASSED [ 26%]
tests/entrypoints/test_api_server_process_manager.py::test_wait_for_completion_or_failure PASSED [ 27%]
tests/entrypoints/test_api_server_process_manager.py::test_normal_completion PASSED [ 28%]
tests/entrypoints/test_api_server_process_manager.py::test_external_process_monitoring PASSED [ 29%]
tests/entrypoints/test_context.py::test_single_turn_token_counting PASSED [ 30%]
tests/entrypoints/test_context.py::test_multi_turn_token_counting PASSED [ 31%]
tests/entrypoints/test_context.py::test_empty_output_tokens PASSED       [ 32%]
tests/entrypoints/test_context.py::test_missing_prompt_token_ids PASSED  [ 33%]
tests/entrypoints/test_context.py::test_reasoning_tokens_counting PASSED [ 34%]
tests/entrypoints/test_context.py::test_zero_tokens_edge_case PASSED     [ 35%]
tests/entrypoints/test_context.py::test_single_turn_no_tool_output PASSED [ 36%]
tests/entrypoints/test_context.py::test_negative_tool_tokens_edge_case PASSED [ 38%]
tests/entrypoints/test_context.py::test_streaming_multi_turn_token_counting PASSED [ 39%]
tests/entrypoints/test_context.py::test_streaming_message_synchronization PASSED [ 40%]
tests/entrypoints/test_context.py::test_turn_metrics_copy_and_reset PASSED [ 41%]
tests/entrypoints/test_context.py::test_simple_context_output_messages_empty PASSED [ 42%]
tests/entrypoints/test_context.py::test_simple_context_output_messages_single_call PASSED [ 43%]
tests/entrypoints/test_context.py::test_simple_context_output_messages_streaming_consolidation PASSED [ 44%]
tests/entrypoints/test_context.py::test_simple_context_output_messages_many_deltas PASSED [ 45%]
tests/entrypoints/test_context.py::test_simple_context_input_messages PASSED [ 46%]
tests/entrypoints/test_context.py::test_simple_context_token_counting PASSED [ 47%]
tests/entrypoints/test_context.py::test_simple_context_final_output PASSED [ 48%]
tests/entrypoints/test_context.py::test_simple_context_output_messages_empty_text_with_tokens PASSED [ 50%]
tests/entrypoints/test_context.py::test_simple_context_output_messages_no_mutation PASSED [ 51%]
tests/entrypoints/test_grpc_server.py::test_health_check ERROR           [ 52%]
tests/entrypoints/test_grpc_server.py::test_get_model_info ERROR         [ 53%]
tests/entrypoints/test_grpc_server.py::test_get_server_info ERROR        [ 54%]
tests/entrypoints/test_grpc_server.py::test_generate_non_streaming ERROR [ 55%]
tests/entrypoints/test_grpc_server.py::test_generate_streaming ERROR     [ 56%]
tests/entrypoints/test_grpc_server.py::test_generate_with_different_sampling_params ERROR [ 57%]
tests/entrypoints/test_grpc_server.py::test_generate_with_stop_strings ERROR [ 58%]
tests/entrypoints/test_grpc_server.py::test_generate_multiple_requests ERROR [ 59%]
tests/entrypoints/test_grpc_server.py::test_generate_with_seed ERROR     [ 60%]
tests/entrypoints/test_grpc_server.py::test_generate_error_handling ERROR [ 61%]
tests/entrypoints/test_grpc_server.py::test_abort_request ERROR          [ 63%]
tests/entrypoints/test_responses_utils.py::TestResponsesUtils::test_convert_tool_responses_to_completions_format PASSED [ 64%]
tests/entrypoints/test_responses_utils.py::TestResponsesUtils::test_construct_chat_messages_with_tool_call PASSED [ 65%]
tests/entrypoints/test_responses_utils.py::TestResponsesUtils::test_construct_single_message_from_response_item PASSED [ 66%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_string_input_returns_false PASSED [ 67%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_empty_list_returns_false PASSED [ 68%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_completed_message_returns_false PASSED [ 69%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_in_progress_message_returns_true PASSED [ 70%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_incomplete_message_returns_true PASSED [ 71%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_in_progress_reasoning_returns_true PASSED [ 72%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_incomplete_reasoning_returns_true PASSED [ 73%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_completed_reasoning_returns_false PASSED [ 75%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_reasoning_with_none_status_returns_false PASSED [ 76%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_only_last_item_matters PASSED [ 77%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_tool_call_returns_false PASSED [ 78%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_dict_in_progress_message_returns_true PASSED [ 79%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_dict_incomplete_message_returns_true PASSED [ 80%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_dict_completed_message_returns_false PASSED [ 81%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_dict_reasoning_in_progress_returns_true PASSED [ 82%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_dict_without_status_returns_false PASSED [ 83%]
tests/entrypoints/test_responses_utils.py::TestShouldContinueFinalMessage::test_dict_with_none_status_returns_false PASSED [ 84%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_returns_none_when_item_id_is_none PASSED [ 85%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_returns_none_when_id_does_not_start_with_mcp_prefix PASSED [ 86%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_returns_none_when_last_message_is_not_assistant PASSED [ 88%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_returns_none_when_last_message_has_no_reasoning PASSED [ 89%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_combines_reasoning_and_mcp_tool_call PASSED [ 90%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_returns_none_for_non_function_tool_call_type PASSED [ 91%]
tests/entrypoints/test_responses_utils.py::TestMaybeCombineReasoningAndToolCall::test_returns_none_when_id_is_empty_string PASSED [ 92%]
tests/entrypoints/test_ssl_cert_refresher.py::test_ssl_refresher PASSED  [ 93%]
tests/entrypoints/test_utils.py::test_sanitize_message PASSED            [ 94%]
tests/entrypoints/weight_transfer/test_weight_transfer_llm.py::test_get_world_size_tp1 PASSED [ 95%]
tests/entrypoints/weight_transfer/test_weight_transfer_llm.py::test_init_weight_transfer_engine_calls_engine PASSED [ 96%]
tests/entrypoints/weight_transfer/test_weight_transfer_llm.py::test_update_weights_calls_engine PASSED [ 97%]
tests/entrypoints/weight_transfer/test_weight_transfer_llm.py::test_full_weight_transfer_flow PASSED [ 98%]
tests/entrypoints/weight_transfer/test_weight_transfer_llm.py::test_weight_transfer_config_backend PASSED [100%]

==================================== ERRORS ====================================
___________ ERROR at setup of test_sagemaker_load_adapter_happy_path ___________

self = <Coroutine test_sagemaker_load_adapter_happy_path>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:728: in pytest_fixture_setup
    return (yield)
            ^^^^^
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
---------------------------- Captured stdout setup -----------------------------
INFO 02-21 03:15:43 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:15:43 [model.py:1555] Using max model len 8192
INFO 02-21 03:15:43 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 8192 --enforce-eager --enable-lora --max-lora-rank 256 --max-cpu-loras 2 --max-num-seqs 64 --port 34587 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 
WARNING 02-21 03:15:51 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:15:51 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:287] 
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:287] 
(APIServer pid=248988) INFO 02-21 03:15:54 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 34587, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 8192, 'enforce_eager': True, 'enable_lora': True, 'max_lora_rank': 256, 'max_cpu_loras': 2, 'max_num_seqs': 64}
(APIServer pid=248988) INFO 02-21 03:15:55 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=248988) INFO 02-21 03:15:55 [model.py:1555] Using max model len 8192
(APIServer pid=248988) INFO 02-21 03:15:55 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=248988) INFO 02-21 03:15:55 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=248988) WARNING 02-21 03:15:55 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:16:04 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:16:04 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=249269) INFO 02-21 03:16:06 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=249269) INFO 02-21 03:16:07 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:52569 backend=xccl
(EngineCore_DP0 pid=249269) INFO 02-21 03:16:07 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=249269) ERROR 02-21 03:16:07 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
---------------------------- Captured stderr setup -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=249269) Process EngineCore_DP0:
(EngineCore_DP0 pid=249269) Traceback (most recent call last):
(EngineCore_DP0 pid=249269)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=249269)     self.run()
(EngineCore_DP0 pid=249269)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=249269)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=249269)     raise e
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=249269)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=249269)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=249269)     super().__init__(
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=249269)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=249269)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=249269)     self._init_executor()
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=249269)     self.driver_worker.init_device()
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=249269)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=249269)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=249269)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=249269)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249269)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=249269)     raise ValueError(
(EngineCore_DP0 pid=249269) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=248988) Traceback (most recent call last):
(APIServer pid=248988)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=248988)     sys.exit(main())
(APIServer pid=248988)              ^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=248988)     args.dispatch_function(args)
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=248988)     uvloop.run(run_server(args))
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=248988)     return __asyncio.run(
(APIServer pid=248988)            ^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=248988)     return runner.run(main)
(APIServer pid=248988)            ^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=248988)     return self._loop.run_until_complete(task)
(APIServer pid=248988)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=248988)     return await main
(APIServer pid=248988)            ^^^^^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=248988)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=248988)     async with build_async_engine_client(
(APIServer pid=248988)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=248988)     return await anext(self.gen)
(APIServer pid=248988)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=248988)     async with build_async_engine_client_from_engine_args(
(APIServer pid=248988)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=248988)     return await anext(self.gen)
(APIServer pid=248988)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=248988)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=248988)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=248988)     return cls(
(APIServer pid=248988)            ^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=248988)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=248988)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=248988)     return AsyncMPClient(*client_args)
(APIServer pid=248988)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=248988)     super().__init__(
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=248988)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=248988)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=248988)     next(self.gen)
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=248988)     wait_for_engine_startup(
(APIServer pid=248988)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=248988)     raise RuntimeError(
(APIServer pid=248988) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
__________ ERROR at setup of test_sagemaker_unload_adapter_happy_path __________

self = <Coroutine test_sagemaker_unload_adapter_happy_path>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
___________ ERROR at setup of test_sagemaker_load_adapter_not_found ____________

self = <Coroutine test_sagemaker_load_adapter_not_found>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
_________ ERROR at setup of test_sagemaker_load_adapter_invalid_files __________

self = <Coroutine test_sagemaker_load_adapter_invalid_files>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
_________ ERROR at setup of test_sagemaker_unload_nonexistent_adapter __________

self = <Coroutine test_sagemaker_unload_nonexistent_adapter>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
__________ ERROR at setup of test_sagemaker_invocations_with_adapter ___________

self = <Coroutine test_sagemaker_invocations_with_adapter>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
________ ERROR at setup of test_sagemaker_multiple_adapters_load_unload ________

self = <Coroutine test_sagemaker_multiple_adapters_load_unload>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f77890>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
_______________ ERROR at setup of test_create_session_badrequest _______________

self = <Coroutine test_create_session_badrequest>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:728: in pytest_fixture_setup
    return (yield)
            ^^^^^
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a0512a20>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
---------------------------- Captured stdout setup -----------------------------
INFO 02-21 03:17:44 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:17:44 [model.py:1555] Using max model len 8192
INFO 02-21 03:17:44 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 8192 --enforce-eager --enable-lora --max-lora-rank 256 --max-cpu-loras 2 --max-num-seqs 64 --port 57107 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 'ONEAPI_ROOT': '/opt/intel/oneapi', 'PKG_CONFIG_PATH': '/opt/intel/oneapi/tbb/2022.3/env/../lib/pkgconfig:/opt/intel/oneapi/mpi/2021.17/lib/pkgconfig:/opt/intel/oneapi/mkl/2025.3/lib/pkgconfig:/opt/intel/oneapi/dpl/2022.10/lib/pkgconfig:/opt/intel/oneapi/dnnl/2025.3/lib/pkgconfig:/opt/intel/oneapi/compiler/2025.3/lib/pkgconfig:/opt/intel/oneapi/ccl/2021.17/lib/pkgconfig/', 'HOSTNAME': 'srf797635.jf.intel.com', 'CCL_ROOT': '/opt/intel/oneapi/ccl/2021.17', 'I_MPI_ROOT': '/opt/intel/oneapi/mpi/2021.17', 'FI_PROVIDER_PATH': '/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric', 'DNNLROOT': '/opt/intel/oneapi/dnnl/2025.3', 'DIAGUTIL_PATH': '/opt/intel/oneapi/compiler/2025.3/etc/compiler/sys_check/sys_check.sh', 'UV_PYTHON_INSTALL_DIR': '/opt/uv/python', 'PWD': '/workspace', 'CCL_CONFIGURATION': 'cpu_gpu_dpcpp', 'DPL_ROOT': '/opt/intel/oneapi/dpl/2022.10', 'MANPATH': '/opt/intel/oneapi/mpi/2021.17/share/man:/opt/intel/oneapi/debugger/2025.3/share/man:/opt/intel/oneapi/compiler/2025.3/share/man:', 'UV_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'TCM_ROOT': '/opt/intel/oneapi/tcm/1.4', '_': '/opt/venv/bin/python3', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'HOME': '/root', 'GDB_INFO': '/opt/intel/oneapi/debugger/2025.3/share/info/', 'CCL_CONFIGURATION_PATH': '', 'LANG': 'C.UTF-8', 'VIRTUAL_ENV': '/opt/venv', 'SETVARS_COMPLETED': '1', 'PIP_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'CMAKE_PREFIX_PATH': '/opt/intel/oneapi/tbb/2022.3/env/..:/opt/intel/oneapi/pti/0.16/lib/cmake/pti:/opt/intel/oneapi/mkl/2025.3/lib/cmake:/opt/intel/oneapi/dpl/2022.10/lib/cmake/oneDPL:/opt/intel/oneapi/dnnl/2025.3/lib/cmake:/opt/intel/oneapi/compiler/2025.3:/opt/intel/oneapi/ccl/2021.17/lib/cmake/oneCCL', 'https_proxy': 'http://proxy-dmz.intel.com:912', 'HF_TOKEN': 'HF_TOKEN_REDACTED', 'UV_LINK_MODE': 'copy', 'DPLROOT': '/opt/intel/oneapi/dpl/2022.10', 'CMPLR_ROOT': '/opt/intel/oneapi/compiler/2025.3', 'ZE_AFFINITY_MASK': '0,1', 'Pti_DIR': '/opt/intel/oneapi/pti/0.16/lib/cmake/pti', 'INFOPATH': '/opt/intel/oneapi/debugger/2025.3/share/info', 'UV_INDEX_STRATEGY': 'unsafe-best-match', 'CPLUS_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dpl/2022.10/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UMF_ROOT': '/opt/intel/oneapi/umf/1.0', 'LIBRARY_PATH': '/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/', 'SHLVL': '1', 'OCL_ICD_FILENAMES': '/opt/intel/oneapi/compiler/2025.3/lib/libintelocl.so', 'http_proxy': 'http://proxy-dmz.intel.com:912', 'VLLM_TARGET_DEVICE': 'xpu', 'CLASSPATH': '/opt/intel/oneapi/mpi/2021.17/share/java/mpi.jar', 'LD_LIBRARY_PATH': '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/debugger/2025.3/opt/debugger/lib:/opt/intel/oneapi/compiler/2025.3/opt/compiler/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/:/usr/local/lib/', 'MKLROOT': '/opt/intel/oneapi/mkl/2025.3', 'HF_HUB_OFFLINE': '0', 'NLSPATH': '/opt/intel/oneapi/compiler/2025.3/lib/compiler/locale/%l_%t/%N', 'PATH': '/opt/venv/bin:/root/.local/bin:/opt/intel/oneapi/mpi/2021.17/bin:/opt/intel/oneapi/mkl/2025.3/bin:/opt/intel/oneapi/dev-utilities/2025.3/bin:/opt/intel/oneapi/debugger/2025.3/opt/debugger/bin:/opt/intel/oneapi/compiler/2025.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'INTEL_PYTHONHOME': '/opt/intel/oneapi/debugger/2025.3/opt/debugger', 'C_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UV_HTTP_TIMEOUT': '500', 'CPATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dev-utilities/2025.3/include:/opt/intel/oneapi/ccl/2021.17/include', 'NIXL_VERSION': '0.7.0', 'PYTEST_VERSION': '9.0.2', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'TORCHINDUCTOR_COMPILE_THREADS': '1', 'PYTEST_CURRENT_TEST': 'tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_create_session_badrequest (setup)', 'VLLM_ALLOW_RUNTIME_LORA_UPDATING': 'True', 'SAGEMAKER_ENABLE_STATEFUL_SESSIONS': 'True'}
WARNING 02-21 03:17:52 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:17:52 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:287] 
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:287] 
(APIServer pid=251324) INFO 02-21 03:17:55 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 57107, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 8192, 'enforce_eager': True, 'enable_lora': True, 'max_lora_rank': 256, 'max_cpu_loras': 2, 'max_num_seqs': 64}
(APIServer pid=251324) INFO 02-21 03:17:55 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=251324) INFO 02-21 03:17:55 [model.py:1555] Using max model len 8192
(APIServer pid=251324) INFO 02-21 03:17:56 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=251324) INFO 02-21 03:17:56 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=251324) WARNING 02-21 03:17:56 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:18:04 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:18:04 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=251605) INFO 02-21 03:18:07 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=251605) INFO 02-21 03:18:07 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:35415 backend=xccl
(EngineCore_DP0 pid=251605) INFO 02-21 03:18:07 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=251605) ERROR 02-21 03:18:08 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
---------------------------- Captured stderr setup -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=251605) Process EngineCore_DP0:
(EngineCore_DP0 pid=251605) Traceback (most recent call last):
(EngineCore_DP0 pid=251605)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=251605)     self.run()
(EngineCore_DP0 pid=251605)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=251605)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=251605)     raise e
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=251605)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=251605)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=251605)     super().__init__(
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=251605)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=251605)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=251605)     self._init_executor()
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=251605)     self.driver_worker.init_device()
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=251605)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=251605)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=251605)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=251605)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251605)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=251605)     raise ValueError(
(EngineCore_DP0 pid=251605) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=251324) Traceback (most recent call last):
(APIServer pid=251324)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=251324)     sys.exit(main())
(APIServer pid=251324)              ^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=251324)     args.dispatch_function(args)
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=251324)     uvloop.run(run_server(args))
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=251324)     return __asyncio.run(
(APIServer pid=251324)            ^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=251324)     return runner.run(main)
(APIServer pid=251324)            ^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=251324)     return self._loop.run_until_complete(task)
(APIServer pid=251324)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=251324)     return await main
(APIServer pid=251324)            ^^^^^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=251324)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=251324)     async with build_async_engine_client(
(APIServer pid=251324)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=251324)     return await anext(self.gen)
(APIServer pid=251324)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=251324)     async with build_async_engine_client_from_engine_args(
(APIServer pid=251324)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=251324)     return await anext(self.gen)
(APIServer pid=251324)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=251324)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=251324)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=251324)     return cls(
(APIServer pid=251324)            ^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=251324)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=251324)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=251324)     return AsyncMPClient(*client_args)
(APIServer pid=251324)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=251324)     super().__init__(
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=251324)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=251324)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=251324)     next(self.gen)
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=251324)     wait_for_engine_startup(
(APIServer pid=251324)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=251324)     raise RuntimeError(
(APIServer pid=251324) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
_ ERROR at setup of test_close_session_badrequest[nonexistent_session_id-session_id_change0-request_body_change0-session not found] _

self = <Coroutine test_close_session_badrequest[nonexistent_session_id-session_id_change0-request_body_change0-session not found]>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a0512a20>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
_ ERROR at setup of test_close_session_badrequest[malformed_close_request-session_id_change1-request_body_change1-None] _

self = <Coroutine test_close_session_badrequest[malformed_close_request-session_id_change1-request_body_change1-None]>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a0512a20>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
_____________ ERROR at setup of test_close_session_invalidrequest ______________

self = <Coroutine test_close_session_invalidrequest>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a0512a20>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
________________________ ERROR at setup of test_session ________________________

self = <Coroutine test_session>

    def setup(self) -> None:
        runner_fixture_id = f"_{self._loop_scope}_scoped_runner"
        if runner_fixture_id not in self.fixturenames:
            self.fixturenames.append(runner_fixture_id)
>       return super().setup()
               ^^^^^^^^^^^^^^^

/opt/venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:458: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/entrypoints/sagemaker/conftest.py:52: in basic_server_with_lora
    with RemoteOpenAIServer(MODEL_NAME_SMOLLM, args, env_dict=envs) as remote_server:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a0512a20>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
_____________________ ERROR at setup of test_health_check ______________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
---------------------------- Captured stdout setup -----------------------------
waiting for server to start...
WARNING 02-21 03:18:37 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:18:37 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO 02-21 03:18:40 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:18:40 [model.py:1555] Using max model len 8192
INFO 02-21 03:18:40 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 02-21 03:18:40 [vllm.py:689] Asynchronous scheduling is enabled.
WARNING 02-21 03:18:48 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:18:48 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=252425) INFO 02-21 03:18:50 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='hmellor/tiny-random-LlamaForCausalLM', speculative_config=None, tokenizer='hmellor/tiny-random-LlamaForCausalLM', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=hmellor/tiny-random-LlamaForCausalLM, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer', 'vllm::unified_kv_cache_update'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [512], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=252425) INFO 02-21 03:18:50 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:48627 backend=xccl
(EngineCore_DP0 pid=252425) INFO 02-21 03:18:50 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]   File "/workspace/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=252425) ERROR 02-21 03:18:51 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
---------------------------- Captured stderr setup -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
[2026-02-21 03:18:39] INFO utils.py:287: vLLM server version 0.1.dev13788+ga2443de5f.d20260220, serving model hmellor/tiny-random-LlamaForCausalLM
[2026-02-21 03:18:39] INFO grpc_server.py:413: vLLM gRPC server args: Namespace(host='localhost', port=38485, disable_log_stats_server=True, model='hmellor/tiny-random-LlamaForCausalLM', runner='auto', convert='auto', tokenizer=None, tokenizer_mode='auto', trust_remote_code=False, dtype='auto', seed=0, hf_config_path=None, allowed_local_media_path='', allowed_media_domains=None, revision=None, code_revision=None, tokenizer_revision=None, max_model_len=None, quantization=None, allow_deprecated_quantization=False, enforce_eager=False, enable_return_routed_experts=False, max_logprobs=20, logprobs_mode='raw_logprobs', disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name=None, config_format='auto', hf_token=None, hf_overrides={}, pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', override_attention_dtype=None, logits_processors=None, io_processor_plugin=None, load_format='auto', download_dir=None, safetensors_load_strategy='lazy', model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu', attention_backend=None, reasoning_parser='', reasoning_parser_plugin='', distributed_executor_backend=None, pipeline_parallel_size=1, master_addr='127.0.0.1', master_port=29501, nnodes=1, node_rank=0, tensor_parallel_size=1, decode_context_parallel_size=1, dcp_kv_cache_interleave_size=1, cp_kv_cache_interleave_size=1, prefill_context_parallel_size=1, data_parallel_size=1, data_parallel_rank=None, data_parallel_start_rank=None, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, data_parallel_backend='mp', data_parallel_hybrid_lb=False, data_parallel_external_lb=False, enable_expert_parallel=False, all2all_backend='allgather_reducescatter', enable_dbo=False, ubatch_size=0, dbo_decode_token_threshold=32, dbo_prefill_token_threshold=512, disable_nccl_for_dp_synchronization=None, enable_eplb=False, eplb_config=EPLBConfig(window_size=1000, step_interval=3000, num_redundant_experts=0, log_balancedness=False, log_balancedness_interval=1, use_async=False, policy='default'), expert_placement_strategy='linear', max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, worker_cls='auto', worker_extension_cls='', block_size=None, gpu_memory_utilization=0.9, kv_cache_memory_bytes=None, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='sha256', cpu_offload_gb=0, calculate_kv_scales=False, kv_sharing_fast_prefill=False, mamba_cache_dtype='auto', mamba_ssm_cache_dtype='auto', mamba_block_size=None, mamba_cache_mode='none', kv_offloading_size=None, kv_offloading_backend='native', language_model_only=False, limit_mm_per_prompt={}, enable_mm_embeds=False, media_io_kwargs={}, mm_processor_kwargs=None, mm_processor_cache_gb=4, mm_processor_cache_type='lru', mm_shm_cache_max_object_size_mb=128, mm_encoder_only=False, mm_encoder_tp_mode='weights', mm_encoder_attn_backend=None, interleave_mm_strings=False, skip_mm_profiling=False, video_pruning_rate=None, enable_lora=None, max_loras=1, max_lora_rank=16, lora_dtype='auto', enable_tower_connector_lora=False, max_cpu_loras=None, fully_sharded_loras=False, default_mm_loras=None, specialize_active_lora=False, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_logging_iteration_details=False, max_num_batched_tokens=512, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls=None, disable_hybrid_kv_cache_manager=None, async_scheduling=None, stream_interval=1, cudagraph_capture_sizes=None, max_cudagraph_capture_size=None, enable_flashinfer_autotune=None, speculative_config=None, kv_transfer_config=None, kv_events_config=None, ec_transfer_config=None, compilation_config={'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': None, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': None, 'static_all_moe_layers': []}, attention_config=AttentionConfig(backend=None, flash_attn_version=None, use_prefill_decode_attention=False, flash_attn_max_num_splits_for_cuda_graph=32, use_cudnn_prefill=False, use_trtllm_ragged_deepseek_prefill=True, use_trtllm_attention=None, disable_flashinfer_prefill=False, disable_flashinfer_q_quantization=False), kernel_config=KernelConfig(enable_flashinfer_autotune=None), additional_config={}, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), profiler_config=ProfilerConfig(profiler=None, torch_profiler_dir='', torch_profiler_with_stack=True, torch_profiler_with_flops=False, torch_profiler_use_gzip=True, torch_profiler_dump_cuda_time_total=True, torch_profiler_record_shapes=False, torch_profiler_with_memory=False, ignore_frontend=False, delay_iterations=0, max_iterations=0), optimization_level=<OptimizationLevel.O2: 2>, weight_transfer_config=None, disable_log_stats=False, aggregate_engine_logging=False, enable_log_requests=False, disable_log_requests=True)
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=252425) Process EngineCore_DP0:
(EngineCore_DP0 pid=252425) Traceback (most recent call last):
(EngineCore_DP0 pid=252425)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=252425)     self.run()
(EngineCore_DP0 pid=252425)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=252425)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=252425)     raise e
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=252425)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=252425)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=252425)     super().__init__(
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=252425)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=252425)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=252425)     self._init_executor()
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=252425)     self.driver_worker.init_device()
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=252425)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=252425)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=252425)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=252425)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=252425)   File "/workspace/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=252425)     raise ValueError(
(EngineCore_DP0 pid=252425) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[2026-02-21 03:18:52] ERROR grpc_server.py:527: Server failed: Engine core initialization failed. See root cause above. Failed core proc(s): {}
Traceback (most recent call last):
  File "/workspace/vllm/entrypoints/grpc_server.py", line 525, in main
    uvloop.run(serve_grpc(args))
  File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/workspace/vllm/entrypoints/grpc_server.py", line 426, in serve_grpc
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
    return cls(
           ^^^^
  File "/workspace/vllm/v1/engine/async_llm.py", line 148, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/vllm/v1/engine/core_client.py", line 835, in __init__
    super().__init__(
  File "/workspace/vllm/v1/engine/core_client.py", line 490, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/workspace/vllm/v1/engine/utils.py", line 925, in launch_core_engines
    wait_for_engine_startup(
  File "/workspace/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
____________________ ERROR at setup of test_get_model_info _____________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
____________________ ERROR at setup of test_get_server_info ____________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
________________ ERROR at setup of test_generate_non_streaming _________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
__________________ ERROR at setup of test_generate_streaming ___________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
________ ERROR at setup of test_generate_with_different_sampling_params ________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
______________ ERROR at setup of test_generate_with_stop_strings _______________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
______________ ERROR at setup of test_generate_multiple_requests _______________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
__________________ ERROR at setup of test_generate_with_seed ___________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
________________ ERROR at setup of test_generate_error_handling ________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
_____________________ ERROR at setup of test_abort_request _____________________

    @pytest_asyncio.fixture(scope="module")
    async def grpc_server():
        """Fixture providing a running gRPC server in a subprocess."""
        server = GrpcServerProcess()
>       await server.start()

tests/entrypoints/test_grpc_server.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.entrypoints.test_grpc_server.GrpcServerProcess object at 0x72b6a0566f30>

    async def start(self):
        """Start the gRPC server process."""
        self.port = find_free_port()
    
        # Start the server as a subprocess
        self.process = subprocess.Popen(
            [
                sys.executable,
                "-m",
                "vllm.entrypoints.grpc_server",
                "--model",
                MODEL_NAME,
                "--host",
                "localhost",
                "--port",
                str(self.port),
                "--max-num-batched-tokens",
                "512",
                "--disable-log-stats-server",
            ],
        )
    
        # Wait for server to be ready
        if not await wait_for_server(self.port):
            self.stop()
>           raise RuntimeError("gRPC server failed to start within timeout")
E           RuntimeError: gRPC server failed to start within timeout

tests/entrypoints/test_grpc_server.py:83: RuntimeError
=================================== FAILURES ===================================
_________ TestHandlerOverrideIntegration.test_handler_env_var_override _________

self = <tests.entrypoints.sagemaker.test_sagemaker_handler_overrides.TestHandlerOverrideIntegration object at 0x72b6d3fbb620>

        @pytest.mark.asyncio
        async def test_handler_env_var_override(self):
            """Test CUSTOM_FASTAPI_PING_HANDLER and CUSTOM_FASTAPI_INVOCATION_HANDLER
            environment variable overrides."""
            try:
                from model_hosting_container_standards.common.fastapi.config import (
                    FastAPIEnvVars,
                )
                from model_hosting_container_standards.sagemaker.config import (
                    SageMakerEnvVars,
                )
            except ImportError:
                pytest.skip("model-hosting-container-standards not available")
    
            # Create a script with both env var handlers and script functions
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                f.write(
                    """
    from fastapi import Request, Response
    import json
    
    async def env_var_ping_handler(raw_request: Request) -> Response:
        return Response(
            content=json.dumps({
                "status": "healthy",
                "source": "env_var_ping",
                "method": "environment_variable"
            }),
            media_type="application/json"
        )
    
    async def env_var_invoke_handler(raw_request: Request) -> Response:
        return Response(
            content=json.dumps({
                "predictions": ["Environment variable response"],
                "source": "env_var_invoke",
                "method": "environment_variable"
            }),
            media_type="application/json"
        )
    
    async def custom_sagemaker_ping_handler():
        return {
            "status": "healthy",
            "source": "script_ping",
            "method": "script_function"
        }
    
    async def custom_sagemaker_invocation_handler(request: Request):
        return {
            "predictions": ["Script function response"],
            "source": "script_invoke",
            "method": "script_function"
        }
    """
                )
                script_path = f.name
    
            try:
                script_dir = os.path.dirname(script_path)
                script_name = os.path.basename(script_path)
    
                # Set environment variables to override both handlers
                env_vars = {
                    SageMakerEnvVars.SAGEMAKER_MODEL_PATH: script_dir,
                    SageMakerEnvVars.CUSTOM_SCRIPT_FILENAME: script_name,
                    FastAPIEnvVars.CUSTOM_FASTAPI_PING_HANDLER: (
                        f"{script_name}:env_var_ping_handler"
                    ),
                    FastAPIEnvVars.CUSTOM_FASTAPI_INVOCATION_HANDLER: (
                        f"{script_name}:env_var_invoke_handler"
                    ),
                }
    
                args = [
                    "--dtype",
                    "bfloat16",
                    "--max-model-len",
                    "2048",
                    "--enforce-eager",
                    "--max-num-seqs",
                    "32",
                ]
    
>               with RemoteOpenAIServer(
                    MODEL_NAME_SMOLLM, args, env_dict=env_vars
                ) as server:

tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py:564: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f50b90>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
----------------------------- Captured stdout call -----------------------------
INFO 02-21 03:14:35 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:14:35 [model.py:1555] Using max model len 2048
INFO 02-21 03:14:35 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 2048 --enforce-eager --max-num-seqs 32 --port 52521 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 'ONEAPI_ROOT': '/opt/intel/oneapi', 'PKG_CONFIG_PATH': '/opt/intel/oneapi/tbb/2022.3/env/../lib/pkgconfig:/opt/intel/oneapi/mpi/2021.17/lib/pkgconfig:/opt/intel/oneapi/mkl/2025.3/lib/pkgconfig:/opt/intel/oneapi/dpl/2022.10/lib/pkgconfig:/opt/intel/oneapi/dnnl/2025.3/lib/pkgconfig:/opt/intel/oneapi/compiler/2025.3/lib/pkgconfig:/opt/intel/oneapi/ccl/2021.17/lib/pkgconfig/', 'HOSTNAME': 'srf797635.jf.intel.com', 'CCL_ROOT': '/opt/intel/oneapi/ccl/2021.17', 'I_MPI_ROOT': '/opt/intel/oneapi/mpi/2021.17', 'FI_PROVIDER_PATH': '/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric', 'DNNLROOT': '/opt/intel/oneapi/dnnl/2025.3', 'DIAGUTIL_PATH': '/opt/intel/oneapi/compiler/2025.3/etc/compiler/sys_check/sys_check.sh', 'UV_PYTHON_INSTALL_DIR': '/opt/uv/python', 'PWD': '/workspace', 'CCL_CONFIGURATION': 'cpu_gpu_dpcpp', 'DPL_ROOT': '/opt/intel/oneapi/dpl/2022.10', 'MANPATH': '/opt/intel/oneapi/mpi/2021.17/share/man:/opt/intel/oneapi/debugger/2025.3/share/man:/opt/intel/oneapi/compiler/2025.3/share/man:', 'UV_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'TCM_ROOT': '/opt/intel/oneapi/tcm/1.4', '_': '/opt/venv/bin/python3', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'HOME': '/root', 'GDB_INFO': '/opt/intel/oneapi/debugger/2025.3/share/info/', 'CCL_CONFIGURATION_PATH': '', 'LANG': 'C.UTF-8', 'VIRTUAL_ENV': '/opt/venv', 'SETVARS_COMPLETED': '1', 'PIP_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'CMAKE_PREFIX_PATH': '/opt/intel/oneapi/tbb/2022.3/env/..:/opt/intel/oneapi/pti/0.16/lib/cmake/pti:/opt/intel/oneapi/mkl/2025.3/lib/cmake:/opt/intel/oneapi/dpl/2022.10/lib/cmake/oneDPL:/opt/intel/oneapi/dnnl/2025.3/lib/cmake:/opt/intel/oneapi/compiler/2025.3:/opt/intel/oneapi/ccl/2021.17/lib/cmake/oneCCL', 'https_proxy': 'http://proxy-dmz.intel.com:912', 'HF_TOKEN': 'HF_TOKEN_REDACTED', 'UV_LINK_MODE': 'copy', 'DPLROOT': '/opt/intel/oneapi/dpl/2022.10', 'CMPLR_ROOT': '/opt/intel/oneapi/compiler/2025.3', 'ZE_AFFINITY_MASK': '0,1', 'Pti_DIR': '/opt/intel/oneapi/pti/0.16/lib/cmake/pti', 'INFOPATH': '/opt/intel/oneapi/debugger/2025.3/share/info', 'UV_INDEX_STRATEGY': 'unsafe-best-match', 'CPLUS_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dpl/2022.10/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UMF_ROOT': '/opt/intel/oneapi/umf/1.0', 'LIBRARY_PATH': '/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/', 'SHLVL': '1', 'OCL_ICD_FILENAMES': '/opt/intel/oneapi/compiler/2025.3/lib/libintelocl.so', 'http_proxy': 'http://proxy-dmz.intel.com:912', 'VLLM_TARGET_DEVICE': 'xpu', 'CLASSPATH': '/opt/intel/oneapi/mpi/2021.17/share/java/mpi.jar', 'LD_LIBRARY_PATH': '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/debugger/2025.3/opt/debugger/lib:/opt/intel/oneapi/compiler/2025.3/opt/compiler/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/:/usr/local/lib/', 'MKLROOT': '/opt/intel/oneapi/mkl/2025.3', 'HF_HUB_OFFLINE': '0', 'NLSPATH': '/opt/intel/oneapi/compiler/2025.3/lib/compiler/locale/%l_%t/%N', 'PATH': '/opt/venv/bin:/root/.local/bin:/opt/intel/oneapi/mpi/2021.17/bin:/opt/intel/oneapi/mkl/2025.3/bin:/opt/intel/oneapi/dev-utilities/2025.3/bin:/opt/intel/oneapi/debugger/2025.3/opt/debugger/bin:/opt/intel/oneapi/compiler/2025.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'INTEL_PYTHONHOME': '/opt/intel/oneapi/debugger/2025.3/opt/debugger', 'C_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UV_HTTP_TIMEOUT': '500', 'CPATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dev-utilities/2025.3/include:/opt/intel/oneapi/ccl/2021.17/include', 'NIXL_VERSION': '0.7.0', 'PYTEST_VERSION': '9.0.2', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'TORCHINDUCTOR_COMPILE_THREADS': '1', 'PYTEST_CURRENT_TEST': 'tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_handler_env_var_override (call)', 'SAGEMAKER_MODEL_PATH': '/tmp', 'CUSTOM_SCRIPT_FILENAME': 'tmp2p0dkee1.py', 'CUSTOM_FASTAPI_PING_HANDLER': 'tmp2p0dkee1.py:env_var_ping_handler', 'CUSTOM_FASTAPI_INVOCATION_HANDLER': 'tmp2p0dkee1.py:env_var_invoke_handler'}
WARNING 02-21 03:14:44 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:14:44 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:287] 
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:287] 
(APIServer pid=247813) INFO 02-21 03:14:47 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 52521, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 2048, 'enforce_eager': True, 'max_num_seqs': 32}
(APIServer pid=247813) INFO 02-21 03:14:48 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=247813) INFO 02-21 03:14:48 [model.py:1555] Using max model len 2048
(APIServer pid=247813) INFO 02-21 03:14:48 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=247813) INFO 02-21 03:14:48 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=247813) WARNING 02-21 03:14:48 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:14:57 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:14:57 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=248094) INFO 02-21 03:14:59 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=248094) INFO 02-21 03:15:00 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:55249 backend=xccl
(EngineCore_DP0 pid=248094) INFO 02-21 03:15:00 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=248094) ERROR 02-21 03:15:01 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
----------------------------- Captured stderr call -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=248094) Process EngineCore_DP0:
(EngineCore_DP0 pid=248094) Traceback (most recent call last):
(EngineCore_DP0 pid=248094)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=248094)     self.run()
(EngineCore_DP0 pid=248094)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=248094)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=248094)     raise e
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=248094)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=248094)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=248094)     super().__init__(
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=248094)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=248094)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=248094)     self._init_executor()
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=248094)     self.driver_worker.init_device()
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=248094)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=248094)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=248094)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=248094)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248094)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=248094)     raise ValueError(
(EngineCore_DP0 pid=248094) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=247813) Traceback (most recent call last):
(APIServer pid=247813)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=247813)     sys.exit(main())
(APIServer pid=247813)              ^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=247813)     args.dispatch_function(args)
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=247813)     uvloop.run(run_server(args))
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=247813)     return __asyncio.run(
(APIServer pid=247813)            ^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=247813)     return runner.run(main)
(APIServer pid=247813)            ^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=247813)     return self._loop.run_until_complete(task)
(APIServer pid=247813)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=247813)     return await main
(APIServer pid=247813)            ^^^^^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=247813)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=247813)     async with build_async_engine_client(
(APIServer pid=247813)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=247813)     return await anext(self.gen)
(APIServer pid=247813)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=247813)     async with build_async_engine_client_from_engine_args(
(APIServer pid=247813)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=247813)     return await anext(self.gen)
(APIServer pid=247813)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=247813)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=247813)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=247813)     return cls(
(APIServer pid=247813)            ^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=247813)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=247813)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=247813)     return AsyncMPClient(*client_args)
(APIServer pid=247813)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=247813)     super().__init__(
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=247813)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=247813)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=247813)     next(self.gen)
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=247813)     wait_for_engine_startup(
(APIServer pid=247813)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=247813)     raise RuntimeError(
(APIServer pid=247813) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
--------------------------- Captured stdout teardown ---------------------------
WARNING 02-21 03:15:06 [parallel_state.py:1644] torch._C._host_emptyCache() only available in Pytorch >=2.5
_ TestHandlerOverrideIntegration.test_env_var_priority_over_decorator_and_script _

self = <tests.entrypoints.sagemaker.test_sagemaker_handler_overrides.TestHandlerOverrideIntegration object at 0x72b6d3fbba40>

        @pytest.mark.asyncio
        async def test_env_var_priority_over_decorator_and_script(self):
            """Test that environment variables have highest priority over decorators
            and script functions for both ping and invocation handlers."""
            try:
                from model_hosting_container_standards.common.fastapi.config import (
                    FastAPIEnvVars,
                )
                from model_hosting_container_standards.sagemaker.config import (
                    SageMakerEnvVars,
                )
            except ImportError:
                pytest.skip("model-hosting-container-standards not available")
    
            # Create a script with all three handler types for both ping and invocation
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                f.write(
                    """
    import model_hosting_container_standards.sagemaker as sagemaker_standards
    from fastapi import Request, Response
    import json
    
    # Environment variable handlers (highest priority)
    async def env_priority_ping(raw_request: Request) -> Response:
        return Response(
            content=json.dumps({
                "status": "healthy",
                "source": "env_var",
                "priority": "environment_variable"
            }),
            media_type="application/json"
        )
    
    async def env_priority_invoke(raw_request: Request) -> Response:
        return Response(
            content=json.dumps({
                "predictions": ["Environment variable response"],
                "source": "env_var",
                "priority": "environment_variable"
            }),
            media_type="application/json"
        )
    
    # Decorator handlers (medium priority)
    @sagemaker_standards.custom_ping_handler
    async def decorator_ping(raw_request: Request) -> Response:
        return Response(
            content=json.dumps({
                "status": "healthy",
                "source": "decorator",
                "priority": "decorator"
            }),
            media_type="application/json"
        )
    
    @sagemaker_standards.custom_invocation_handler
    async def decorator_invoke(raw_request: Request) -> Response:
        return Response(
            content=json.dumps({
                "predictions": ["Decorator response"],
                "source": "decorator",
                "priority": "decorator"
            }),
            media_type="application/json"
        )
    
    # Script functions (lowest priority)
    async def custom_sagemaker_ping_handler():
        return {
            "status": "healthy",
            "source": "script",
            "priority": "script_function"
        }
    
    async def custom_sagemaker_invocation_handler(request: Request):
        return {
            "predictions": ["Script function response"],
            "source": "script",
            "priority": "script_function"
        }
    """
                )
                script_path = f.name
    
            try:
                script_dir = os.path.dirname(script_path)
                script_name = os.path.basename(script_path)
    
                # Set environment variables to specify highest priority handlers
                env_vars = {
                    SageMakerEnvVars.SAGEMAKER_MODEL_PATH: script_dir,
                    SageMakerEnvVars.CUSTOM_SCRIPT_FILENAME: script_name,
                    FastAPIEnvVars.CUSTOM_FASTAPI_PING_HANDLER: (
                        f"{script_name}:env_priority_ping"
                    ),
                    FastAPIEnvVars.CUSTOM_FASTAPI_INVOCATION_HANDLER: (
                        f"{script_name}:env_priority_invoke"
                    ),
                }
    
                args = [
                    "--dtype",
                    "bfloat16",
                    "--max-model-len",
                    "2048",
                    "--enforce-eager",
                    "--max-num-seqs",
                    "32",
                ]
    
>               with RemoteOpenAIServer(
                    MODEL_NAME_SMOLLM, args, env_dict=env_vars
                ) as server:

tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py:705: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a8f65e20>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
----------------------------- Captured stdout call -----------------------------
INFO 02-21 03:15:06 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:15:06 [model.py:1555] Using max model len 2048
INFO 02-21 03:15:06 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 2048 --enforce-eager --max-num-seqs 32 --port 46277 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 'ONEAPI_ROOT': '/opt/intel/oneapi', 'PKG_CONFIG_PATH': '/opt/intel/oneapi/tbb/2022.3/env/../lib/pkgconfig:/opt/intel/oneapi/mpi/2021.17/lib/pkgconfig:/opt/intel/oneapi/mkl/2025.3/lib/pkgconfig:/opt/intel/oneapi/dpl/2022.10/lib/pkgconfig:/opt/intel/oneapi/dnnl/2025.3/lib/pkgconfig:/opt/intel/oneapi/compiler/2025.3/lib/pkgconfig:/opt/intel/oneapi/ccl/2021.17/lib/pkgconfig/', 'HOSTNAME': 'srf797635.jf.intel.com', 'CCL_ROOT': '/opt/intel/oneapi/ccl/2021.17', 'I_MPI_ROOT': '/opt/intel/oneapi/mpi/2021.17', 'FI_PROVIDER_PATH': '/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric', 'DNNLROOT': '/opt/intel/oneapi/dnnl/2025.3', 'DIAGUTIL_PATH': '/opt/intel/oneapi/compiler/2025.3/etc/compiler/sys_check/sys_check.sh', 'UV_PYTHON_INSTALL_DIR': '/opt/uv/python', 'PWD': '/workspace', 'CCL_CONFIGURATION': 'cpu_gpu_dpcpp', 'DPL_ROOT': '/opt/intel/oneapi/dpl/2022.10', 'MANPATH': '/opt/intel/oneapi/mpi/2021.17/share/man:/opt/intel/oneapi/debugger/2025.3/share/man:/opt/intel/oneapi/compiler/2025.3/share/man:', 'UV_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'TCM_ROOT': '/opt/intel/oneapi/tcm/1.4', '_': '/opt/venv/bin/python3', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'HOME': '/root', 'GDB_INFO': '/opt/intel/oneapi/debugger/2025.3/share/info/', 'CCL_CONFIGURATION_PATH': '', 'LANG': 'C.UTF-8', 'VIRTUAL_ENV': '/opt/venv', 'SETVARS_COMPLETED': '1', 'PIP_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'CMAKE_PREFIX_PATH': '/opt/intel/oneapi/tbb/2022.3/env/..:/opt/intel/oneapi/pti/0.16/lib/cmake/pti:/opt/intel/oneapi/mkl/2025.3/lib/cmake:/opt/intel/oneapi/dpl/2022.10/lib/cmake/oneDPL:/opt/intel/oneapi/dnnl/2025.3/lib/cmake:/opt/intel/oneapi/compiler/2025.3:/opt/intel/oneapi/ccl/2021.17/lib/cmake/oneCCL', 'https_proxy': 'http://proxy-dmz.intel.com:912', 'HF_TOKEN': 'HF_TOKEN_REDACTED', 'UV_LINK_MODE': 'copy', 'DPLROOT': '/opt/intel/oneapi/dpl/2022.10', 'CMPLR_ROOT': '/opt/intel/oneapi/compiler/2025.3', 'ZE_AFFINITY_MASK': '0,1', 'Pti_DIR': '/opt/intel/oneapi/pti/0.16/lib/cmake/pti', 'INFOPATH': '/opt/intel/oneapi/debugger/2025.3/share/info', 'UV_INDEX_STRATEGY': 'unsafe-best-match', 'CPLUS_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dpl/2022.10/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UMF_ROOT': '/opt/intel/oneapi/umf/1.0', 'LIBRARY_PATH': '/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/', 'SHLVL': '1', 'OCL_ICD_FILENAMES': '/opt/intel/oneapi/compiler/2025.3/lib/libintelocl.so', 'http_proxy': 'http://proxy-dmz.intel.com:912', 'VLLM_TARGET_DEVICE': 'xpu', 'CLASSPATH': '/opt/intel/oneapi/mpi/2021.17/share/java/mpi.jar', 'LD_LIBRARY_PATH': '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/debugger/2025.3/opt/debugger/lib:/opt/intel/oneapi/compiler/2025.3/opt/compiler/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/:/usr/local/lib/', 'MKLROOT': '/opt/intel/oneapi/mkl/2025.3', 'HF_HUB_OFFLINE': '0', 'NLSPATH': '/opt/intel/oneapi/compiler/2025.3/lib/compiler/locale/%l_%t/%N', 'PATH': '/opt/venv/bin:/root/.local/bin:/opt/intel/oneapi/mpi/2021.17/bin:/opt/intel/oneapi/mkl/2025.3/bin:/opt/intel/oneapi/dev-utilities/2025.3/bin:/opt/intel/oneapi/debugger/2025.3/opt/debugger/bin:/opt/intel/oneapi/compiler/2025.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'INTEL_PYTHONHOME': '/opt/intel/oneapi/debugger/2025.3/opt/debugger', 'C_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UV_HTTP_TIMEOUT': '500', 'CPATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dev-utilities/2025.3/include:/opt/intel/oneapi/ccl/2021.17/include', 'NIXL_VERSION': '0.7.0', 'PYTEST_VERSION': '9.0.2', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'TORCHINDUCTOR_COMPILE_THREADS': '1', 'PYTEST_CURRENT_TEST': 'tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_env_var_priority_over_decorator_and_script (call)', 'SAGEMAKER_MODEL_PATH': '/tmp', 'CUSTOM_SCRIPT_FILENAME': 'tmp0_210oav.py', 'CUSTOM_FASTAPI_PING_HANDLER': 'tmp0_210oav.py:env_priority_ping', 'CUSTOM_FASTAPI_INVOCATION_HANDLER': 'tmp0_210oav.py:env_priority_invoke'}
WARNING 02-21 03:15:14 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:15:14 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:287] 
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:287] 
(APIServer pid=248397) INFO 02-21 03:15:17 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 46277, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 2048, 'enforce_eager': True, 'max_num_seqs': 32}
(APIServer pid=248397) INFO 02-21 03:15:18 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=248397) INFO 02-21 03:15:18 [model.py:1555] Using max model len 2048
(APIServer pid=248397) INFO 02-21 03:15:18 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=248397) INFO 02-21 03:15:18 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=248397) WARNING 02-21 03:15:18 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:15:27 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:15:27 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=248678) INFO 02-21 03:15:29 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=248678) INFO 02-21 03:15:30 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:53183 backend=xccl
(EngineCore_DP0 pid=248678) INFO 02-21 03:15:30 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=248678) ERROR 02-21 03:15:31 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
----------------------------- Captured stderr call -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=248678) Process EngineCore_DP0:
(EngineCore_DP0 pid=248678) Traceback (most recent call last):
(EngineCore_DP0 pid=248678)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=248678)     self.run()
(EngineCore_DP0 pid=248678)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=248678)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=248678)     raise e
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=248678)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=248678)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=248678)     super().__init__(
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=248678)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=248678)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=248678)     self._init_executor()
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=248678)     self.driver_worker.init_device()
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=248678)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=248678)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=248678)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=248678)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=248678)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=248678)     raise ValueError(
(EngineCore_DP0 pid=248678) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=248397) Traceback (most recent call last):
(APIServer pid=248397)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=248397)     sys.exit(main())
(APIServer pid=248397)              ^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=248397)     args.dispatch_function(args)
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=248397)     uvloop.run(run_server(args))
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=248397)     return __asyncio.run(
(APIServer pid=248397)            ^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=248397)     return runner.run(main)
(APIServer pid=248397)            ^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=248397)     return self._loop.run_until_complete(task)
(APIServer pid=248397)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=248397)     return await main
(APIServer pid=248397)            ^^^^^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=248397)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=248397)     async with build_async_engine_client(
(APIServer pid=248397)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=248397)     return await anext(self.gen)
(APIServer pid=248397)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=248397)     async with build_async_engine_client_from_engine_args(
(APIServer pid=248397)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=248397)     return await anext(self.gen)
(APIServer pid=248397)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=248397)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=248397)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=248397)     return cls(
(APIServer pid=248397)            ^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=248397)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=248397)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=248397)     return AsyncMPClient(*client_args)
(APIServer pid=248397)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=248397)     super().__init__(
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=248397)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=248397)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=248397)     next(self.gen)
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=248397)     wait_for_engine_startup(
(APIServer pid=248397)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=248397)     raise RuntimeError(
(APIServer pid=248397) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
--------------------------- Captured stdout teardown ---------------------------
WARNING 02-21 03:15:36 [parallel_state.py:1644] torch._C._host_emptyCache() only available in Pytorch >=2.5
_____ TestMiddlewareIntegration.test_customer_middleware_with_vllm_server ______

self = <tests.entrypoints.sagemaker.test_sagemaker_middleware_integration.TestMiddlewareIntegration object at 0x72b6d3fed7f0>

        @pytest.mark.asyncio
        async def test_customer_middleware_with_vllm_server(self):
            """Test that customer middlewares work with actual vLLM server.
    
            Tests decorator-based middlewares (@custom_middleware, @input_formatter,
            @output_formatter)
            on multiple endpoints (chat/completions, invocations).
            """
            try:
                from model_hosting_container_standards.sagemaker.config import (
                    SageMakerEnvVars,
                )
            except ImportError:
                pytest.skip("model-hosting-container-standards not available")
    
            # Customer writes a middleware script with multiple decorators
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                f.write(
                    """
    from model_hosting_container_standards.common.fastapi.middleware import (
        custom_middleware, input_formatter, output_formatter
    )
    
    # Global flag to track if input formatter was called
    _input_formatter_called = False
    
    @input_formatter
    async def customer_input_formatter(request):
        # Process input - mark that input formatter was called
        global _input_formatter_called
        _input_formatter_called = True
        return request
    
    @custom_middleware("throttle")
    async def customer_throttle_middleware(request, call_next):
        response = await call_next(request)
        response.headers["X-Customer-Throttle"] = "applied"
        order = response.headers.get("X-Middleware-Order", "")
        response.headers["X-Middleware-Order"] = order + "throttle,"
        return response
    
    @output_formatter
    async def customer_output_formatter(response):
        global _input_formatter_called
        response.headers["X-Customer-Processed"] = "true"
        # Since input_formatter and output_formatter are combined into
        # pre_post_process middleware,
        # if output_formatter is called, input_formatter should have been called too
        if _input_formatter_called:
            response.headers["X-Input-Formatter-Called"] = "true"
        order = response.headers.get("X-Middleware-Order", "")
        response.headers["X-Middleware-Order"] = order + "output_formatter,"
        return response
    """
                )
                script_path = f.name
    
            try:
                script_dir = os.path.dirname(script_path)
                script_name = os.path.basename(script_path)
    
                # Set environment variables to point to customer script
                env_vars = {
                    SageMakerEnvVars.SAGEMAKER_MODEL_PATH: script_dir,
                    SageMakerEnvVars.CUSTOM_SCRIPT_FILENAME: script_name,
                }
    
                args = [
                    "--dtype",
                    "bfloat16",
                    "--max-model-len",
                    "2048",
                    "--enforce-eager",
                    "--max-num-seqs",
                    "32",
                ]
    
>               with RemoteOpenAIServer(
                    MODEL_NAME_SMOLLM, args, env_dict=env_vars
                ) as server:

tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a05d4e60>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
----------------------------- Captured stdout call -----------------------------
INFO 02-21 03:16:13 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:16:13 [model.py:1555] Using max model len 2048
INFO 02-21 03:16:13 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 2048 --enforce-eager --max-num-seqs 32 --port 55989 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 'ONEAPI_ROOT': '/opt/intel/oneapi', 'PKG_CONFIG_PATH': '/opt/intel/oneapi/tbb/2022.3/env/../lib/pkgconfig:/opt/intel/oneapi/mpi/2021.17/lib/pkgconfig:/opt/intel/oneapi/mkl/2025.3/lib/pkgconfig:/opt/intel/oneapi/dpl/2022.10/lib/pkgconfig:/opt/intel/oneapi/dnnl/2025.3/lib/pkgconfig:/opt/intel/oneapi/compiler/2025.3/lib/pkgconfig:/opt/intel/oneapi/ccl/2021.17/lib/pkgconfig/', 'HOSTNAME': 'srf797635.jf.intel.com', 'CCL_ROOT': '/opt/intel/oneapi/ccl/2021.17', 'I_MPI_ROOT': '/opt/intel/oneapi/mpi/2021.17', 'FI_PROVIDER_PATH': '/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric', 'DNNLROOT': '/opt/intel/oneapi/dnnl/2025.3', 'DIAGUTIL_PATH': '/opt/intel/oneapi/compiler/2025.3/etc/compiler/sys_check/sys_check.sh', 'UV_PYTHON_INSTALL_DIR': '/opt/uv/python', 'PWD': '/workspace', 'CCL_CONFIGURATION': 'cpu_gpu_dpcpp', 'DPL_ROOT': '/opt/intel/oneapi/dpl/2022.10', 'MANPATH': '/opt/intel/oneapi/mpi/2021.17/share/man:/opt/intel/oneapi/debugger/2025.3/share/man:/opt/intel/oneapi/compiler/2025.3/share/man:', 'UV_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'TCM_ROOT': '/opt/intel/oneapi/tcm/1.4', '_': '/opt/venv/bin/python3', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'HOME': '/root', 'GDB_INFO': '/opt/intel/oneapi/debugger/2025.3/share/info/', 'CCL_CONFIGURATION_PATH': '', 'LANG': 'C.UTF-8', 'VIRTUAL_ENV': '/opt/venv', 'SETVARS_COMPLETED': '1', 'PIP_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'CMAKE_PREFIX_PATH': '/opt/intel/oneapi/tbb/2022.3/env/..:/opt/intel/oneapi/pti/0.16/lib/cmake/pti:/opt/intel/oneapi/mkl/2025.3/lib/cmake:/opt/intel/oneapi/dpl/2022.10/lib/cmake/oneDPL:/opt/intel/oneapi/dnnl/2025.3/lib/cmake:/opt/intel/oneapi/compiler/2025.3:/opt/intel/oneapi/ccl/2021.17/lib/cmake/oneCCL', 'https_proxy': 'http://proxy-dmz.intel.com:912', 'HF_TOKEN': 'HF_TOKEN_REDACTED', 'UV_LINK_MODE': 'copy', 'DPLROOT': '/opt/intel/oneapi/dpl/2022.10', 'CMPLR_ROOT': '/opt/intel/oneapi/compiler/2025.3', 'ZE_AFFINITY_MASK': '0,1', 'Pti_DIR': '/opt/intel/oneapi/pti/0.16/lib/cmake/pti', 'INFOPATH': '/opt/intel/oneapi/debugger/2025.3/share/info', 'UV_INDEX_STRATEGY': 'unsafe-best-match', 'CPLUS_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dpl/2022.10/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UMF_ROOT': '/opt/intel/oneapi/umf/1.0', 'LIBRARY_PATH': '/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/', 'SHLVL': '1', 'OCL_ICD_FILENAMES': '/opt/intel/oneapi/compiler/2025.3/lib/libintelocl.so', 'http_proxy': 'http://proxy-dmz.intel.com:912', 'VLLM_TARGET_DEVICE': 'xpu', 'CLASSPATH': '/opt/intel/oneapi/mpi/2021.17/share/java/mpi.jar', 'LD_LIBRARY_PATH': '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/debugger/2025.3/opt/debugger/lib:/opt/intel/oneapi/compiler/2025.3/opt/compiler/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/:/usr/local/lib/', 'MKLROOT': '/opt/intel/oneapi/mkl/2025.3', 'HF_HUB_OFFLINE': '0', 'NLSPATH': '/opt/intel/oneapi/compiler/2025.3/lib/compiler/locale/%l_%t/%N', 'PATH': '/opt/venv/bin:/root/.local/bin:/opt/intel/oneapi/mpi/2021.17/bin:/opt/intel/oneapi/mkl/2025.3/bin:/opt/intel/oneapi/dev-utilities/2025.3/bin:/opt/intel/oneapi/debugger/2025.3/opt/debugger/bin:/opt/intel/oneapi/compiler/2025.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'INTEL_PYTHONHOME': '/opt/intel/oneapi/debugger/2025.3/opt/debugger', 'C_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UV_HTTP_TIMEOUT': '500', 'CPATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dev-utilities/2025.3/include:/opt/intel/oneapi/ccl/2021.17/include', 'NIXL_VERSION': '0.7.0', 'PYTEST_VERSION': '9.0.2', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'TORCHINDUCTOR_COMPILE_THREADS': '1', 'PYTEST_CURRENT_TEST': 'tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_customer_middleware_with_vllm_server (call)', 'SAGEMAKER_MODEL_PATH': '/tmp', 'CUSTOM_SCRIPT_FILENAME': 'tmpof1psua4.py'}
WARNING 02-21 03:16:22 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:16:22 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:287] 
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:287] 
(APIServer pid=249572) INFO 02-21 03:16:25 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 55989, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 2048, 'enforce_eager': True, 'max_num_seqs': 32}
(APIServer pid=249572) INFO 02-21 03:16:25 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=249572) INFO 02-21 03:16:25 [model.py:1555] Using max model len 2048
(APIServer pid=249572) INFO 02-21 03:16:26 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=249572) INFO 02-21 03:16:26 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=249572) WARNING 02-21 03:16:26 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:16:34 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:16:34 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=249853) INFO 02-21 03:16:37 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=249853) INFO 02-21 03:16:38 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:35999 backend=xccl
(EngineCore_DP0 pid=249853) INFO 02-21 03:16:38 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=249853) ERROR 02-21 03:16:38 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
----------------------------- Captured stderr call -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=249853) Process EngineCore_DP0:
(EngineCore_DP0 pid=249853) Traceback (most recent call last):
(EngineCore_DP0 pid=249853)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=249853)     self.run()
(EngineCore_DP0 pid=249853)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=249853)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=249853)     raise e
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=249853)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=249853)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=249853)     super().__init__(
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=249853)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=249853)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=249853)     self._init_executor()
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=249853)     self.driver_worker.init_device()
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=249853)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=249853)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=249853)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=249853)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=249853)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=249853)     raise ValueError(
(EngineCore_DP0 pid=249853) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=249572) Traceback (most recent call last):
(APIServer pid=249572)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=249572)     sys.exit(main())
(APIServer pid=249572)              ^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=249572)     args.dispatch_function(args)
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=249572)     uvloop.run(run_server(args))
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=249572)     return __asyncio.run(
(APIServer pid=249572)            ^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=249572)     return runner.run(main)
(APIServer pid=249572)            ^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=249572)     return self._loop.run_until_complete(task)
(APIServer pid=249572)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=249572)     return await main
(APIServer pid=249572)            ^^^^^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=249572)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=249572)     async with build_async_engine_client(
(APIServer pid=249572)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=249572)     return await anext(self.gen)
(APIServer pid=249572)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=249572)     async with build_async_engine_client_from_engine_args(
(APIServer pid=249572)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=249572)     return await anext(self.gen)
(APIServer pid=249572)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=249572)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=249572)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=249572)     return cls(
(APIServer pid=249572)            ^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=249572)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=249572)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=249572)     return AsyncMPClient(*client_args)
(APIServer pid=249572)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=249572)     super().__init__(
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=249572)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=249572)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=249572)     next(self.gen)
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=249572)     wait_for_engine_startup(
(APIServer pid=249572)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=249572)     raise RuntimeError(
(APIServer pid=249572) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
--------------------------- Captured stdout teardown ---------------------------
WARNING 02-21 03:16:43 [parallel_state.py:1644] torch._C._host_emptyCache() only available in Pytorch >=2.5
_________ TestMiddlewareIntegration.test_middleware_with_ping_endpoint _________

self = <tests.entrypoints.sagemaker.test_sagemaker_middleware_integration.TestMiddlewareIntegration object at 0x72b6d3fedca0>

        @pytest.mark.asyncio
        async def test_middleware_with_ping_endpoint(self):
            """Test that middlewares work with SageMaker ping endpoint."""
            try:
                from model_hosting_container_standards.sagemaker.config import (
                    SageMakerEnvVars,
                )
            except ImportError:
                pytest.skip("model-hosting-container-standards not available")
    
            # Customer writes a middleware script
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                f.write(
                    """
    from model_hosting_container_standards.common.fastapi.middleware import (
        custom_middleware
    )
    
    @custom_middleware("pre_post_process")
    async def ping_tracking_middleware(request, call_next):
        response = await call_next(request)
        if request.url.path == "/ping":
            response.headers["X-Ping-Tracked"] = "true"
        return response
    """
                )
                script_path = f.name
    
            try:
                script_dir = os.path.dirname(script_path)
                script_name = os.path.basename(script_path)
    
                env_vars = {
                    SageMakerEnvVars.SAGEMAKER_MODEL_PATH: script_dir,
                    SageMakerEnvVars.CUSTOM_SCRIPT_FILENAME: script_name,
                }
    
                args = [
                    "--dtype",
                    "bfloat16",
                    "--max-model-len",
                    "2048",
                    "--enforce-eager",
                    "--max-num-seqs",
                    "32",
                ]
    
>               with RemoteOpenAIServer(
                    MODEL_NAME_SMOLLM, args, env_dict=env_vars
                ) as server:

tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py:233: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b8b1074680>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
----------------------------- Captured stdout call -----------------------------
INFO 02-21 03:16:44 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:16:44 [model.py:1555] Using max model len 2048
INFO 02-21 03:16:44 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 2048 --enforce-eager --max-num-seqs 32 --port 40423 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 'ONEAPI_ROOT': '/opt/intel/oneapi', 'PKG_CONFIG_PATH': '/opt/intel/oneapi/tbb/2022.3/env/../lib/pkgconfig:/opt/intel/oneapi/mpi/2021.17/lib/pkgconfig:/opt/intel/oneapi/mkl/2025.3/lib/pkgconfig:/opt/intel/oneapi/dpl/2022.10/lib/pkgconfig:/opt/intel/oneapi/dnnl/2025.3/lib/pkgconfig:/opt/intel/oneapi/compiler/2025.3/lib/pkgconfig:/opt/intel/oneapi/ccl/2021.17/lib/pkgconfig/', 'HOSTNAME': 'srf797635.jf.intel.com', 'CCL_ROOT': '/opt/intel/oneapi/ccl/2021.17', 'I_MPI_ROOT': '/opt/intel/oneapi/mpi/2021.17', 'FI_PROVIDER_PATH': '/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric', 'DNNLROOT': '/opt/intel/oneapi/dnnl/2025.3', 'DIAGUTIL_PATH': '/opt/intel/oneapi/compiler/2025.3/etc/compiler/sys_check/sys_check.sh', 'UV_PYTHON_INSTALL_DIR': '/opt/uv/python', 'PWD': '/workspace', 'CCL_CONFIGURATION': 'cpu_gpu_dpcpp', 'DPL_ROOT': '/opt/intel/oneapi/dpl/2022.10', 'MANPATH': '/opt/intel/oneapi/mpi/2021.17/share/man:/opt/intel/oneapi/debugger/2025.3/share/man:/opt/intel/oneapi/compiler/2025.3/share/man:', 'UV_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'TCM_ROOT': '/opt/intel/oneapi/tcm/1.4', '_': '/opt/venv/bin/python3', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'HOME': '/root', 'GDB_INFO': '/opt/intel/oneapi/debugger/2025.3/share/info/', 'CCL_CONFIGURATION_PATH': '', 'LANG': 'C.UTF-8', 'VIRTUAL_ENV': '/opt/venv', 'SETVARS_COMPLETED': '1', 'PIP_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'CMAKE_PREFIX_PATH': '/opt/intel/oneapi/tbb/2022.3/env/..:/opt/intel/oneapi/pti/0.16/lib/cmake/pti:/opt/intel/oneapi/mkl/2025.3/lib/cmake:/opt/intel/oneapi/dpl/2022.10/lib/cmake/oneDPL:/opt/intel/oneapi/dnnl/2025.3/lib/cmake:/opt/intel/oneapi/compiler/2025.3:/opt/intel/oneapi/ccl/2021.17/lib/cmake/oneCCL', 'https_proxy': 'http://proxy-dmz.intel.com:912', 'HF_TOKEN': 'HF_TOKEN_REDACTED', 'UV_LINK_MODE': 'copy', 'DPLROOT': '/opt/intel/oneapi/dpl/2022.10', 'CMPLR_ROOT': '/opt/intel/oneapi/compiler/2025.3', 'ZE_AFFINITY_MASK': '0,1', 'Pti_DIR': '/opt/intel/oneapi/pti/0.16/lib/cmake/pti', 'INFOPATH': '/opt/intel/oneapi/debugger/2025.3/share/info', 'UV_INDEX_STRATEGY': 'unsafe-best-match', 'CPLUS_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dpl/2022.10/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UMF_ROOT': '/opt/intel/oneapi/umf/1.0', 'LIBRARY_PATH': '/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/', 'SHLVL': '1', 'OCL_ICD_FILENAMES': '/opt/intel/oneapi/compiler/2025.3/lib/libintelocl.so', 'http_proxy': 'http://proxy-dmz.intel.com:912', 'VLLM_TARGET_DEVICE': 'xpu', 'CLASSPATH': '/opt/intel/oneapi/mpi/2021.17/share/java/mpi.jar', 'LD_LIBRARY_PATH': '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/debugger/2025.3/opt/debugger/lib:/opt/intel/oneapi/compiler/2025.3/opt/compiler/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/:/usr/local/lib/', 'MKLROOT': '/opt/intel/oneapi/mkl/2025.3', 'HF_HUB_OFFLINE': '0', 'NLSPATH': '/opt/intel/oneapi/compiler/2025.3/lib/compiler/locale/%l_%t/%N', 'PATH': '/opt/venv/bin:/root/.local/bin:/opt/intel/oneapi/mpi/2021.17/bin:/opt/intel/oneapi/mkl/2025.3/bin:/opt/intel/oneapi/dev-utilities/2025.3/bin:/opt/intel/oneapi/debugger/2025.3/opt/debugger/bin:/opt/intel/oneapi/compiler/2025.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'INTEL_PYTHONHOME': '/opt/intel/oneapi/debugger/2025.3/opt/debugger', 'C_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UV_HTTP_TIMEOUT': '500', 'CPATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dev-utilities/2025.3/include:/opt/intel/oneapi/ccl/2021.17/include', 'NIXL_VERSION': '0.7.0', 'PYTEST_VERSION': '9.0.2', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'TORCHINDUCTOR_COMPILE_THREADS': '1', 'PYTEST_CURRENT_TEST': 'tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_middleware_with_ping_endpoint (call)', 'SAGEMAKER_MODEL_PATH': '/tmp', 'CUSTOM_SCRIPT_FILENAME': 'tmp2n3hnw1n.py'}
WARNING 02-21 03:16:53 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:16:53 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:287] 
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:287] 
(APIServer pid=250156) INFO 02-21 03:16:55 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 40423, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 2048, 'enforce_eager': True, 'max_num_seqs': 32}
(APIServer pid=250156) INFO 02-21 03:16:56 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=250156) INFO 02-21 03:16:56 [model.py:1555] Using max model len 2048
(APIServer pid=250156) INFO 02-21 03:16:56 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=250156) INFO 02-21 03:16:56 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=250156) WARNING 02-21 03:16:56 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:17:05 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:17:05 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=250437) INFO 02-21 03:17:07 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=250437) INFO 02-21 03:17:08 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:50033 backend=xccl
(EngineCore_DP0 pid=250437) INFO 02-21 03:17:08 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=250437) ERROR 02-21 03:17:08 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
----------------------------- Captured stderr call -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=250437) Process EngineCore_DP0:
(EngineCore_DP0 pid=250437) Traceback (most recent call last):
(EngineCore_DP0 pid=250437)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=250437)     self.run()
(EngineCore_DP0 pid=250437)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=250437)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=250437)     raise e
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=250437)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=250437)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=250437)     super().__init__(
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=250437)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=250437)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=250437)     self._init_executor()
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=250437)     self.driver_worker.init_device()
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=250437)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=250437)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=250437)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=250437)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=250437)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=250437)     raise ValueError(
(EngineCore_DP0 pid=250437) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=250156) Traceback (most recent call last):
(APIServer pid=250156)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=250156)     sys.exit(main())
(APIServer pid=250156)              ^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=250156)     args.dispatch_function(args)
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=250156)     uvloop.run(run_server(args))
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=250156)     return __asyncio.run(
(APIServer pid=250156)            ^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=250156)     return runner.run(main)
(APIServer pid=250156)            ^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=250156)     return self._loop.run_until_complete(task)
(APIServer pid=250156)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=250156)     return await main
(APIServer pid=250156)            ^^^^^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=250156)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=250156)     async with build_async_engine_client(
(APIServer pid=250156)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=250156)     return await anext(self.gen)
(APIServer pid=250156)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=250156)     async with build_async_engine_client_from_engine_args(
(APIServer pid=250156)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=250156)     return await anext(self.gen)
(APIServer pid=250156)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=250156)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=250156)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=250156)     return cls(
(APIServer pid=250156)            ^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=250156)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=250156)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=250156)     return AsyncMPClient(*client_args)
(APIServer pid=250156)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=250156)     super().__init__(
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=250156)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=250156)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=250156)     next(self.gen)
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=250156)     wait_for_engine_startup(
(APIServer pid=250156)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=250156)     raise RuntimeError(
(APIServer pid=250156) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
--------------------------- Captured stdout teardown ---------------------------
WARNING 02-21 03:17:13 [parallel_state.py:1644] torch._C._host_emptyCache() only available in Pytorch >=2.5
__________ TestMiddlewareIntegration.test_middleware_env_var_override __________

self = <tests.entrypoints.sagemaker.test_sagemaker_middleware_integration.TestMiddlewareIntegration object at 0x>

        @pytest.mark.asyncio
        async def test_middleware_env_var_override(self):
            """Test middleware environment variable overrides."""
            try:
                from model_hosting_container_standards.common.fastapi.config import (
                    FastAPIEnvVars,
                )
                from model_hosting_container_standards.sagemaker.config import (
                    SageMakerEnvVars,
                )
            except ImportError:
                pytest.skip("model-hosting-container-standards not available")
    
            # Create a script with middleware functions specified via env vars
            with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
                f.write(
                    """
    from fastapi import Request
    
    # Global flag to track if pre_process was called
    _pre_process_called = False
    
    async def env_throttle_middleware(request, call_next):
        response = await call_next(request)
        response.headers["X-Env-Throttle"] = "applied"
        return response
    
    async def env_pre_process(request: Request) -> Request:
        # Mark that pre_process was called
        global _pre_process_called
        _pre_process_called = True
        return request
    
    async def env_post_process(response):
        global _pre_process_called
        if hasattr(response, 'headers'):
            response.headers["X-Env-Post-Process"] = "applied"
            # Since pre_process and post_process are combined into
            # pre_post_process middleware,
            # if post_process is called, pre_process should have been called too
            if _pre_process_called:
                response.headers["X-Pre-Process-Called"] = "true"
        return response
    """
                )
                script_path = f.name
    
            try:
                script_dir = os.path.dirname(script_path)
                script_name = os.path.basename(script_path)
    
                # Set environment variables for middleware
                # Use script_name with .py extension as per plugin example
                env_vars = {
                    SageMakerEnvVars.SAGEMAKER_MODEL_PATH: script_dir,
                    SageMakerEnvVars.CUSTOM_SCRIPT_FILENAME: script_name,
                    FastAPIEnvVars.CUSTOM_FASTAPI_MIDDLEWARE_THROTTLE: (
                        f"{script_name}:env_throttle_middleware"
                    ),
                    FastAPIEnvVars.CUSTOM_PRE_PROCESS: f"{script_name}:env_pre_process",
                    FastAPIEnvVars.CUSTOM_POST_PROCESS: f"{script_name}:env_post_process",
                }
    
                args = [
                    "--dtype",
                    "bfloat16",
                    "--max-model-len",
                    "2048",
                    "--enforce-eager",
                    "--max-num-seqs",
                    "32",
                ]
    
>               with RemoteOpenAIServer(
                    MODEL_NAME_SMOLLM, args, env_dict=env_vars
                ) as server:

tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py:319: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils.py:194: in __init__
    self._wait_for_server(url=self.url_for("health"), timeout=max_wait_seconds)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tests.utils.RemoteOpenAIServer object at 0x72b6a05d5460>

    def _wait_for_server(self, *, url: str, timeout: float):
        # run health check
        start = time.time()
        client = (
            httpx.Client(transport=httpx.HTTPTransport(uds=self.uds))
            if self.uds
            else requests
        )
        while True:
            try:
                if client.get(url).status_code == 200:
                    break
            except Exception:
                # this exception can only be raised by requests.get,
                # which means the server is not ready yet.
                # the stack trace is not useful, so we suppress it
                # by using `raise from None`.
                result = self._poll()
                if result is not None and result != 0:
>                   raise RuntimeError("Server exited unexpectedly.") from None
E                   RuntimeError: Server exited unexpectedly.

tests/utils.py:304: RuntimeError
----------------------------- Captured stdout call -----------------------------
INFO 02-21 03:17:14 [model.py:531] Resolved architecture: LlamaForCausalLM
INFO 02-21 03:17:14 [model.py:1555] Using max model len 2048
INFO 02-21 03:17:14 [weight_utils.py:579] No model.safetensors.index.json found in remote.
Launching RemoteOpenAIServer with: vllm serve HuggingFaceTB/SmolLM2-135M-Instruct --dtype bfloat16 --max-model-len 2048 --enforce-eager --max-num-seqs 32 --port 33487 --seed 0
Environment variables: {'TBBROOT': '/opt/intel/oneapi/tbb/2022.3/env/..', 'no_proxy': 'localhost,127.0.0.1,10.23.14.76,.intel.com', 'ONEAPI_ROOT': '/opt/intel/oneapi', 'PKG_CONFIG_PATH': '/opt/intel/oneapi/tbb/2022.3/env/../lib/pkgconfig:/opt/intel/oneapi/mpi/2021.17/lib/pkgconfig:/opt/intel/oneapi/mkl/2025.3/lib/pkgconfig:/opt/intel/oneapi/dpl/2022.10/lib/pkgconfig:/opt/intel/oneapi/dnnl/2025.3/lib/pkgconfig:/opt/intel/oneapi/compiler/2025.3/lib/pkgconfig:/opt/intel/oneapi/ccl/2021.17/lib/pkgconfig/', 'HOSTNAME': 'srf797635.jf.intel.com', 'CCL_ROOT': '/opt/intel/oneapi/ccl/2021.17', 'I_MPI_ROOT': '/opt/intel/oneapi/mpi/2021.17', 'FI_PROVIDER_PATH': '/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib/prov:/usr/lib/x86_64-linux-gnu/libfabric', 'DNNLROOT': '/opt/intel/oneapi/dnnl/2025.3', 'DIAGUTIL_PATH': '/opt/intel/oneapi/compiler/2025.3/etc/compiler/sys_check/sys_check.sh', 'UV_PYTHON_INSTALL_DIR': '/opt/uv/python', 'PWD': '/workspace', 'CCL_CONFIGURATION': 'cpu_gpu_dpcpp', 'DPL_ROOT': '/opt/intel/oneapi/dpl/2022.10', 'MANPATH': '/opt/intel/oneapi/mpi/2021.17/share/man:/opt/intel/oneapi/debugger/2025.3/share/man:/opt/intel/oneapi/compiler/2025.3/share/man:', 'UV_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'TCM_ROOT': '/opt/intel/oneapi/tcm/1.4', '_': '/opt/venv/bin/python3', 'VLLM_WORKER_MULTIPROC_METHOD': 'spawn', 'HOME': '/root', 'GDB_INFO': '/opt/intel/oneapi/debugger/2025.3/share/info/', 'CCL_CONFIGURATION_PATH': '', 'LANG': 'C.UTF-8', 'VIRTUAL_ENV': '/opt/venv', 'SETVARS_COMPLETED': '1', 'PIP_EXTRA_INDEX_URL': 'https://download.pytorch.org/whl/xpu', 'CMAKE_PREFIX_PATH': '/opt/intel/oneapi/tbb/2022.3/env/..:/opt/intel/oneapi/pti/0.16/lib/cmake/pti:/opt/intel/oneapi/mkl/2025.3/lib/cmake:/opt/intel/oneapi/dpl/2022.10/lib/cmake/oneDPL:/opt/intel/oneapi/dnnl/2025.3/lib/cmake:/opt/intel/oneapi/compiler/2025.3:/opt/intel/oneapi/ccl/2021.17/lib/cmake/oneCCL', 'https_proxy': 'http://proxy-dmz.intel.com:912', 'HF_TOKEN': 'HF_TOKEN_REDACTED', 'UV_LINK_MODE': 'copy', 'DPLROOT': '/opt/intel/oneapi/dpl/2022.10', 'CMPLR_ROOT': '/opt/intel/oneapi/compiler/2025.3', 'ZE_AFFINITY_MASK': '0,1', 'Pti_DIR': '/opt/intel/oneapi/pti/0.16/lib/cmake/pti', 'INFOPATH': '/opt/intel/oneapi/debugger/2025.3/share/info', 'UV_INDEX_STRATEGY': 'unsafe-best-match', 'CPLUS_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dpl/2022.10/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UMF_ROOT': '/opt/intel/oneapi/umf/1.0', 'LIBRARY_PATH': '/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/', 'SHLVL': '1', 'OCL_ICD_FILENAMES': '/opt/intel/oneapi/compiler/2025.3/lib/libintelocl.so', 'http_proxy': 'http://proxy-dmz.intel.com:912', 'VLLM_TARGET_DEVICE': 'xpu', 'CLASSPATH': '/opt/intel/oneapi/mpi/2021.17/share/java/mpi.jar', 'LD_LIBRARY_PATH': '/opt/venv/lib/python3.12/site-packages/cv2/../../lib64:/opt/intel/oneapi/tcm/1.4/lib:/opt/intel/oneapi/umf/1.0/lib:/opt/intel/oneapi/tbb/2022.3/env/../lib/intel64/gcc4.8:/opt/intel/oneapi/pti/0.16/lib:/opt/intel/oneapi/mpi/2021.17/opt/mpi/libfabric/lib:/opt/intel/oneapi/mpi/2021.17/lib:/opt/intel/oneapi/mkl/2025.3/lib:/opt/intel/oneapi/dnnl/2025.3/lib:/opt/intel/oneapi/debugger/2025.3/opt/debugger/lib:/opt/intel/oneapi/compiler/2025.3/opt/compiler/lib:/opt/intel/oneapi/compiler/2025.3/lib:/opt/intel/oneapi/ccl/2021.17/lib/:/usr/local/lib/', 'MKLROOT': '/opt/intel/oneapi/mkl/2025.3', 'HF_HUB_OFFLINE': '0', 'NLSPATH': '/opt/intel/oneapi/compiler/2025.3/lib/compiler/locale/%l_%t/%N', 'PATH': '/opt/venv/bin:/root/.local/bin:/opt/intel/oneapi/mpi/2021.17/bin:/opt/intel/oneapi/mkl/2025.3/bin:/opt/intel/oneapi/dev-utilities/2025.3/bin:/opt/intel/oneapi/debugger/2025.3/opt/debugger/bin:/opt/intel/oneapi/compiler/2025.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'INTEL_PYTHONHOME': '/opt/intel/oneapi/debugger/2025.3/opt/debugger', 'C_INCLUDE_PATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/tbb/2022.3/env/../include:/opt/intel/oneapi/pti/0.16/include:/opt/intel/oneapi/mpi/2021.17/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dnnl/2025.3/include', 'UV_HTTP_TIMEOUT': '500', 'CPATH': '/opt/intel/oneapi/umf/1.0/include:/opt/intel/oneapi/mkl/2025.3/include:/opt/intel/oneapi/dev-utilities/2025.3/include:/opt/intel/oneapi/ccl/2021.17/include', 'NIXL_VERSION': '0.7.0', 'PYTEST_VERSION': '9.0.2', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'PYTORCH_NVML_BASED_CUDA_CHECK': '1', 'TORCHINDUCTOR_COMPILE_THREADS': '1', 'PYTEST_CURRENT_TEST': 'tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_middleware_env_var_override (call)', 'SAGEMAKER_MODEL_PATH': '/tmp', 'CUSTOM_SCRIPT_FILENAME': 'tmpbmtgjq5u.py', 'CUSTOM_FASTAPI_MIDDLEWARE_THROTTLE': 'tmpbmtgjq5u.py:env_throttle_middleware', 'CUSTOM_PRE_PROCESS': 'tmpbmtgjq5u.py:env_pre_process', 'CUSTOM_POST_PROCESS': 'tmpbmtgjq5u.py:env_post_process'}
WARNING 02-21 03:17:22 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:17:22 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:287] 
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:287]        █     █     █▄   ▄█
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:287]  ▄▄ ▄█ █     █     █ ▀▄▀ █  version 0.1.dev13788+ga2443de5f.d20260220
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:287]   █▄█▀ █     █     █     █  model   HuggingFaceTB/SmolLM2-135M-Instruct
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:287]    ▀▀  ▀▀▀▀▀ ▀▀▀▀▀ ▀     ▀
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:287] 
(APIServer pid=250740) INFO 02-21 03:17:25 [utils.py:223] non-default args: {'model_tag': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'api_server_count': 1, 'port': 33487, 'model': 'HuggingFaceTB/SmolLM2-135M-Instruct', 'dtype': 'bfloat16', 'max_model_len': 2048, 'enforce_eager': True, 'max_num_seqs': 32}
(APIServer pid=250740) INFO 02-21 03:17:26 [model.py:531] Resolved architecture: LlamaForCausalLM
(APIServer pid=250740) INFO 02-21 03:17:26 [model.py:1555] Using max model len 2048
(APIServer pid=250740) INFO 02-21 03:17:26 [scheduler.py:224] Chunked prefill is enabled with max_num_batched_tokens=2048.
(APIServer pid=250740) INFO 02-21 03:17:26 [vllm.py:689] Asynchronous scheduling is enabled.
(APIServer pid=250740) WARNING 02-21 03:17:26 [vllm.py:727] Enforce eager set, overriding optimization level to -O0
WARNING 02-21 03:17:35 [importing.py:53] Triton is installed, but `triton.backends` could not be imported. Disabling Triton.
INFO 02-21 03:17:35 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
(EngineCore_DP0 pid=251021) INFO 02-21 03:17:37 [core.py:97] Initializing a V1 LLM engine (v0.1.dev13788+ga2443de5f.d20260220) with config: model='HuggingFaceTB/SmolLM2-135M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-135M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-135M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False, 'fuse_act_padding': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': None, 'fast_moe_cold_start': True, 'static_all_moe_layers': []}
(EngineCore_DP0 pid=251021) INFO 02-21 03:17:38 [parallel_state.py:1234] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.23.14.76:57131 backend=xccl
(EngineCore_DP0 pid=251021) INFO 02-21 03:17:38 [parallel_state.py:1445] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006] EngineCore failed to start.
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006] Traceback (most recent call last):
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     super().__init__(
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     self._init_executor()
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     self.driver_worker.init_device()
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006]     raise ValueError(
(EngineCore_DP0 pid=251021) ERROR 02-21 03:17:38 [core.py:1006] ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
----------------------------- Captured stderr call -----------------------------
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
Skipping import of cpp extensions due to incompatible torch version 2.10.0+xpu for torchao version 0.14.1+xpu             Please see https://github.com/pytorch/ao/issues/2919 for more info
(EngineCore_DP0 pid=251021) Process EngineCore_DP0:
(EngineCore_DP0 pid=251021) Traceback (most recent call last):
(EngineCore_DP0 pid=251021)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=251021)     self.run()
(EngineCore_DP0 pid=251021)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=251021)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 1010, in run_engine_core
(EngineCore_DP0 pid=251021)     raise e
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 996, in run_engine_core
(EngineCore_DP0 pid=251021)     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
(EngineCore_DP0 pid=251021)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 740, in __init__
(EngineCore_DP0 pid=251021)     super().__init__(
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
(EngineCore_DP0 pid=251021)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=251021)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 103, in __init__
(EngineCore_DP0 pid=251021)     self._init_executor()
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=251021)     self.driver_worker.init_device()
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 322, in init_device
(EngineCore_DP0 pid=251021)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=251021)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/xpu_worker.py", line 97, in init_device
(EngineCore_DP0 pid=251021)     self.requested_memory = request_memory(init_snapshot, self.cache_config)
(EngineCore_DP0 pid=251021)                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=251021)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/worker/utils.py", line 102, in request_memory
(EngineCore_DP0 pid=251021)     raise ValueError(
(EngineCore_DP0 pid=251021) ValueError: Free memory on device xpu:0 (7.09/30.3 GiB) on startup is less than desired GPU memory utilization (0.9, 27.27 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
(APIServer pid=250740) Traceback (most recent call last):
(APIServer pid=250740)   File "/opt/venv/bin/vllm", line 10, in <module>
(APIServer pid=250740)     sys.exit(main())
(APIServer pid=250740)              ^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
(APIServer pid=250740)     args.dispatch_function(args)
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 111, in cmd
(APIServer pid=250740)     uvloop.run(run_server(args))
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
(APIServer pid=250740)     return __asyncio.run(
(APIServer pid=250740)            ^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
(APIServer pid=250740)     return runner.run(main)
(APIServer pid=250740)            ^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
(APIServer pid=250740)     return self._loop.run_until_complete(task)
(APIServer pid=250740)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
(APIServer pid=250740)     return await main
(APIServer pid=250740)            ^^^^^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 471, in run_server
(APIServer pid=250740)     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 490, in run_server_worker
(APIServer pid=250740)     async with build_async_engine_client(
(APIServer pid=250740)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=250740)     return await anext(self.gen)
(APIServer pid=250740)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 96, in build_async_engine_client
(APIServer pid=250740)     async with build_async_engine_client_from_engine_args(
(APIServer pid=250740)   File "/usr/lib/python3.12/contextlib.py", line 210, in __aenter__
(APIServer pid=250740)     return await anext(self.gen)
(APIServer pid=250740)            ^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 137, in build_async_engine_client_from_engine_args
(APIServer pid=250740)     async_llm = AsyncLLM.from_vllm_config(
(APIServer pid=250740)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 222, in from_vllm_config
(APIServer pid=250740)     return cls(
(APIServer pid=250740)            ^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 148, in __init__
(APIServer pid=250740)     self.engine_core = EngineCoreClient.make_async_mp_client(
(APIServer pid=250740)                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 124, in make_async_mp_client
(APIServer pid=250740)     return AsyncMPClient(*client_args)
(APIServer pid=250740)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 835, in __init__
(APIServer pid=250740)     super().__init__(
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 490, in __init__
(APIServer pid=250740)     with launch_core_engines(vllm_config, executor_class, log_stats) as (
(APIServer pid=250740)   File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
(APIServer pid=250740)     next(self.gen)
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 925, in launch_core_engines
(APIServer pid=250740)     wait_for_engine_startup(
(APIServer pid=250740)   File "/opt/venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 984, in wait_for_engine_startup
(APIServer pid=250740)     raise RuntimeError(
(APIServer pid=250740) RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
--------------------------- Captured stdout teardown ---------------------------
WARNING 02-21 03:17:43 [parallel_state.py:1644] torch._C._host_emptyCache() only available in Pytorch >=2.5
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:488
  <frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

../opt/venv/lib/python3.12/site-packages/torch/jit/_script.py:362: 14 warnings
  /opt/venv/lib/python3.12/site-packages/torch/jit/_script.py:362: DeprecationWarning: `torch.jit.script_method` is deprecated. Please switch to `torch.compile` or `torch.export`.
    warnings.warn(

tests/entrypoints/test_ssl_cert_refresher.py::test_ssl_refresher
  /workspace/tests/entrypoints/test_ssl_cert_refresher.py:46: DeprecationWarning: ssl.SSLContext() without protocol argument is deprecated.
    ssl_context = MockSSLContext()

tests/entrypoints/test_ssl_cert_refresher.py::test_ssl_refresher
  /workspace/tests/entrypoints/test_ssl_cert_refresher.py:46: DeprecationWarning: ssl.PROTOCOL_TLS is deprecated
    ssl_context = MockSSLContext()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
------------------ generated xml file: /workspace/results.xml ------------------
=========================== short test summary info ============================
FAILED tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_handler_env_var_override
FAILED tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py::TestHandlerOverrideIntegration::test_env_var_priority_over_decorator_and_script
FAILED tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_customer_middleware_with_vllm_server
FAILED tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_middleware_with_ping_endpoint
FAILED tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py::TestMiddlewareIntegration::test_middleware_env_var_override
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_load_adapter_happy_path
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_unload_adapter_happy_path
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_load_adapter_not_found
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_load_adapter_invalid_files
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_unload_nonexistent_adapter
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_invocations_with_adapter
ERROR tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py::test_sagemaker_multiple_adapters_load_unload
ERROR tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_create_session_badrequest
ERROR tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_close_session_badrequest[nonexistent_session_id-session_id_change0-request_body_change0-session not found]
ERROR tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_close_session_badrequest[malformed_close_request-session_id_change1-request_body_change1-None]
ERROR tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_close_session_invalidrequest
ERROR tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py::test_session
ERROR tests/entrypoints/test_grpc_server.py::test_health_check - RuntimeError...
ERROR tests/entrypoints/test_grpc_server.py::test_get_model_info - RuntimeErr...
ERROR tests/entrypoints/test_grpc_server.py::test_get_server_info - RuntimeEr...
ERROR tests/entrypoints/test_grpc_server.py::test_generate_non_streaming - Ru...
ERROR tests/entrypoints/test_grpc_server.py::test_generate_streaming - Runtim...
ERROR tests/entrypoints/test_grpc_server.py::test_generate_with_different_sampling_params
ERROR tests/entrypoints/test_grpc_server.py::test_generate_with_stop_strings
ERROR tests/entrypoints/test_grpc_server.py::test_generate_multiple_requests
ERROR tests/entrypoints/test_grpc_server.py::test_generate_with_seed - Runtim...
ERROR tests/entrypoints/test_grpc_server.py::test_generate_error_handling - R...
ERROR tests/entrypoints/test_grpc_server.py::test_abort_request - RuntimeErro...
======= 5 failed, 64 passed, 18 warnings, 23 errors in 573.68s (0:09:33) =======
sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
