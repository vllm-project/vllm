# MLA prefill backend comparison
#
# Compares all available MLA prefill backends:
#   FA backends:  fa2, fa3, fa4 (FlashAttention versions)
#   Non-FA:       flashinfer, cudnn, trtllm (Blackwell-only, require flashinfer)
#
# Uses cutlass_mla as the decode backend for impl construction
# (only the prefill path is exercised).
#
# Backends that aren't available on the current platform will report errors
# in the results table (e.g., fa3 on Blackwell, cudnn without artifactory).
#
# Usage:
#   python benchmark.py --config configs/mla_prefill.yaml

description: "MLA prefill backend comparison"

model:
  name: "deepseek-v3"
  num_layers: 60
  num_q_heads: 128
  num_kv_heads: 1
  head_dim: 576
  kv_lora_rank: 512
  qk_nope_head_dim: 128
  qk_rope_head_dim: 64
  v_head_dim: 128
  block_size: 128

batch_specs:
  # Pure prefill (q_len == kv_len)
  - "q512"
  - "q1k"
  - "q2k"
  - "q4k"
  - "q8k"

  # Batched prefill
  - "2q1k"
  - "4q1k"
  - "8q512"

  # Chunked prefill / extend (q_len < kv_len)
  - "q128s1k"
  - "q256s2k"
  - "q512s4k"
  - "q1ks4k"

decode_backends:
  - cutlass_mla

prefill_backends:
  - fa2
  - fa3
  - fa4
  - flashinfer
  - cudnn
  - trtllm

device: "cuda:0"
repeats: 5
warmup_iters: 3
