# Study 2: Does head count matter for FlashAttn MLA vs FlashMLA on Hopper?
# Question: Which backend performs better on Hopper GPUs (SM90+)?
# Question: Does the number of attention heads affect relative performance?

description: "FlashAttn MLA vs FlashMLA head count comparison on Hopper"

# Compare these two Hopper backends
backends:
  - flashattn_mla
  - flashmla

# Comprehensive batch spec matrix: batch sizes Ã— query lengths
# Batch sizes (num requests): 1, 2, 4, 8, 16, 32, 64, 128
# Query lengths: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512
# KV cache length: 1k (fixed)
batch_specs:
  # Batch size: 1
  - "1q1s1k"
  - "1q2s1k"
  - "1q4s1k"
  - "1q8s1k"
  - "1q16s1k"
  - "1q32s1k"
  - "1q64s1k"
  - "1q128s1k"
  - "1q256s1k"
  - "1q512s1k"

  # Batch size: 2
  - "2q1s1k"
  - "2q2s1k"
  - "2q4s1k"
  - "2q8s1k"
  - "2q16s1k"
  - "2q32s1k"
  - "2q64s1k"
  - "2q128s1k"
  - "2q256s1k"
  - "2q512s1k"

  # Batch size: 4
  - "4q1s1k"
  - "4q2s1k"
  - "4q4s1k"
  - "4q8s1k"
  - "4q16s1k"
  - "4q32s1k"
  - "4q64s1k"
  - "4q128s1k"
  - "4q256s1k"
  - "4q512s1k"

  # Batch size: 8
  - "8q1s1k"
  - "8q2s1k"
  - "8q4s1k"
  - "8q8s1k"
  - "8q16s1k"
  - "8q32s1k"
  - "8q64s1k"
  - "8q128s1k"
  - "8q256s1k"
  - "8q512s1k"

  # Batch size: 16
  - "16q1s1k"
  - "16q2s1k"
  - "16q4s1k"
  - "16q8s1k"
  - "16q16s1k"
  - "16q32s1k"
  - "16q64s1k"
  - "16q128s1k"
  - "16q256s1k"
  # - "16q512s1k"

  # Batch size: 32
  - "32q1s1k"
  - "32q2s1k"
  - "32q4s1k"
  - "32q8s1k"
  - "32q16s1k"
  - "32q32s1k"
  - "32q64s1k"
  - "32q128s1k"
  - "32q256s1k"
  # - "32q512s1k"

  # # Batch size: 64
  - "64q1s1k"
  - "64q2s1k"
  - "64q4s1k"
  - "64q8s1k"
  - "64q16s1k"
  - "64q32s1k"
  - "64q64s1k"
  - "64q128s1k"
  - "64q256s1k"
  # - "64q512s1k"

  # # Batch size: 128
  - "128q1s1k"
  - "128q2s1k"
  - "128q4s1k"
  - "128q8s1k"
  - "128q16s1k"
  - "128q32s1k"
  - "128q64s1k"
  - "128q128s1k"
  - "128q256s1k"
  # - "128q512s1k"

# Model configuration
model:
  num_layers: 10
  head_dim: 576        # MLA uses 576
  num_q_heads: 128     # Default value (will be overridden by sweep)
  num_kv_heads: 1      # MLA uses single KV head
  block_size: 64

# Model parameter sweep - test different head counts
model_parameter_sweep:
  param_name: "num_q_heads"
  values: [16, 32, 64, 128, 256]
  label_format: "{backend}_heads_{value}"

# Benchmark settings
benchmark:
  device: "cuda:0"
  repeats: 10
  warmup_iters: 5
  profile_memory: true  # Track memory usage differences

# Output
output:
  csv: "hopper_head_count_results.csv"
  json: "hopper_head_count_results.json"

# Expected outcome:
# - Determine which backend is faster on Hopper
# - Identify if head count impacts relative performance
# - Inform backend selection for DeepSeek V2/V3 models
