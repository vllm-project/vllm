# Study 3: Is FlashInfer-MLA better than CUTLASS MLA after num-splits optimization?
# Question: After optimizing CUTLASS MLA's num_kv_splits, is FlashInfer-MLA still competitive?

description: "FlashInfer-MLA vs optimized CUTLASS MLA comparison"

# Compare these two backends
backends:
  - cutlass_mla
  - flashinfer_mla

# Test various decode workloads
batch_specs:
  - "32q1s1k"     # 32 decode requests, 1k KV cache
  - "64q1s1k"     # 64 decode requests, 1k KV cache
  - "64q1s4k"     # 64 decode requests, 4k KV cache
  - "64q1s16k"    # 64 decode requests, 16k KV cache
  - "128q1s1k"    # 128 decode requests, 1k KV cache
  - "128q1s4k"    # 128 decode requests, 4k KV cache
  - "128q1s16k"   # 128 decode requests, 16k KV cache

# For CUTLASS, test optimized num_kv_splits
# Based on Study 1 results, you may want to adjust these values
parameter_sweep:
  param_name: "num_kv_splits"
  values: [4, 8, 16]  # Often optimal for medium to large batches
  include_auto: true  # Also compare against auto-selection
  label_format: "{backend}_numsplits_{value}"

# Model configuration (DeepSeek V2/V3 defaults)
model:
  num_layers: 10
  head_dim: 576
  num_q_heads: 128
  num_kv_heads: 1
  block_size: 128

# Benchmark settings
benchmark:
  device: "cuda:0"
  repeats: 10
  warmup_iters: 5
  profile_memory: true  # Compare memory efficiency

# Output
output:
  csv: "flashinfer_vs_cutlass_results.csv"
  json: "flashinfer_vs_cutlass_results.json"

# Expected outcome:
# - Determine if FlashInfer-MLA is competitive with optimized CUTLASS
# - Identify which backend to use for different batch sizes
# - Assess memory efficiency trade-offs
# - Inform default backend selection strategy
