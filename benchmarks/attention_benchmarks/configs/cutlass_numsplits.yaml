# Study 1: What is the optimal CUTLASS_MLA num_kv_splits for different batch sizes?

description: "CUTLASS MLA num-splits optimization study"

# Single backend for this study
backend: cutlass_mla

# Fine-grained matrix sweep: batch sizes Ã— sequence lengths
# Batch sizes: 8, 16, 24, 32, 48, 64, 96, 128
# Sequence lengths: 1k, 2k, 4k, 8k, 16k, 32k, 64k
batch_specs:
  # Batch size: 1
  - "1q1s1k"
  - "1q1s2k"
  - "1q1s4k"
  - "1q1s8k"
  - "1q1s16k"
  - "1q1s32k"
  - "1q1s64k"
  - "1q1s128k"

  # Batch size: 2
  - "2q1s1k"
  - "2q1s2k"
  - "2q1s4k"
  - "2q1s8k"
  - "2q1s16k"
  - "2q1s32k"
  - "2q1s64k"
  - "2q1s128k"

  # Batch size: 4
  - "4q1s1k"
  - "4q1s2k"
  - "4q1s4k"
  - "4q1s8k"
  - "4q1s16k"
  - "4q1s32k"
  - "4q1s64k"
  - "4q1s128k"

  # Batch size: 8
  - "8q1s1k"
  - "8q1s2k"
  - "8q1s4k"
  - "8q1s8k"
  - "8q1s16k"
  - "8q1s32k"
  - "8q1s64k"
  - "8q1s128k"

  # Batch size: 16
  - "16q1s1k"
  - "16q1s2k"
  - "16q1s4k"
  - "16q1s8k"
  - "16q1s16k"
  - "16q1s32k"
  - "16q1s64k"
  - "16q1s128k"

  # Batch size: 24
  - "24q1s1k"
  - "24q1s2k"
  - "24q1s4k"
  - "24q1s8k"
  - "24q1s16k"
  - "24q1s32k"
  - "24q1s64k"
  - "24q1s128k"

  # Batch size: 32
  - "32q1s1k"
  - "32q1s2k"
  - "32q1s4k"
  - "32q1s8k"
  - "32q1s16k"
  - "32q1s32k"
  - "32q1s64k"
  - "32q1s128k"

  # Batch size: 48
  - "48q1s1k"
  - "48q1s2k"
  - "48q1s4k"
  - "48q1s8k"
  - "48q1s16k"
  - "48q1s32k"
  - "48q1s64k"
  - "48q1s128k"

  # Batch size: 64
  - "64q1s1k"
  - "64q1s2k"
  - "64q1s4k"
  - "64q1s8k"
  - "64q1s16k"
  - "64q1s32k"
  - "64q1s64k"
  - "64q1s128k"

  # Batch size: 96
  - "96q1s1k"
  - "96q1s2k"
  - "96q1s4k"
  - "96q1s8k"
  - "96q1s16k"
  - "96q1s32k"
  - "96q1s64k"
  - "96q1s128k"

  # Batch size: 128
  - "128q1s1k"
  - "128q1s2k"
  - "128q1s4k"
  - "128q1s8k"
  - "128q1s16k"
  - "128q1s32k"
  - "128q1s64k"
  - "128q1s128k"

# Unified parameter sweep configuration
parameter_sweep:
  param_name: "num_kv_splits"
  values: [1, 2, 4, 8, 16]
  include_auto: true
  label_format: "{backend}_numsplits_{value}"

# Model configuration (DeepSeek V2/V3 defaults)
model:
  num_layers: 10
  head_dim: 576        # MLA uses 576 (kv_lora_rank=512 + 64)
  num_q_heads: 128
  num_kv_heads: 1      # MLA uses single KV head
  block_size: 128

# Benchmark settings
benchmark:
  device: "cuda:0"
  repeats: 20
  warmup_iters: 5
  profile_memory: false

# Output
output:
  csv: "cutlass_numsplits_results.csv"
  json: "cutlass_numsplits_results.json"

# Expected outcome:
# - Identify if auto-selection heuristic is optimal
# - Determine if we should revert PRs #24966, #25509
# - Find optimal num_kv_splits per batch configuration
