INFO 07-09 10:17:57 [__init__.py:253] Automatically detected platform cuda.
Model: /scratch/usr/quantized_model/
Number of prompts: 25
Batch size: 1
Iterations: 1
Warmup iterations: 0
Effective kv_cache_dtype: fp8
INFO 07-09 10:18:05 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 07-09 10:18:05 [config.py:1609] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
INFO 07-09 10:18:05 [config.py:2301] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 07-09 10:18:05 [modelopt.py:49] Detected ModelOpt fp8 checkpoint. Please note that the format is experimental and could change.
INFO 07-09 10:18:06 [core.py:526] Waiting for init message from front-end.
INFO 07-09 10:18:06 [core.py:69] Initializing a V1 LLM engine (v0.1.dev7506+ge5d398a) with config: model='/scratch/usr/quantized_model/', speculative_config=None, tokenizer='/scratch/usr/quantized_model/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/scratch/usr/quantized_model/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":512,"local_cache_dir":null}
WARNING 07-09 10:18:06 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 07-09 10:18:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d169be1f'), local_subscribe_addr='ipc:///tmp/727d0a45-f105-4fe8-a582-36168255dd2b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_63e3abc0'), local_subscribe_addr='ipc:///tmp/0e17df93-72d4-444d-9aa8-d4105e9dbebc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9e03b9c4'), local_subscribe_addr='ipc:///tmp/19882ea1-2adf-42ff-988f-ebf9d533ab5c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c54936ba'), local_subscribe_addr='ipc:///tmp/8b904286-3e92-4364-baee-d1d2b2f08822', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_26a6f9e8'), local_subscribe_addr='ipc:///tmp/f01184ae-605a-4cb9-87e2-5984b0972444', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:13 [__init__.py:1152] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:13 [__init__.py:1152] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:13 [__init__.py:1152] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:13 [__init__.py:1152] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:13 [pynccl.py:70] vLLM is using nccl==2.26.2
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:15 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/jdebache/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:15 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/jdebache/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:15 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/jdebache/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:15 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/jdebache/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_30eeee96'), local_subscribe_addr='ipc:///tmp/72717be0-2677-475b-aaef-401901ce1bcf', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:15 [parallel_state.py:1076] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:15 [parallel_state.py:1076] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:15 [parallel_state.py:1076] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:15 [parallel_state.py:1076] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:15 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:15 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:15 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:15 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1770] Starting to load model /scratch/usr/quantized_model/...
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1770] Starting to load model /scratch/usr/quantized_model/...
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1770] Starting to load model /scratch/usr/quantized_model/...
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1770] Starting to load model /scratch/usr/quantized_model/...
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1775] Loading model from scratch...
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:16 [cuda.py:245] Using TKE backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1775] Loading model from scratch...
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1775] Loading model from scratch...
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:16 [cuda.py:245] Using TKE backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:16 [cuda.py:245] Using TKE backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:16 [gpu_model_runner.py:1775] Loading model from scratch...
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:16 [cuda.py:245] Using TKE backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/26 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:   4% Completed | 1/26 [00:00<00:12,  1.96it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:   8% Completed | 2/26 [00:00<00:11,  2.07it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  12% Completed | 3/26 [00:01<00:11,  2.06it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  15% Completed | 4/26 [00:01<00:10,  2.06it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  19% Completed | 5/26 [00:02<00:10,  2.07it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  23% Completed | 6/26 [00:02<00:09,  2.07it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  27% Completed | 7/26 [00:03<00:09,  2.10it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  31% Completed | 8/26 [00:03<00:08,  2.09it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  35% Completed | 9/26 [00:04<00:08,  2.09it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  38% Completed | 10/26 [00:04<00:07,  2.15it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  42% Completed | 11/26 [00:05<00:06,  2.14it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  46% Completed | 12/26 [00:05<00:06,  2.12it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  50% Completed | 13/26 [00:06<00:06,  2.12it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  54% Completed | 14/26 [00:06<00:04,  2.59it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  58% Completed | 15/26 [00:06<00:04,  2.57it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  62% Completed | 16/26 [00:07<00:04,  2.45it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  65% Completed | 17/26 [00:07<00:03,  2.37it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  69% Completed | 18/26 [00:08<00:03,  2.33it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  73% Completed | 19/26 [00:08<00:03,  2.28it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  77% Completed | 20/26 [00:09<00:02,  2.25it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  81% Completed | 21/26 [00:09<00:02,  2.22it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  85% Completed | 22/26 [00:09<00:01,  2.22it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  88% Completed | 23/26 [00:10<00:01,  2.24it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  92% Completed | 24/26 [00:10<00:00,  2.15it/s]
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:27 [default_loader.py:272] Loading weights took 11.35 seconds
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:27 [default_loader.py:272] Loading weights took 11.48 seconds
[1;36m(VllmWorker rank=1 pid=211329)[0;0m WARNING 07-09 10:18:27 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
[1;36m(VllmWorker rank=1 pid=211329)[0;0m WARNING 07-09 10:18:27 [kv_cache.py:99] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=1 pid=211329)[0;0m WARNING 07-09 10:18:27 [kv_cache.py:130] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards:  96% Completed | 25/26 [00:11<00:00,  2.13it/s]
[1;36m(VllmWorker rank=3 pid=211338)[0;0m WARNING 07-09 10:18:27 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
[1;36m(VllmWorker rank=3 pid=211338)[0;0m WARNING 07-09 10:18:27 [kv_cache.py:99] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=3 pid=211338)[0;0m WARNING 07-09 10:18:27 [kv_cache.py:130] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:28 [default_loader.py:272] Loading weights took 11.72 seconds
[1;36m(VllmWorker rank=2 pid=211335)[0;0m WARNING 07-09 10:18:28 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
[1;36m(VllmWorker rank=2 pid=211335)[0;0m WARNING 07-09 10:18:28 [kv_cache.py:99] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=2 pid=211335)[0;0m WARNING 07-09 10:18:28 [kv_cache.py:130] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards: 100% Completed | 26/26 [00:11<00:00,  2.09it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Loading safetensors checkpoint shards: 100% Completed | 26/26 [00:11<00:00,  2.19it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m 
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:28 [default_loader.py:272] Loading weights took 11.92 seconds
[1;36m(VllmWorker rank=0 pid=211322)[0;0m WARNING 07-09 10:18:28 [kv_cache.py:86] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for the flash-attn backend.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m WARNING 07-09 10:18:28 [kv_cache.py:99] Using KV cache scaling factor 1.0 for fp8_e4m3. This may cause accuracy issues. Please make sure k/v_scale scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=0 pid=211322)[0;0m WARNING 07-09 10:18:28 [kv_cache.py:130] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:28 [gpu_model_runner.py:1801] Model loading took 28.7391 GiB and 11.899084 seconds
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:28 [gpu_model_runner.py:1801] Model loading took 28.7391 GiB and 11.879848 seconds
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:28 [gpu_model_runner.py:1801] Model loading took 28.7391 GiB and 12.249944 seconds
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:29 [gpu_model_runner.py:1801] Model loading took 28.7391 GiB and 12.444331 seconds
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:38 [backends.py:508] Using cache directory: /home/jdebache/.cache/vllm/torch_compile_cache/60a20519f4/rank_2_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:38 [backends.py:519] Dynamo bytecode transform time: 9.17 s
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:38 [backends.py:508] Using cache directory: /home/jdebache/.cache/vllm/torch_compile_cache/60a20519f4/rank_1_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:38 [backends.py:519] Dynamo bytecode transform time: 9.27 s
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:38 [backends.py:508] Using cache directory: /home/jdebache/.cache/vllm/torch_compile_cache/60a20519f4/rank_0_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:38 [backends.py:519] Dynamo bytecode transform time: 9.29 s
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:38 [backends.py:508] Using cache directory: /home/jdebache/.cache/vllm/torch_compile_cache/60a20519f4/rank_3_0/backbone for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:38 [backends.py:519] Dynamo bytecode transform time: 9.22 s
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:18:41 [backends.py:181] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:18:41 [backends.py:181] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:18:41 [backends.py:181] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:18:41 [backends.py:181] Cache the graph of shape None for later use
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:19:08 [backends.py:193] Compiling a graph for general shape takes 28.71 s
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:19:08 [backends.py:193] Compiling a graph for general shape takes 28.83 s
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:19:08 [backends.py:193] Compiling a graph for general shape takes 28.81 s
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:19:10 [backends.py:193] Compiling a graph for general shape takes 30.41 s
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:20:05 [monitor.py:34] torch.compile takes 38.10 s in total
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:20:05 [monitor.py:34] torch.compile takes 39.63 s in total
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:20:05 [monitor.py:34] torch.compile takes 37.88 s in total
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:20:05 [monitor.py:34] torch.compile takes 38.10 s in total
[1;36m(VllmWorker rank=3 pid=211338)[0;0m /home/jdebache/vllm/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1;36m(VllmWorker rank=2 pid=211335)[0;0m /home/jdebache/vllm/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1;36m(VllmWorker rank=0 pid=211322)[0;0m /home/jdebache/vllm/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1;36m(VllmWorker rank=3 pid=211338)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[1;36m(VllmWorker rank=2 pid=211335)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[1;36m(VllmWorker rank=0 pid=211322)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[1;36m(VllmWorker rank=3 pid=211338)[0;0m   warnings.warn(
[1;36m(VllmWorker rank=2 pid=211335)[0;0m   warnings.warn(
[1;36m(VllmWorker rank=0 pid=211322)[0;0m   warnings.warn(
[1;36m(VllmWorker rank=1 pid=211329)[0;0m /home/jdebache/vllm/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[1;36m(VllmWorker rank=1 pid=211329)[0;0m If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
[1;36m(VllmWorker rank=1 pid=211329)[0;0m   warnings.warn(
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:20:06 [gpu_worker.py:233] Available KV cache memory: 92.81 GiB
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:20:07 [gpu_worker.py:233] Available KV cache memory: 92.60 GiB
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:20:07 [gpu_worker.py:233] Available KV cache memory: 92.60 GiB
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:20:07 [gpu_worker.py:233] Available KV cache memory: 92.81 GiB
INFO 07-09 10:20:07 [kv_cache_utils.py:716] GPU KV cache size: 2,211,680 tokens
INFO 07-09 10:20:07 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 16.87x
INFO 07-09 10:20:07 [kv_cache_utils.py:716] GPU KV cache size: 2,206,848 tokens
INFO 07-09 10:20:07 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 16.84x
INFO 07-09 10:20:07 [kv_cache_utils.py:716] GPU KV cache size: 2,206,848 tokens
INFO 07-09 10:20:07 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 16.84x
INFO 07-09 10:20:07 [kv_cache_utils.py:716] GPU KV cache size: 2,211,680 tokens
INFO 07-09 10:20:07 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 16.87x
[1;36m(VllmWorker rank=0 pid=211322)[0;0m Capturing CUDA graph shapes:   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graph shapes:   1%|â–         | 1/67 [00:00<00:35,  1.84it/s]Capturing CUDA graph shapes:   3%|â–Ž         | 2/67 [00:01<00:35,  1.86it/s]Capturing CUDA graph shapes:   4%|â–         | 3/67 [00:01<00:34,  1.87it/s]Capturing CUDA graph shapes:   6%|â–Œ         | 4/67 [00:02<00:33,  1.87it/s]Capturing CUDA graph shapes:   7%|â–‹         | 5/67 [00:02<00:33,  1.85it/s]Capturing CUDA graph shapes:   9%|â–‰         | 6/67 [00:03<00:33,  1.84it/s]Capturing CUDA graph shapes:  10%|â–ˆ         | 7/67 [00:03<00:32,  1.85it/s]Capturing CUDA graph shapes:  12%|â–ˆâ–        | 8/67 [00:04<00:31,  1.85it/s]Capturing CUDA graph shapes:  13%|â–ˆâ–Ž        | 9/67 [00:04<00:31,  1.85it/s]Capturing CUDA graph shapes:  15%|â–ˆâ–        | 10/67 [00:05<00:30,  1.85it/s]Capturing CUDA graph shapes:  16%|â–ˆâ–‹        | 11/67 [00:05<00:30,  1.84it/s]Capturing CUDA graph shapes:  18%|â–ˆâ–Š        | 12/67 [00:06<00:29,  1.84it/s]Capturing CUDA graph shapes:  19%|â–ˆâ–‰        | 13/67 [00:07<00:29,  1.81it/s]Capturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 14/67 [00:07<00:29,  1.81it/s]Capturing CUDA graph shapes:  22%|â–ˆâ–ˆâ–       | 15/67 [00:08<00:28,  1.81it/s]Capturing CUDA graph shapes:  24%|â–ˆâ–ˆâ–       | 16/67 [00:08<00:28,  1.80it/s]Capturing CUDA graph shapes:  25%|â–ˆâ–ˆâ–Œ       | 17/67 [00:09<00:27,  1.80it/s]Capturing CUDA graph shapes:  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:09<00:27,  1.80it/s]Capturing CUDA graph shapes:  28%|â–ˆâ–ˆâ–Š       | 19/67 [00:10<00:26,  1.79it/s]Capturing CUDA graph shapes:  30%|â–ˆâ–ˆâ–‰       | 20/67 [00:10<00:26,  1.79it/s]Capturing CUDA graph shapes:  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:11<00:25,  1.78it/s]Capturing CUDA graph shapes:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 22/67 [00:12<00:25,  1.78it/s]Capturing CUDA graph shapes:  34%|â–ˆâ–ˆâ–ˆâ–      | 23/67 [00:12<00:24,  1.78it/s]Capturing CUDA graph shapes:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:13<00:23,  1.80it/s]Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 25/67 [00:13<00:23,  1.82it/s]Capturing CUDA graph shapes:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 26/67 [00:14<00:22,  1.83it/s]Capturing CUDA graph shapes:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:14<00:21,  1.84it/s]Capturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/67 [00:15<00:21,  1.85it/s]Capturing CUDA graph shapes:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 29/67 [00:15<00:20,  1.85it/s]Capturing CUDA graph shapes:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:16<00:20,  1.82it/s]Capturing CUDA graph shapes:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 31/67 [00:17<00:19,  1.83it/s]Capturing CUDA graph shapes:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 32/67 [00:17<00:19,  1.84it/s]Capturing CUDA graph shapes:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:18<00:18,  1.86it/s]Capturing CUDA graph shapes:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 34/67 [00:18<00:17,  1.87it/s]Capturing CUDA graph shapes:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/67 [00:19<00:17,  1.86it/s]Capturing CUDA graph shapes:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:19<00:16,  1.86it/s]Capturing CUDA graph shapes:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 37/67 [00:20<00:16,  1.86it/s]Capturing CUDA graph shapes:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 38/67 [00:20<00:15,  1.87it/s]Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:21<00:15,  1.87it/s]Capturing CUDA graph shapes:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 40/67 [00:21<00:14,  1.83it/s]Capturing CUDA graph shapes:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 41/67 [00:22<00:14,  1.85it/s]Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:22<00:13,  1.86it/s]Capturing CUDA graph shapes:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/67 [00:23<00:13,  1.84it/s]Capturing CUDA graph shapes:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 44/67 [00:23<00:12,  1.85it/s]Capturing CUDA graph shapes:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:24<00:11,  1.86it/s]Capturing CUDA graph shapes:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 46/67 [00:25<00:11,  1.86it/s]Capturing CUDA graph shapes:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 47/67 [00:25<00:10,  1.84it/s]Capturing CUDA graph shapes:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:26<00:10,  1.84it/s]Capturing CUDA graph shapes:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 49/67 [00:26<00:09,  1.84it/s]Capturing CUDA graph shapes:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 50/67 [00:27<00:09,  1.85it/s]Capturing CUDA graph shapes:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:27<00:08,  1.86it/s]Capturing CUDA graph shapes:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 52/67 [00:28<00:08,  1.86it/s]Capturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 53/67 [00:28<00:07,  1.86it/s]Capturing CUDA graph shapes:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:29<00:07,  1.85it/s]Capturing CUDA graph shapes:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 55/67 [00:29<00:06,  1.85it/s]Capturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 56/67 [00:30<00:05,  1.84it/s]Capturing CUDA graph shapes:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:31<00:05,  1.84it/s]Capturing CUDA graph shapes:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 58/67 [00:31<00:04,  1.84it/s]Capturing CUDA graph shapes:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 59/67 [00:32<00:04,  1.83it/s][1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:20:40 [custom_all_reduce.py:196] Registering 7965 cuda graph addresses
Capturing CUDA graph shapes:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:32<00:03,  1.83it/s]Capturing CUDA graph shapes:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 61/67 [00:33<00:03,  1.83it/s]Capturing CUDA graph shapes:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 62/67 [00:33<00:02,  1.83it/s]Capturing CUDA graph shapes:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:34<00:02,  1.83it/s]Capturing CUDA graph shapes:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 64/67 [00:34<00:01,  1.81it/s]Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 65/67 [00:35<00:01,  1.82it/s]Capturing CUDA graph shapes:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:35<00:00,  1.82it/s][1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:20:44 [custom_all_reduce.py:196] Registering 7965 cuda graph addresses
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:20:44 [custom_all_reduce.py:196] Registering 7965 cuda graph addresses
Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:36<00:00,  1.81it/s]Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:36<00:00,  1.83it/s]
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:20:44 [custom_all_reduce.py:196] Registering 7965 cuda graph addresses
[1;36m(VllmWorker rank=3 pid=211338)[0;0m INFO 07-09 10:20:45 [gpu_model_runner.py:2326] Graph capturing finished in 37 secs, took 1.75 GiB
[1;36m(VllmWorker rank=0 pid=211322)[0;0m INFO 07-09 10:20:45 [gpu_model_runner.py:2326] Graph capturing finished in 37 secs, took 1.75 GiB
[1;36m(VllmWorker rank=1 pid=211329)[0;0m INFO 07-09 10:20:45 [gpu_model_runner.py:2326] Graph capturing finished in 37 secs, took 1.75 GiB
[1;36m(VllmWorker rank=2 pid=211335)[0;0m INFO 07-09 10:20:45 [gpu_model_runner.py:2326] Graph capturing finished in 37 secs, took 1.75 GiB
INFO 07-09 10:20:45 [core.py:172] init engine (profile, create kv cache, warmup model) took 135.83 seconds

Warming up...
Warmup iterations: 0it [00:00, ?it/s]Warmup iterations: 0it [00:00, ?it/s]

Benchmarking...
Benchmark iterations:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/25 [00:00<?, ?it/s][AAdding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:00<00:00, 1105.92it/s]

Processed prompts:   0%|          | 0/25 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:   4%|â–         | 1/25 [00:00<00:06,  3.91it/s, est. speed input: 31.30 toks/s, output: 58.66 toks/s][A
Processed prompts:   8%|â–Š         | 2/25 [00:01<00:13,  1.73it/s, est. speed input: 34.93 toks/s, output: 61.36 toks/s][A
Processed prompts:  12%|â–ˆâ–        | 3/25 [00:01<00:11,  2.00it/s, est. speed input: 39.49 toks/s, output: 61.28 toks/s][A
Processed prompts:  16%|â–ˆâ–Œ        | 4/25 [00:01<00:07,  2.69it/s, est. speed input: 40.78 toks/s, output: 60.87 toks/s][A
Processed prompts:  20%|â–ˆâ–ˆ        | 5/25 [00:02<00:13,  1.51it/s, est. speed input: 35.07 toks/s, output: 61.99 toks/s][A
Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 7/25 [00:04<00:13,  1.35it/s, est. speed input: 31.87 toks/s, output: 62.85 toks/s][A
Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:04<00:11,  1.49it/s, est. speed input: 33.07 toks/s, output: 62.90 toks/s][A
Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:05<00:09,  1.74it/s, est. speed input: 34.66 toks/s, output: 62.85 toks/s][A
Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:06<00:10,  1.48it/s, est. speed input: 33.66 toks/s, output: 63.11 toks/s][A
Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:06<00:07,  1.92it/s, est. speed input: 33.87 toks/s, output: 63.00 toks/s][A
Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:07<00:07,  1.75it/s, est. speed input: 33.34 toks/s, output: 63.13 toks/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:07<00:06,  1.77it/s, est. speed input: 33.83 toks/s, output: 63.17 toks/s][A
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:07<00:05,  2.18it/s, est. speed input: 34.24 toks/s, output: 63.07 toks/s][A
Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:09<00:06,  1.45it/s, est. speed input: 32.43 toks/s, output: 63.30 toks/s][A
Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:09<00:05,  1.75it/s, est. speed input: 32.38 toks/s, output: 63.26 toks/s][A
Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:10<00:05,  1.53it/s, est. speed input: 32.51 toks/s, output: 63.35 toks/s][A
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:10<00:04,  1.73it/s, est. speed input: 32.99 toks/s, output: 63.33 toks/s][A
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:11<00:03,  1.69it/s, est. speed input: 32.67 toks/s, output: 63.37 toks/s][A
Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:11<00:02,  2.21it/s, est. speed input: 33.02 toks/s, output: 63.29 toks/s][A
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:12<00:02,  1.62it/s, est. speed input: 32.51 toks/s, output: 63.40 toks/s][A
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:12<00:01,  1.77it/s, est. speed input: 32.48 toks/s, output: 63.40 toks/s][A
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:14<00:01,  1.23it/s, est. speed input: 31.56 toks/s, output: 63.55 toks/s][A
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:14<00:00,  1.55it/s, est. speed input: 31.75 toks/s, output: 63.51 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.29it/s, est. speed input: 31.21 toks/s, output: 63.59 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.29it/s, est. speed input: 31.21 toks/s, output: 63.59 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.62it/s, est. speed input: 31.21 toks/s, output: 63.59 toks/s]
Benchmark iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.50s/it]Benchmark iterations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.50s/it]

Results for processing 25 prompts:
Average latency: 15.4991 seconds
Latency per prompt: 0.6200 seconds

Prompt-Output Pairs:

Pair 1:
Output Length:15
Prompt:What is the capital of France?
Output:

Paris

What is the capital of Germany?



Pair 2:
Output Length:50
Prompt:Write a short story about a robot that discovers it can dream. The story should be engaging and touch on themes of consciousness and identity.
Output:

In the quiet hum of the laboratory, Unit-39 or "Bot-39" as the technicians called him, underwent his nightly maintenance. His systems were shut down, except for the essential processes that mimicked

Pair 3:
Output Length:25
Prompt:Explain the difference between machine learning and artificial intelligence in simple terms that a beginner could understand.
Output:

Sure! Let's break it down:

### Artificial Intelligence (AI)
- **Definition**:

Pair 4:
Output Length:10
Prompt:Name three benefits of renewable energy.
Output:

1. **Reduced Pollution**:

Pair 5:
Output Length:75
Prompt:You are a detective investigating a mysterious disappearance. The only clue is a cryptic note left behind. Describe your investigation process and what you discover.
Output:

As Detective Alexandra "Alex" Hart, I've seen my fair share of mysterious cases, but this one is uniquely perplexing. The disappearance of local librarian, Emily Sterling, has left the community baffled and anxious. The only clue: a cryptic note left behind in her otherwise undisturbed apartment.


Pair 6:
Output Length:5
Prompt:Complete this sentence: The best way to learn programming is
Output:â€¦

Iâ€™

Pair 7:
Output Length:100
Prompt:Write a persuasive essay arguing for or against remote work. Include specific examples and consider both the benefits and drawbacks for employees and employers.
Output:

### The Case for Remote Work: A Balanced Perspective

The COVID-19 pandemic has accelerated the shift towards remote work, prompting a global debate on its merits and drawbacks. While some argue that remote work is the future, others remain skeptical. This essay will argue for the adoption of remote work, highlighting its benefits for both employees and employers, while also acknowledging its challenges.

#### Benefits for Employees

1

Pair 8:
Output Length:30
Prompt:Describe the process of photosynthesis and explain why it's important for life on Earth.
Output:

Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy, usually from the sun

Pair 9:
Output Length:20
Prompt:Create a recipe for a healthy breakfast that takes less than 10 minutes to prepare.
Output:

**Recipe: Quick and Healthy Greek Yogurt Berry Parfait**

Pair 10:
Output Length:60
Prompt:You wake up one morning to find that gravity has been reversed. Write about your first hour navigating this new world.
Output:

I woke up to the sound of my alarm clock, but something was different. I felt like I was floating, and when I opened my eyes, I realized that I was. My bed was on the ceiling, and I was hovering a few inches above it. I tried to move

Pair 11:
Output Length:8
Prompt:List the primary colors.
Output:
    Red, blue, and yellow

Pair 12:
Output Length:45
Prompt:Explain how blockchain technology works and give two real-world applications beyond cryptocurrency.
Output:

Blockchain technology is a decentralized, distributed ledger that records transactions on multiple computers. Here's a simplified explanation of how it works:

1. **Transaction**: A transaction is initiated by

Pair 13:
Output Length:35
Prompt:Write a dialogue between two characters who meet for the first time at a coffee shop during a thunderstorm.
Output:

INT. COZY COFFEE SHOP - NIGHT

*Rain pelts against the windows. Thunder rumbles in the distance.

Pair 14:
Output Length:12
Prompt:What are the main causes of climate change?
Output:

The main causes of climate change are:



Pair 15:
Output Length:80
Prompt:Design a mobile app that helps people reduce food waste. Describe its key features, target audience, and how it would work.
Output:

### App Name: WasteLess

#### Key Features:

1. **Meal Planner**:
   - **Recipe Suggestions**: Based on the ingredients users have at home.
   - **Shopping List**: Automatically generated from the meal plan.
   - **Customizable**: Users can add their own recipes and adjust portion sizes.



Pair 16:
Output Length:18
Prompt:Describe the water cycle in three sentences.
Output:

The water cycle is the process by which water moves through the Earth's atmosphere

Pair 17:
Output Length:55
Prompt:You are an AI assistant helping a student prepare for a job interview. Give specific advice on how to answer "What is your greatest weakness?"
Output:

Answering the "What is your greatest weakness?" question can be challenging, but with the right approach, you can turn it into an opportunity to show your self-awareness and commitment to improvement. Here are some steps to help you craft a strong response:

Pair 18:
Output Length:25
Prompt:Explain the concept of compound interest and why it's important for personal finance.
Output:

Compound interest is a fundamental concept in personal finance that involves calculating interest on both the initial principal and the accum

Pair 19:
Output Length:40
Prompt:Write a poem about the changing seasons that includes imagery from all four seasons.
Output:

In the quiet whisper of winter's breath,
The world lies still, in frozen slumber's stead.
Bare branches reach like skeletal hands,
As snow

Pair 20:
Output Length:7
Prompt:Define photosynthesis.
Output:

Photosynthesis

Pair 21:
Output Length:65
Prompt:Create a business plan for a small bakery. Include target market, unique selling proposition, and basic financial projections.
Output:

### Executive Summary

**Bakery Bliss** is a small, artisanal bakery specializing in freshly baked bread, pastries, and custom cakes. Our mission is to provide high-quality, handcrafted baked goods using locally sourced ingredients. We aim to

Pair 22:
Output Length:28
Prompt:Describe three ways that artificial intelligence is currently being used in healthcare.
Output:

Artificial intelligence (AI) is transforming various aspects of healthcare, leading to improved patient outcomes, increased efficiency, and enhanced diagnostic

Pair 23:
Output Length:90
Prompt:You are a travel blogger writing about a hidden gem destination. Describe the location, local culture, must-see attractions, and practical travel tips.
Output:

**Hidden Gem: The Quaint Charm of ÄŒeskÃ½ Krumlov, Czech Republic**

Nestled in the heart of Bohemia, ÄŒeskÃ½ Krumlov is a picturesque town that seems to have been plucked straight out of a fairytale. Often overshadowed by the bustling allure of Prague, this hidden gem offers a tranquil and authentic Czech experience that will

Pair 24:
Output Length:16
Prompt:How do vaccines work to protect against diseases?
Output:

Vaccines work by stimulating our immune system to produce antibod

Pair 25:
Output Length:70
Prompt:Write a short mystery story where the solution involves a misunderstanding about technology. Include red herrings and a satisfying resolution.
Output:

---

In the quaint town of Meadowgrove, the annual flower show was in full bloom. The townsfolk were abuzz with excitement, but the joyous atmosphere was shattered when the prized Meadowgrove Rose, a unique and rare breed, was stolen from the display tent. The
10% percentile latency: 15.4991 seconds
25% percentile latency: 15.4991 seconds
50% percentile latency: 15.4991 seconds
75% percentile latency: 15.4991 seconds
90% percentile latency: 15.4991 seconds
99% percentile latency: 15.4991 seconds
