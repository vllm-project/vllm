<!-- 1886f5f8-c538-4179-9dac-0f0940c56205 37533ec7-3a84-451b-b31e-26d0f6b7c235 -->
# KV-Embedded Hybrid SSM – Phase 2 Plan

## Overview

- Implement a KV-embedded representation of SSM state that is updated on sliding-window eviction and read directly by attention kernels.
- Introduce a composite sliding-window+SSM manager, a state-update kernel, and unified-attention wiring while preserving the existing Phase 1 (separate SSM pool) path as a guarded fallback.

## 1. New KV cache spec for hybrid SSM state

- Define `HybridSSMSpec` in `[vllm/v1/kv_cache_interface.py]`:
- Subclass `KVCacheSpec` and add fields: `block_size`, `ssm_state_size`, `page_size_bytes`.
- Implement `max_memory_usage_bytes(...)` assuming one logical SSM state block per active sequence (or per KV group), consistent with how sliding-window groups account for memory.
- Document the relationship between `block_size` (tokens per block) and `ssm_state_size` (state dimension/compressed rank) and how `page_size_bytes` is derived.
- Integrate `HybridSSMSpec` into KV grouping in `[vllm/v1/core/kv_cache_utils.py]`:
- Extend `get_kv_cache_groups` / `get_kv_cache_configs` to accept a hybrid-SSM configuration and construct a corresponding SSM KV group.
- Decide how the SSM group aligns with existing sliding-window groups (e.g., one SSM group per sliding-window group) and ensure `max_memory_usage_bytes` includes both.
- Update `get_kv_cache_config_from_groups` to treat `HybridSSMSpec` as a first-class group alongside existing KV specs.

## 2. Composite Hybrid Sliding-Window + SSM manager

- Implement `HybridSSMManager` in `[vllm/v1/core/single_type_kv_cache_manager.py]`:
- Subclass `SingleTypeKVCacheManager` and parameterize it with `HybridSSMSpec`.
- Use a `BlockPool` to allocate a fixed number of SSM blocks per request (e.g., one per request per group), and track `request_id -> KVCacheBlock` mappings.
- Expose helpers:
- `get_blocks(request_id)` → current SSM block(s) for the request.
- `get_state_ptrs_or_offsets(request_id, ...)` → device pointers/offsets for SSM state regions required by kernels.
- Implement `HybridSlidingWindowManager` (composite manager) in the same module:
- Subclass `SlidingWindowManager` and hold a `HybridSSMManager` instance for the paired SSM group.
- Override/extend initialization to wire in both the sliding-window KV group and its associated SSM group, including any group IDs or indices used elsewhere.
- Provide convenience methods to resolve, for a `(kv_cache_group_id, request_id)`, both the standard KV blocks (for attention) and the SSM block (for state).

## 3. State-update kernel that absorbs evicted blocks

- Add a Triton kernel for state updates (e.g., `hybrid_ssm_state_update.py`) under `[vllm/model_executor/layers/mamba/ops/]` (or a nearby attention-ops directory):
- Pattern its structure after `_state_passing_fwd_kernel` / `_state_passing_fwd` from `mamba/ops/ssd_state_passing.py`.
- Inputs per request/head:
- Pointer(s) to the current SSM state in the SSM KV block.
- Views onto K/V (or projected summaries) for the evicted blocks.
- Optional decay parameters `(A, Δt)` or precomputed coefficients for the SSM.
- Implement update rule:
- For each head, compute `state_new = f(state_old, KV_evicted)` (e.g., `exp(A * Δt) * state_old + encode(K, V)` for linear SSMs) and write back to the SSM block.
- Ensure the kernel supports batched heads and multiple evicted blocks at once for good utilization.
- Implement a host-side wrapper `update_hybrid_ssm_state_from_evicted_blocks(...)` in an appropriate Python module (e.g., `[vllm/model_executor/layers/mamba/hybrid_ssm_utils.py]`):
- Accept `(request_id, evicted_block_ids, ssm_block, model_params)`.
- Use the `KVCacheTensor` layout to construct device views/slices for K and V corresponding to `evicted_block_ids`.
- Marshal SSM state pointers from `HybridSSMManager` and pass them, along with hyperparameters and launch configuration, to the Triton kernel.
- Return any metadata needed for debugging or testing (e.g., number of tokens/blocks processed).

## 4. Hook KV eviction into SSM state update

- Modify `HybridSlidingWindowManager.remove_skipped_blocks` in `[vllm/v1/core/single_type_kv_cache_manager.py]`:
- Determine which concrete block IDs for the sliding-window group will be freed for a given `request_id` and `num_computed_tokens`.
- Before freeing them:
- Collect the list of evicted `KVCacheBlock` IDs and the associated group/layer context.
- Call into `HybridSSMManager.update_state_from_evicted_blocks(request_id, evicted_block_ids, ...)`, which internally invokes `update_hybrid_ssm_state_from_evicted_blocks`.
- Only after the state-update path completes, replace entries in `req_to_blocks` with `null_block` and free them via `block_pool.free_blocks(removed_blocks)`.
- Make the eviction/state-update logic group-aware:
- Track a mapping from sliding-window KV cache group IDs to their corresponding `HybridSSMManager` instances.
- Ensure multi-group scenarios (e.g., different attention types or partitions) route the correct evicted blocks to the matching SSM manager.

## 5. Expose SSM state to the unified attention kernel

- Extend the unified attention Triton kernel (e.g., `kernel_unified_attention_2d` in `[vllm/attention/ops/triton_unified_attention.py]`) to integrate SSM directly:
- Reserve a convention in the block tables, such as using block index `0` as the SSM state block for each sequence and starting real KV history blocks at index `1`.
- At kernel entry, for each sequence/head:
- Load the corresponding SSM state from the SSM block via the block table and/or explicit offsets.
- Incorporate the SSM contribution either as extra sink-like contributions in the attention computation or as an additive term in the value/output accumulation, aligning with Phase 1 behavior but using KV-embedded state.
- Add any required kernel arguments (e.g., SSM projection weights, decay coefficients, offsets) and ensure launch-side code passes them correctly.
- Update the hybrid attention implementation (e.g., `HybridAttentionImpl` or a new `KVEmbeddedHybridAttentionImpl`) in the model executor:
- Remove dependence on a separate SSM state pool; instead, rely on the block table and `HybridSSMManager`-managed SSM block to provide state.
- Keep existing K/V reshape and cache-write logic (e.g., `triton_reshape_and_cache_flash`) but update it as needed to respect the reserved SSM block index convention.
- Ensure compatibility with both Phase 1 and Phase 2 paths so the same model configuration can select behavior via flags.

## 6. Configuration, flags, and migration

- Add a configuration flag (e.g., `enable_hybrid_kv_embedded_ssm: bool = False`) in the appropriate config class (`VllmConfig` or `CacheConfig`):
- Use this flag to decide whether to instantiate `HybridSlidingWindowManager` + `HybridSSMManager` and to enable SSM-aware kernel arguments.
- Keep the existing Phase 1 path (separate SSM pool, separate adapter/kernel) as the default when the flag is `False`.
- Wire the flag through model construction and engine initialization:
- Ensure KV cache spec creation, manager selection, and attention implementation respect the flag.
- Add documentation comments to the config explaining Phase 1 vs Phase 2 behavior and any compatibility constraints.

## 7. Testing and validation for Phase 2

- Low-level correctness tests:
- Add unit tests for the state-update kernel and its Python wrapper:
- Construct small toy SSM states and synthetic K/V blocks; compare kernel output against a Python reference SSM update.
- Add tests for `HybridSlidingWindowManager.remove_skipped_blocks`:
- Simulate sliding-window progression, verify that:
- The state-update path is invoked with the correct evicted block IDs and group context.
- The SSM block content changes as expected (within numerical tolerances).
- Freed blocks are correctly returned to `block_pool` and `req_to_blocks` is updated.
- End-to-end behavior tests:
- Add or extend tests comparing Phase 1 (separate SSM pool) vs Phase 2 (KV-embedded SSM) on the same hybrid models and prompts:
- Check outputs, log-probs, and long-range behavior equivalence within acceptable numerical tolerances.
- Stress tests:
- Very long contexts with aggressive sliding windows to ensure history compression behaves correctly.
- High concurrency workloads to confirm no deadlocks and acceptable latency impact from eviction-driven updates.
- Performance and memory evaluation:
- Add benchmarks or profiling scripts to measure:
- Overhead of eviction-triggered state updates vs Phase 1.
- Overall KV usage and any memory savings due to tighter integration.
- Iterate on kernel launch parameters, block sizes, and SSM block layout (e.g., alignment, vectorization) to keep the overhead low while maintaining numerical fidelity.

### To-dos

- [ ] Define `HybridSSMSpec` and integrate it into KV cache grouping and config utilities.
- [ ] Implement `HybridSSMManager` and composite `HybridSlidingWindowManager` with proper group and request mappings.
- [ ] Add the Triton state-update kernel and Python wrapper to update SSM state from evicted KV blocks.
- [ ] Modify composite sliding-window eviction logic to invoke the SSM state-update path before freeing blocks.
- [ ] Extend unified attention kernels and hybrid attention implementation to read SSM state from KV-embedded blocks.
- [ ] Add configuration flag to toggle KV-embedded SSM and wire it through initialization paths.
- [ ] Implement unit, integration, and performance tests comparing Phase 1 vs Phase 2 behavior and costs.