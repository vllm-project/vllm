# vLLM AWS EC2 Automated Deployment

This example provides Ansible playbooks to automatically deploy vLLM on AWS EC2
GPU instances. The setup creates a vLLM server running the Qwen/Qwen3-0.6B
model accessible via OpenAI-compatible endpoints.

DEMO VIDEO: [Deploy vLLM on AWS in under 10 Minutes!](https://www.youtube.com/watch?v=X00lavDEV9w)

## Overview

The deployment consists of two main playbooks:
1. **AWS Helper** (`aws-helper.yml`) - Interactive tool to help you select existing AWS resources and generate a `vars.yml`
2. **vLLM Installer** (`vllm-installer.yml`) - Automated installation of NVIDIA GPU drivers, Docker, and vLLM

## Prerequisites

### Local Requirements

**Install Ansible**
```bash
pip install ansible
```

NOTE: Ensure you are on Ansible 2.18 or greater.

**Install Required Ansible Collections:**

```bash
ansible-galaxy collection install amazon.aws:10.1.1 community.general:10.4.0 community.docker:4.4.0
```

**NOTE:** The following versions tested amazon.aws 10.1.1, community.general 10.4.0 and community.docker 4.4

### AWS Setup

**1. Install AWS CLI**

Install AWS CLI version 2 using the official installers:

**macOS:**
```bash
curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
```
```bash
sudo installer -pkg AWSCLIV2.pkg -target /
```

**Linux:**
```bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
```
```bash
unzip awscliv2.zip
```
```bash
sudo ./aws/install
```

**Windows:**

Download and run the MSI installer from: https://awscli.amazonaws.com/AWSCLIV2.msi


**NOTE:** [Official AWS setup docs](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)

**2. Configure AWS Credentials**

Create your AWS credentials file at `~/.aws/credentials`. Use either access keys OR session token depending on your setup:

**For IAM User (Access Keys):**
```ini
[default]
aws_access_key_id = AKIA******
aws_secret_access_key = wJa**********
```

**For Temporary/SSO Credentials (Session Token):**
```ini
[default]
aws_access_key_id = ASIA*******
aws_secret_access_key = wJa***********
aws_session_token = IQo*************
```

**3. Test AWS Configuration**
```bash
# Verify your credentials work
aws sts get-caller-identity
```
```bash
# Should return something like:
# {
#     "UserId": "AIDA******",
#     "Account": "12345678910", 
#     "Arn": "arn:aws:iam::12345678910:user/your-username"
# }
```

**4. AWS Service Quotas (IMPORTANT)**
You must request a quota increase for GPU instances before deployment:

- Go to AWS Service Quotas console
- Search for "Running On-Demand G and VT instances"
- Request an increase at the account level
- This typically takes 1-5 business days to process

**WARNING:** Without this quota increase, your EC2 instance launch will fail.

**5. Hugging Face Token**
Get your Hugging Face access token:

1. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Click "Create new token" on top right corner of page.
3. Choose "Read" access type
4. Provide a token name and click "Create token"
5. Copy the generated token (starts with `hf_`)
6. Save the token as it will be needed in the vLLM deployment process.

### AWS Resources Required
- SSH key pair created in your target AWS region
- VPC with public subnet (for external API access)
- Security group allowing inbound traffic on port 8000

**NOTE:** Ensure that your subnet is located in a region that supports G6 EC2 instances.

## Quick Start

### Step 1: Select AWS Resources

Run the interactive helper to browse your existing AWS resources and generate configuration:

```bash
ansible-playbook aws-helper.yml
```

This will show you existing resources in your AWS account and let you select:
- VPC from available options
- Security group (ensure port 8000 is open within your security group)
- Subnet (preferably public for external access)
- SSH key pair

A `vars.yml` file will be generated with your selections.

Your `vars.yml` will look similar to this:

```yaml
# Generated by Ansible Helper Playbook
vm_name: ubuntu-vllm-vm
volume_size: 40
region: us-east-1
ami_id: ami-0c02fb55956c7d316
instance_type: g6.2xlarge
key_name: my-keypair
subnet_id: subnet-0123456789abcdef0
security_group_ids:
  - sg-0987654321fedcba0
ssh_user: ubuntu
key_path: ~/.ssh/my-keypair.pem
```


### Step 2: Deploy vLLM

Export your `HF_TOKEN`:

```
export HF_TOKEN="YOUR_HUGGING_FACE_TOKEN"
```

Run the main deployment playbook:

```bash
ansible-playbook vllm-installer.yml
```

NOTE: You will need to verify and accept the hostâ€™s RSA key fingerprint on first connection.

Replace `YOUR_HUGGING_FACE_TOKEN` with your Hugging Face access token.

### Step 3: Test the API
Once deployment completes, test the vLLM API:

List available models
```bash
curl http://YOUR_EC2_PUBLIC_IP:8000/v1/models
```

Test chat completion
```bash
curl http://YOUR_EC2_PUBLIC_IP:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-0.6B",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Is a hotdog a sandwich?"}
    ]
  }'
```

## Configuration

### Instance Selection

We selected `g6.2xlarge` as the default because it's the most economical option for open source developers to get started with vLLM:
- 1x NVIDIA L4 GPU (24GB GPU memory)
- 8 vCPUs, 32GB RAM
- Good performance-to-cost ratio for experimentation

### Storage Requirements

The deployment requires 40GB minimum storage due to the large vLLM container image size. The `vars.yml` defaults to 40GB but you can increase it if needed for larger models or additional data.

## What Gets Installed

The deployment process:

1. **System Updates** - Updates package cache and installs dependencies
2. **NVIDIA Drivers** - Installs GPU drivers using `ubuntu-drivers`
3. **Docker** - Installs Docker via snap for container management
4. **NVIDIA Container Toolkit** - Enables GPU access in Docker containers
5. **Kernel Modules** - Loads required NVIDIA kernel modules
6. **vLLM Container** - Pulls and starts the vLLM server
7. **Validation** - Tests GPU access and functionality

## Development

### Customizing the Deployment
1. Fork this repository
2. Modify `vllm-installer.yml` for your requirements
3. Update container settings or add additional configuration

### Contributing
- Follow Ansible best practices
- Test changes on your AWS account
- Update documentation for any new features

## License

This project is released under the MIT License, making it freely available for both personal and commercial use.
