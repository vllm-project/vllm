{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnpDAg8qFnOW"
      },
      "source": [
        "# Running DeepSeek-V3.2-Exp with vLLM on NVIDIA GPUs\n",
        "\n",
        "This notebook provides a comprehensive guide on how to run the **DeepSeek-V3.2-Exp** model using vLLM, optimized for NVIDIA's latest Blackwell architecture (GB200, B200).\n",
        "\n",
        "## About DeepSeek-V3.2-Exp\n",
        "\n",
        "**DeepSeek-V3.2-Exp** is an open-weight 685B parameter sparse MoE model.\n",
        "\n",
        "**Quick Facts:**\n",
        "- 685B total params, ~37B active per token | 128K context length | MIT license\n",
        "- **Main Innovation:** Sparse attention = 2-3x faster on long contexts (90% compute reduction at 128K tokens)\n",
        "- **Best For:** Math reasoning (89.3% AIME), coding (Codeforces 2121), long documents, agentic workflows\n",
        "- **Hardware:** Requires 8x GB200/B200 or H200 GPUs\n",
        "\n",
        "**Resources:**\n",
        "- Model card: [deepseek-ai/DeepSeek-V3.2-Exp](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp)\n",
        "- vLLM Recipe: [DeepSeek-V3.2-Exp Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08iybzc0FnOY"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Part 1: Installation and Serving](#Part-1:-Installation-and-Serving)\n",
        "  - [Prerequisites](#Prerequisites)\n",
        "  - [Environment Setup](#Environment-Setup)\n",
        "  - [Installing vLLM](#Installing-vLLM)\n",
        "  - [Launch OpenAI-Compatible Server](#Launch-OpenAI-Compatible-Server)\n",
        "  - [Client Setup](#Client-Setup)\n",
        "- [Part 2: Applications and Examples](#Part-2:-Applications-and-Examples)\n",
        "  - [Long Context Testing](#Long-Context-Testing)\n",
        "  - [Real-World Applications](#Real-World-Applications)\n",
        "  - [Optimization Tips](#Optimization-Tips)\n",
        "- [Conclusion](#Conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7i40CmHFnOY"
      },
      "source": [
        "### Performance Benchmarks\n",
        "\n",
        "DeepSeek-V3.2-Exp achieves **state-of-the-art performance** across multiple domains while being 2-3x faster on long contexts than dense models.\n",
        "\n",
        "#### Key Results\n",
        "\n",
        "| Category | Highlight Benchmark | Score | Notes |\n",
        "|----------|-------------------|-------|-------|\n",
        "| **Mathematics** | AIME 2025 | **89.3%** | Competition-level math |\n",
        "| | GSM8K (5-shot) | **95.91%** | Grade school math |\n",
        "| **Coding** | Codeforces Rating | **2121** | Competitive programming |\n",
        "| | SWE Verified | **67.8%** | Real GitHub issues |\n",
        "| **Agents** | SimpleQA | **97.1%** | Tool use & reasoning |\n",
        "| **Language** | MMLU-Pro | **85.0%** | Graduate knowledge |\n",
        "\n",
        "\n",
        "#### Long Context Performance\n",
        "\n",
        "DeepSeek-V3.2-Exp features **sparse attention** that provides significant speed improvements on long contexts (up to 128K tokens) while maintaining quality. The sparse attention mechanism reduces computational requirements, making it 2-3x faster than dense models on longer sequences.\n",
        "\n",
        "*Benchmark scores from the [DeepSeek-V3.2-Exp model card](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G48QOAlKFnOY"
      },
      "source": [
        "---\n",
        "## Part 1: Installation and Serving\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpCO9AXOFnOY"
      },
      "source": [
        "### Prerequisites\n",
        "\n",
        "Before starting, ensure your environment meets these requirements:\n",
        "\n",
        "#### Hardware Requirements\n",
        "\n",
        "**Minimum Configuration:**\n",
        "- 8x NVIDIA H200 GPUs (141GB HBM3 each)\n",
        "- 2TB+ system RAM\n",
        "- 5TB+ SSD storage for model weights (~1.3TB) and workspace\n",
        "- High-speed GPU interconnect (NVLink 4.0+)\n",
        "\n",
        "\n",
        "#### Software Requirements\n",
        "\n",
        "**Core Dependencies:**\n",
        "- Python 3.12+ (required for DeepGEMM kernels)\n",
        "- CUDA 12.1+ (12.9 recommended for Blackwell)\n",
        "- cuDNN 8.9+\n",
        "- NCCL 2.18+ for multi-GPU communication\n",
        "\n",
        "**Python Packages:**\n",
        "- PyTorch 2.3+ with CUDA support\n",
        "- vLLM 0.10.2rc3+ (custom build for DeepSeek-V3.2)\n",
        "- Transformers 4.38+\n",
        "- OpenAI Python client 1.12+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvdvr485FnOY"
      },
      "source": [
        "### Environment Setup\n",
        "\n",
        "Let's verify your system is ready for DeepSeek-V3.2-Exp:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZy1OBe4FnOY",
        "outputId": "956ea68c-4cd0-43d2-9996-cb890edb3838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SYSTEM INFORMATION\n",
            "======================================================================\n",
            "OS: Linux 6.8.0-79-generic\n",
            "Python: 3.12.11\n",
            "PyTorch: 2.8.0+cu128\n",
            "CUDA available: True\n",
            "CUDA version: 12.8\n",
            "cuDNN version: 91002\n",
            "Number of GPUs: 8\n",
            "\n",
            "======================================================================\n",
            "GPU DETAILS\n",
            "======================================================================\n",
            "\n",
            "GPU[0]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[1]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[2]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[3]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[4]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[5]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[6]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "GPU[7]:\n",
            "  Name: NVIDIA B200\n",
            "  Compute Capability: 10.0\n",
            "  Memory: 191.50 GB\n",
            "  Multi-Processors: 148\n",
            "  Status: âœ… Blackwell architecture - Optimal\n",
            "\n",
            "Total GPU Memory: 1532.03 GB\n",
            "\n",
            "======================================================================\n",
            "NVLINK STATUS\n",
            "======================================================================\n",
            "âœ… NVLink detected - Multi-GPU performance will be optimal\n",
            "\n",
            "======================================================================\n",
            "CONFIGURATION RECOMMENDATIONS\n",
            "======================================================================\n",
            "âœ… Sufficient GPU memory for DeepSeek-V3.2-Exp\n",
            "   Recommended mode: EP/DP (--dp 8 --enable-expert-parallel)\n"
          ]
        }
      ],
      "source": [
        "# GPU environment check\n",
        "import torch\n",
        "import platform\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SYSTEM INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"OS: {platform.system()} {platform.release()}\")\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"GPU DETAILS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_memory_gb = 0\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        memory_gb = props.total_memory / 1e9\n",
        "        total_memory_gb += memory_gb\n",
        "\n",
        "        print(f\"\\nGPU[{i}]:\")\n",
        "        print(f\"  Name: {props.name}\")\n",
        "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
        "        print(f\"  Memory: {memory_gb:.2f} GB\")\n",
        "        print(f\"  Multi-Processors: {props.multi_processor_count}\")\n",
        "\n",
        "        # Check if suitable for DeepSeek-V3.2\n",
        "        if \"H200\" in props.name:\n",
        "            print(f\"  Status: âœ… Hopper architecture - Supported\")\n",
        "        elif \"B200\" in props.name or \"GB200\" in props.name:\n",
        "            print(f\"  Status: âœ… Blackwell architecture - Optimal\")\n",
        "        else:\n",
        "            print(f\"  Status: âš ï¸ May not be supported\")\n",
        "\n",
        "    print(f\"\\nTotal GPU Memory: {total_memory_gb:.2f} GB\")\n",
        "\n",
        "    # Check NVLink connectivity\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"NVLINK STATUS\")\n",
        "    print(\"=\"*70)\n",
        "    try:\n",
        "        nvlink_output = subprocess.check_output(['nvidia-smi', 'nvlink', '--status'],\n",
        "                                                stderr=subprocess.STDOUT, text=True)\n",
        "        print(\"âœ… NVLink detected - Multi-GPU performance will be optimal\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Could not detect NVLink - Performance may be degraded\")\n",
        "\n",
        "    # Recommendations\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CONFIGURATION RECOMMENDATIONS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if total_memory_gb >= 1100:\n",
        "        print(\"âœ… Sufficient GPU memory for DeepSeek-V3.2-Exp\")\n",
        "        print(\"   Recommended mode: EP/DP (--dp 8 --enable-expert-parallel)\")\n",
        "    elif total_memory_gb >= 900:\n",
        "        print(\"âš ï¸ Marginal GPU memory - Consider using FP8 quantization\")\n",
        "        print(\"   Recommended mode: Tensor Parallel (--tensor-parallel-size 8)\")\n",
        "    else:\n",
        "        print(\"âŒ Insufficient GPU memory for full model\")\n",
        "        print(\"   Consider: Smaller model or quantized version\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ CUDA not available - GPU required for this model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLguGRKyFnOZ"
      },
      "source": [
        "### Installing vLLM\n",
        "\n",
        "We'll install vLLM nightly build with DeepGEMM support for optimal performance.\n",
        "\n",
        "**Note:** For the latest installation steps, refer to the [official vLLM DeepSeek-V3.2-Exp Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kU3kDg6LFnOZ"
      },
      "outputs": [],
      "source": [
        "# Install uv (fast Python package installer)\n",
        "!pip install --upgrade pip\n",
        "!pip install uv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKw68mHVFnOZ"
      },
      "source": [
        "#### Step 1: Install vLLM Nightly Build\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-aELAS-FnOa",
        "outputId": "cbe29103-b55f-41bd-ca8a-dfe4572bfb57"
      },
      "outputs": [],
      "source": [
        "# Install vLLM nightly build from wheels\n",
        "!uv pip install vllm --extra-index-url https://wheels.vllm.ai/nightly\n",
        "\n",
        "print(\"âœ… vLLM nightly build installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0_VPKHgFnOa"
      },
      "source": [
        "#### Step 2: Install DeepGEMM Kernels\n",
        "\n",
        "DeepGEMM provides optimized matrix multiplication kernels for DeepSeek models. It is used in two places: MoE and MQA logits computation (necessary for MQA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0QSRXV7FnOa",
        "outputId": "58abbce4-583d-402f-dbcc-c91fc54d2987"
      },
      "outputs": [],
      "source": [
        "# Install DeepGEMM kernels\n",
        "!uv pip install https://wheels.vllm.ai/dsv32/deep_gemm-2.1.0%2B594953a-cp312-cp312-linux_x86_64.whl\n",
        "\n",
        "print(\"âœ… DeepGEMM kernels installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecnmf4ycFnOa"
      },
      "source": [
        "#### Step 3: Install Additional Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5A-nDssFnOa"
      },
      "outputs": [],
      "source": [
        "# Install required Python packages\n",
        "!pip install openai transformers accelerate numpy --quiet\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Installation Complete!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nYou can now proceed to launch the vLLM server.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axar5IztFnOa"
      },
      "source": [
        "### Launch OpenAI-Compatible Server\n",
        "\n",
        "vLLM provides multiple serving modes for DeepSeek-V3.2-Exp. We'll cover both the **recommended EP/DP mode** and the **fallback TP mode**.\n",
        "\n",
        "#### Serving Modes Explained\n",
        "\n",
        "**1. Expert Parallelism + Data Parallelism (EP/DP) - Recommended**\n",
        "- Each GPU runs full expert inference (TP=1)\n",
        "- Experts distributed across GPUs\n",
        "- Data parallelism for batching\n",
        "- **Best performance** but requires stable setup\n",
        "- Command: `-dp 8 --enable-expert-parallel`\n",
        "\n",
        "**2. Tensor Parallelism (TP) - Fallback**\n",
        "- Model sharded across GPUs\n",
        "- More robust, easier to debug\n",
        "- **~20-30% slower** than EP/DP\n",
        "- Command: `--tensor-parallel-size 8`\n",
        "\n",
        "#### Launch Commands\n",
        "\n",
        "**Important:** Run these commands in a **separate terminal**, not in this notebook. The server needs to run continuously in the background.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfHY4n4PFnOa"
      },
      "source": [
        "#### Option 1: EP/DP Mode (Recommended for H200/B200)\n",
        "\n",
        "```bash\n",
        "# For 8x GB200/B200/H200 (or H20) GPUs - Optimal Performance\n",
        "vllm serve deepseek-ai/DeepSeek-V3.2-Exp -dp 8 --enable-expert-parallel\n",
        "```\n",
        "\n",
        "This is the recommended serving mode as the kernels are mainly optimized for TP=1. The command uses:\n",
        "- `-dp 8`: Data parallelism across 8 GPUs  \n",
        "- `--enable-expert-parallel`: Expert parallelism for MoE layers\n",
        "- Default settings optimized for the model\n",
        "\n",
        "**Full command with additional options:**\n",
        "```bash\n",
        "vllm serve deepseek-ai/DeepSeek-V3.2-Exp \\\n",
        "    -dp 8 \\\n",
        "    --enable-expert-parallel \\\n",
        "    --served-model-name deepseek-v32 \\\n",
        "    --host 0.0.0.0 \\\n",
        "    --port 8000 \\\n",
        "    --max-num-seqs 256 \\\n",
        "    --gpu-memory-utilization 0.95 \\\n",
        "    --trust-remote-code\n",
        "```\n",
        "\n",
        "**Note:** If you encounter errors like `CUDA error: invalid configuration argument`, try reducing `--max-num-seqs` to 256 or smaller (default is 1024).\n",
        "\n",
        "#### Option 2: Tensor Parallel Mode (Fallback)\n",
        "\n",
        "```bash\n",
        "# Tensor Parallel - More Robust, works when EP/DP has issues\n",
        "vllm serve deepseek-ai/DeepSeek-V3.2-Exp -tp 8\n",
        "```\n",
        "\n",
        "Use this mode if you hit errors or hangs with EP/DP mode. Simple tensor parallel works and is more robust, but the performance is not optimal.\n",
        "\n",
        "#### Expected Output\n",
        "\n",
        "When the server starts successfully, you should see:\n",
        "\n",
        "```\n",
        "INFO: Started server process\n",
        "INFO: Waiting for application startup.\n",
        "INFO: Application startup complete.\n",
        "INFO: Uvicorn running on http://0.0.0.0:8000\n",
        "```\n",
        "\n",
        "The first startup will take **10-20 minutes** to download the model weights (~1.3TB).\n",
        "\n",
        "**Supported Hardware:** Only Hopper (H100/H200) and Blackwell (B200/GB200) data center GPUs are supported.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4ekGCWFnOa"
      },
      "source": [
        "### Client Setup\n",
        "\n",
        "Once the server is running, connect using the OpenAI Python client:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awl4Wod8FnOa",
        "outputId": "0380748f-e82c-4bf9-8980-4bfbd818eab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to vLLM server at http://127.0.0.1:8000\n",
            "Using model: deepseek-v32\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "# Connect to vLLM server\n",
        "client = OpenAI(\n",
        "    base_url=\"http://127.0.0.1:8000/v1\",\n",
        "    api_key=\"dummy\"  # vLLM doesn't require a real API key\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"deepseek-v32\"\n",
        "\n",
        "print(f\"Connected to vLLM server at http://127.0.0.1:8000\")\n",
        "print(f\"Using model: {MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVRdJQnFnOa"
      },
      "source": [
        "#### Quick Test\n",
        "\n",
        "Let's verify the server is working correctly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_zX_PifFnOa",
        "outputId": "26878f40-b304-4887-833b-0128ea0a2e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MODEL RESPONSE\n",
            "======================================================================\n",
            "Of course! 2 + 2 equals **4**.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Simple test to verify the server is working\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me what 2+2 equals?\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=300\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL RESPONSE\")\n",
        "print(\"=\"*70)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWKY_EGkFnOb"
      },
      "source": [
        "---\n",
        "## Part 2: Applications and Examples\n",
        "\n",
        "### Long Context Testing\n",
        "\n",
        "DeepSeek-V3.2-Exp's sparse attention mechanism truly shines with long contexts. Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvltb4aLFnOb",
        "outputId": "e9cc3d02-3843-4fe1-f686-280cf29244d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Testing with ~1,992 words (~2,656.0 tokens)...\n",
            "\n",
            "======================================================================\n",
            "LONG CONTEXT TEST\n",
            "======================================================================\n",
            "Context Size: ~1,992 words (~2,656.0 tokens)\n",
            "\n",
            "ğŸ’¬ Model Response:\n",
            "Looking at the document, I can see that each sentence mentions a specific topic number from 0 to 9, and this pattern repeats throughout the text.\n",
            "\n",
            "The topics mentioned are: topic 0, topic 1, topic 2, topic 3, topic 4, topic 5, topic 6, topic 7, topic 8, and topic 9.\n",
            "\n",
            "Therefore, there are **10 different topics** discussed in the document.\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Testing with ~7,992 words (~10,656.0 tokens)...\n",
            "\n",
            "======================================================================\n",
            "LONG CONTEXT TEST\n",
            "======================================================================\n",
            "Context Size: ~7,992 words (~10,656.0 tokens)\n",
            "\n",
            "ğŸ’¬ Model Response:\n",
            "Let's look at the pattern in the text.  \n",
            "\n",
            "Each sentence says:  \n",
            "\n",
            "> \"This is sentence number X â€¦ specifically topic Y.\"  \n",
            "\n",
            "Looking at a few examples:  \n",
            "- Sentence 0 â†’ topic 0  \n",
            "- Sentence 1 â†’ topic 1  \n",
            "- Sentence 2 â†’ topic 2  \n",
            "- â€¦  \n",
            "- Sentence 9 â†’ topic 9  \n",
            "- Sentence 10 â†’ topic 0  \n",
            "- Sentence 11 â†’ topic 1  \n",
            "- â€¦  \n",
            "\n",
            "The topics cycle from 0 to 9 repeatedly. That means the only topics mentioned are **0, 1, 2, 3, 4, 5, 6, 7, 8, 9** â€” a total of **10 different topics**.\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Testing with ~16,002 words (~21,336.0 tokens)...\n",
            "\n",
            "======================================================================\n",
            "LONG CONTEXT TEST\n",
            "======================================================================\n",
            "Context Size: ~16,002 words (~21,336.0 tokens)\n",
            "\n",
            "ğŸ’¬ Model Response:\n",
            "The document follows a pattern where sentence number \\( i \\) mentions \"topic \\( i \\mod 10 \\)\".  \n",
            "\n",
            "This means the topics cycle through **0, 1, 2, 3, 4, 5, 6, 7, 8, 9** repeatedly.  \n",
            "\n",
            "Therefore, there are **10 different topics** discussed in total.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def test_long_context(context_length_words: int = 8000):\n",
        "    \"\"\"\n",
        "    Test with long input contexts.\n",
        "    DeepSeek Sparse Attention should handle this efficiently.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a long context document\n",
        "    long_document = \" \".join([\n",
        "        f\"This is sentence number {i} in a very long technical document. \"\n",
        "        f\"It discusses advanced topics in artificial intelligence, specifically topic {i % 10}. \"\n",
        "        f\"The research findings indicate significant improvements in performance metrics. \"\n",
        "        for i in range(context_length_words // 30)  # Approximate word count\n",
        "    ])\n",
        "\n",
        "    # Add a question that requires understanding the full context\n",
        "    question = \"Based on the entire document above, how many different topics are discussed?\"\n",
        "    full_prompt = f\"{long_document}\\n\\n{question}\"\n",
        "\n",
        "    word_count = len(full_prompt.split())\n",
        "    estimated_tokens = word_count // 0.75  # Rough estimate: 1 token â‰ˆ 0.75 words\n",
        "\n",
        "    print(f\"ğŸ“„ Testing with ~{word_count:,} words (~{estimated_tokens:,} tokens)...\")\n",
        "\n",
        "    # Get streaming response\n",
        "    chunks = []\n",
        "    stream = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=150,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        if chunk.choices[0].delta.content:\n",
        "            chunks.append(chunk.choices[0].delta.content)\n",
        "\n",
        "    response_text = \"\".join(chunks)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"LONG CONTEXT TEST\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Context Size: ~{word_count:,} words (~{estimated_tokens:,} tokens)\")\n",
        "    print(f\"\\nğŸ’¬ Model Response:\")\n",
        "    print(response_text)\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Test with increasing context sizes\n",
        "for ctx_len in [2000, 8000, 16000]:\n",
        "    test_long_context(ctx_len)\n",
        "    time.sleep(2)  # Brief pause between tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVMg8yzLFnOb"
      },
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Now let's explore specific applications where DeepSeek-V3.2-Exp excels:\n",
        "\n",
        "#### Application 1: Advanced Mathematical Reasoning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbdt3Dl3FnOb",
        "outputId": "66e0d250-9acd-4c1d-e96c-df001f14d6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "APPLICATION 1: MATHEMATICAL REASONING\n",
            "======================================================================\n",
            "Letâ€™s go step-by-step.\n",
            "\n",
            "---\n",
            "\n",
            "We are told:  \n",
            "\\[\n",
            "a_1 = 1, \\quad a_{n+1} = 2a_n + 3 \\quad \\text{for } n \\ge 1.\n",
            "\\]\n",
            "\n",
            "---\n",
            "\n",
            "**Step 1: Find a pattern or closed form**\n",
            "\n",
            "The recurrence is:\n",
            "\\[\n",
            "a_{n+1} = 2a_n + 3.\n",
            "\\]\n",
            "This is a linear first-order recurrence.  \n",
            "We can solve it by finding the fixed point \\( L \\) such that \\( L = 2L + 3 \\), i.e. \\( L = -3 \\).\n",
            "\n",
            "Now define \\( b_n = a_n - L = a_n + 3 \\). Then:\n",
            "\\[\n",
            "a_{n+1} + 3 = 2a_n + 3 + 3 = 2a_n + 6 = 2(a_n + 3).\n",
            "\\]\n",
            "So:\n",
            "\\[\n",
            "b_{n+1} = 2 b_n.\n",
            "\\]\n",
            "Thus \\( b_n \\) is geometric:\n",
            "\\[\n",
            "b_n = b_1 \\cdot 2^{n-1}.\n",
            "\\]\n",
            "We have \\( b_1 = a_1 + 3 = 1 + 3 = 4 \\).\n",
            "\n",
            "So:\n",
            "\\[\n",
            "b_n = 4 \\cdot 2^{n-1} = 2^{n+1}.\n",
            "\\]\n",
            "Therefore:\n",
            "\\[\n",
            "a_n = b_n - 3 = 2^{n+1} - 3.\n",
            "\\]\n",
            "\n",
            "---\n",
            "\n",
            "**Step 2: Verify for small \\( n \\)**\n",
            "\n",
            "- \\( n = 1 \\): \\( a_1 = 2^{2} - 3 = 4 - 3 = 1 \\) âœ“  \n",
            "- \\( n = 2 \\): \\( a_2 = 2^{3} - 3 = 8 - 3 = 5 \\), check via recurrence: \\( a_2 = 2a_1 + 3 = 2 + 3 = 5 \\) âœ“  \n",
            "- \\( n = 3 \\): \\( a_3 = 2^{4} - 3 = 16 - 3 = 13 \\), check: \\( a_3 = 2a_2 + 3 = 10 + 3 = 13 \\) âœ“  \n",
            "\n",
            "So the closed form is correct.\n",
            "\n",
            "---\n",
            "\n",
            "**Step 3: Compute \\( a_{10} \\)**\n",
            "\n",
            "\\[\n",
            "a_{10} = 2^{10+1} - 3 = 2^{11} - 3.\n",
            "\\]\n",
            "\\[\n",
            "2^{11} = 2048.\n",
            "\\]\n",
            "\\[\n",
            "a_{10} = 2048 - 3 = 2045.\n",
            "\\]\n",
            "\n",
            "---\n",
            "\n",
            "**Step 4: Verify by recurrence up to \\( n=10 \\) (quick check)**\n",
            "\n",
            "We can compute a few more terms to be sure:\n",
            "\n",
            "\\( a_4 = 2a_3 + 3 = 26 + 3 = 29 \\), closed form: \\( 2^{5} - 3 = 32 - 3 = 29 \\) âœ“  \n",
            "\\( a_5 = 2a_4 + 3 = 58 + 3 = 61 \\), closed form: \\( 2^{6} - 3 = 64 - 3 = 61 \\) âœ“  \n",
            "\\( a_6 = 125 \\), \\( a_7 = 253 \\), \\( a_8 = 509 \\), \\( a_9 = 1021 \\), \\( a_{10} = 2 \\cdot 1021 + 3 = 2042 + 3 = 2045 \\) âœ“\n",
            "\n",
            "---\n",
            "\n",
            "**Final answer:**\n",
            "\\[\n",
            "\\boxed{2045}\n",
            "\\]\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test mathematical reasoning (AIME-level)\n",
        "math_prompt = \"\"\"\n",
        "Solve this problem step by step:\n",
        "\n",
        "A sequence is defined by aâ‚ = 1 and aâ‚™â‚Šâ‚ = 2aâ‚™ + 3 for n â‰¥ 1.\n",
        "Find aâ‚â‚€.\n",
        "\n",
        "Show your work clearly and verify your answer.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert mathematician. Solve problems step-by-step with clear reasoning.\"},\n",
        "        {\"role\": \"user\", \"content\": math_prompt}\n",
        "    ],\n",
        "    temperature=0.3,  # Lower temperature for precise reasoning\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"APPLICATION 1: MATHEMATICAL REASONING\")\n",
        "print(\"=\"*70)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mHrDeqeFnOb"
      },
      "source": [
        "#### Application 2: Code Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEfw8kBBFnOb",
        "outputId": "d7e52aea-9f16-4094-adde-ac5285162202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "APPLICATION 2: CODE GENERATION\n",
            "======================================================================\n",
            "```python\n",
            "def reverse_string(text: str) -> str:\n",
            "    \"\"\"\n",
            "    Reverse the input string.\n",
            "    \n",
            "    Args:\n",
            "        text (str): The string to be reversed\n",
            "        \n",
            "    Returns:\n",
            "        str: The reversed string\n",
            "        \n",
            "    Examples:\n",
            "        >>> reverse_string(\"hello\")\n",
            "        'olleh'\n",
            "        >>> reverse_string(\"Python\")\n",
            "        'nohtyP'\n",
            "        >>> reverse_string(\"\")\n",
            "        ''\n",
            "        >>> reverse_string(\"a\")\n",
            "        'a'\n",
            "    \"\"\"\n",
            "    return text[::-1]\n",
            "```\n",
            "\n",
            "This function:\n",
            "\n",
            "1. **Uses type hints**: `text: str` specifies the input type, and `-> str` specifies the return type\n",
            "2. **Has a comprehensive docstring** that includes:\n",
            "   - Brief description\n",
            "   - Args section explaining the parameter\n",
            "   - Returns section explaining the return value\n",
            "   - Examples showing usage with different cases\n",
            "\n",
            "3. **Implements the reversal efficiently** using Python's slice notation `[::-1]` which:\n",
            "   - Starts at the end of the string\n",
            "   - Goes to the beginning\n",
            "   - Steps backwards one character at a time\n",
            "\n",
            "4. **Handles edge cases**:\n",
            "   - Empty strings return empty strings\n",
            "   - Single character strings return themselves\n",
            "   - Works with any valid Python string\n",
            "\n",
            "Alternative implementation using `reversed()` and `join()`:\n",
            "```python\n",
            "def reverse_string(text: str) -> str:\n",
            "    \"\"\"\n",
            "    Reverse the input string.\n",
            "    \n",
            "    Args:\n",
            "        text (str): The string to be reversed\n",
            "        \n",
            "    Returns:\n",
            "        str: The reversed string\n",
            "    \"\"\"\n",
            "    return ''.join(reversed(text))\n",
            "```\n",
            "\n",
            "Both implementations are valid, but the slice notation is generally more Pythonic and slightly more efficient for this specific use case.\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test coding capabilities\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function to reverse a string. Include type hints and a docstring.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_NAME,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert Python developer. Write clean, efficient, well-documented code.\"},\n",
        "        {\"role\": \"user\", \"content\": code_prompt}\n",
        "    ],\n",
        "    temperature=0.4,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"APPLICATION 2: CODE GENERATION\")\n",
        "print(\"=\"*70)\n",
        "print(response.choices[0].message.content)\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77xB7uN_FnOb"
      },
      "source": [
        "#### Application 3: Multi-lingual Understanding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_ZCgRJKFnOb",
        "outputId": "39b5369d-e633-47ad-985c-c0bc2d73f134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "APPLICATION 3: MULTILINGUAL CAPABILITIES\n",
            "======================================================================\n",
            "\n",
            "ğŸŒ English:\n",
            "Prompt: Explain how neural networks learn in 3 sentences.\n",
            "Response: Neural networks learn by iteratively adjusting their internal parameters, called weights and biases, to minimize the difference between their predictions and the actual correct answers. This adjustment process is guided by an optimization algorithm, typically a variant of gradient descent, which calculates how to change the weights to reduce error using the chain rule from calculus (a technique called backpropagation). Over many cycles of processing data and making these small corrections, the network's configuration gradually improves, allowing it to model complex patterns and relationships within the data.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸŒ ä¸­æ–‡ (Chinese):\n",
            "Prompt: ç”¨ä¸‰å¥è¯è§£é‡Šç¥ç»ç½‘ç»œå¦‚ä½•å­¦ä¹ ã€‚\n",
            "Response: ç¥ç»ç½‘ç»œé€šè¿‡å¤§é‡æ•°æ®æ ·æœ¬ä¸æ–­è°ƒæ•´å†…éƒ¨å‚æ•°ï¼Œä½¿é¢„æµ‹ç»“æœé€æ¸æ¥è¿‘çœŸå®å€¼ã€‚æ¯æ¬¡é¢„æµ‹è¯¯å·®ä¼šé€šè¿‡åå‘ä¼ æ’­ç®—æ³•ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚ä¼ é€’ï¼ŒæŒ‡å¯¼å„å±‚å‚æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚ç»è¿‡åå¤è¿­ä»£ï¼Œç½‘ç»œæœ€ç»ˆè‡ªåŠ¨æå–æ•°æ®å…³é”®ç‰¹å¾ï¼Œå½¢æˆä»è¾“å…¥åˆ°è¾“å‡ºçš„ç²¾å‡†æ˜ å°„èƒ½åŠ›ã€‚\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸŒ EspaÃ±ol (Spanish):\n",
            "Prompt: Explica cÃ³mo aprenden las redes neuronales en 3 frases.\n",
            "Response: Las redes neuronales aprenden ajustando progresivamente los pesos de las conexiones entre sus neuronas. Estos ajustes se realizan mediante algoritmos como el descenso de gradiente, que minimiza la diferencia entre las predicciones de la red y los resultados reales. A travÃ©s de este proceso iterativo en grandes volÃºmenes de datos, la red descubre y refina patrones para realizar su tarea con mayor precisiÃ³n.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ğŸŒ FranÃ§ais (French):\n",
            "Prompt: Expliquez en 3 phrases comment les rÃ©seaux neuronaux apprennent.\n",
            "Response: Les rÃ©seaux neuronaux apprennent en ajustant progressivement les poids des connexions entre leurs neurones. Ils comparent leurs prÃ©dictions aux rÃ©sultats attendus pour calculer une erreur, puis utilisent la rÃ©tropropagation pour dÃ©terminer comment modifier chaque poids afin de rÃ©duire cette erreur. Ce processus itÃ©ratif, alimentÃ© par de grandes quantitÃ©s de donnÃ©es, permet au rÃ©seau d'affiner ses paramÃ¨tres pour amÃ©liorer sa prÃ©cision.\n",
            "----------------------------------------------------------------------\n",
            "======================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test multilingual understanding\n",
        "multilingual_prompts = [\n",
        "    (\"English\", \"Explain how neural networks learn in 3 sentences.\"),\n",
        "    (\"ä¸­æ–‡ (Chinese)\", \"ç”¨ä¸‰å¥è¯è§£é‡Šç¥ç»ç½‘ç»œå¦‚ä½•å­¦ä¹ ã€‚\"),\n",
        "    (\"EspaÃ±ol (Spanish)\", \"Explica cÃ³mo aprenden las redes neuronales en 3 frases.\"),\n",
        "    (\"FranÃ§ais (French)\", \"Expliquez en 3 phrases comment les rÃ©seaux neuronaux apprennent.\")\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"APPLICATION 3: MULTILINGUAL CAPABILITIES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for language, prompt in multilingual_prompts:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.6,\n",
        "        max_tokens=200\n",
        "    )\n",
        "    print(f\"\\nğŸŒ {language}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response.choices[0].message.content}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4aHzEbEFnOf"
      },
      "source": [
        "### Optimization Tips\n",
        "\n",
        "Here are production-ready optimization strategies for DeepSeek-V3.2-Exp on H200/B200:\n",
        "\n",
        "#### 1. Parallelism Strategy Selection\n",
        "\n",
        "**EP/DP Mode (Recommended for Production)**\n",
        "```bash\n",
        "# Best Performance - Use this for H200/B200 deployments\n",
        "vllm serve deepseek-ai/DeepSeek-V3.2-Exp -dp 8 --enable-expert-parallel\n",
        "```\n",
        "**When to use:**\n",
        "- Production deployments\n",
        "- Maximum throughput needed\n",
        "- Stable hardware setup\n",
        "- H200/B200/H20 GPUs\n",
        "\n",
        "**Why EP/DP is recommended:** The kernels are mainly optimized for TP=1, so it's best to run this model under EP/DP mode (DP=8, EP=8, TP=1).\n",
        "\n",
        "**Tensor Parallel Mode (Fallback)**\n",
        "```bash\n",
        "# More Stable - Use for debugging or if EP/DP has issues\n",
        "vllm serve deepseek-ai/DeepSeek-V3.2-Exp -tp 8\n",
        "```\n",
        "**When to use:**\n",
        "- Initial testing and debugging\n",
        "- EP/DP mode encounters errors or hangs\n",
        "- Need maximum stability\n",
        "- Performance is not optimal but more robust\n",
        "\n",
        "#### 2. DeepGEMM Configuration\n",
        "\n",
        "DeepGEMM is used in two places: MoE and MQA logits computation. It is necessary for MQA logits computation.\n",
        "\n",
        "```bash\n",
        "# Disable DeepGEMM for MoE (some users report better performance on H20)\n",
        "VLLM_USE_DEEP_GEMM=0 vllm serve deepseek-ai/DeepSeek-V3.2-Exp -dp 8 --enable-expert-parallel\n",
        "```\n",
        "\n",
        "**Benefits of disabling DeepGEMM:**\n",
        "- Better performance on some GPUs (e.g., H20)\n",
        "- Skips the long warmup period\n",
        "- Still uses DeepGEMM for MQA (necessary part)\n",
        "\n",
        "#### 3. KV Cache Configuration\n",
        "\n",
        "**For Short Contexts (<2K tokens):**\n",
        "```bash\n",
        "--kv-cache-dtype bfloat16\n",
        "```\n",
        "- Less quantization overhead\n",
        "- Better latency for short requests\n",
        "- Uses more GPU memory\n",
        "\n",
        "**For Long Contexts (>2K tokens) - Default:**\n",
        "```bash\n",
        "# FP8 is default (custom fp8 kvcache), no flag needed\n",
        "# Or explicitly set:\n",
        "--kv-cache-dtype fp8\n",
        "```\n",
        "- Saves GPU memory\n",
        "- Allows more tokens to be cached\n",
        "- Better for long documents\n",
        "- Incurs additional quantization/dequantization overhead\n",
        "\n",
        "**Recommendation:** Use `bfloat16` for short requests and `fp8` for long requests.\n",
        "\n",
        "#### 4. Batch Size Tuning\n",
        "\n",
        "```bash\n",
        "# Default (may cause errors with large batches)\n",
        "# Default is 1024\n",
        "\n",
        "# Recommended if you see CUDA errors\n",
        "--max-num-seqs 256\n",
        "\n",
        "# Conservative (if seeing OOM errors)\n",
        "--max-num-seqs 128\n",
        "```\n",
        "\n",
        "**Note:** If you encounter errors like `CUDA error (flashmla-src/csrc/smxx/mla_combine.cu:201): invalid configuration argument`, it might be caused by too large batch size. Try with `--max-num-seqs 256` or smaller.\n",
        "\n",
        "#### 5. Memory Management\n",
        "\n",
        "```bash\n",
        "# Aggressive memory use (recommended for dedicated servers)\n",
        "--gpu-memory-utilization 0.95\n",
        "\n",
        "# Conservative (if running other workloads)\n",
        "--gpu-memory-utilization 0.85\n",
        "\n",
        "# Very conservative (shared GPU environment)\n",
        "--gpu-memory-utilization 0.75\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH7VvPRGFnOf"
      },
      "source": [
        "---\n",
        "## Conclusion\n",
        "\n",
        "### Summary\n",
        "\n",
        "Congratulations! You've successfully set up and benchmarked **DeepSeek-V3.2-Exp** with vLLM on NVIDIA GB200/B200 GPUs. This notebook covered:\n",
        "\n",
        "âœ… **Introduction**\n",
        "- Architecture and innovations (Sparse Attention, MoE)\n",
        "- Performance benchmarks across multiple domains\n",
        "- Why DeepSeek-V3.2-Exp is special\n",
        "\n",
        "âœ… **Part 1: Installation and Serving**\n",
        "- Complete installation guide\n",
        "- Server configuration for EP/DP and TP modes\n",
        "- Client setup and basic testing\n",
        "\n",
        "âœ… **Part 2: Applications and Examples**\n",
        "- Tested long context handling\n",
        "- Demonstrated real-world applications\n",
        "- Optimization strategies for production\n",
        "\n",
        "### Resources\n",
        "\n",
        "#### Documentation\n",
        "- ğŸ“š [DeepSeek-V3.2-Exp Model Card](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp)\n",
        "- ğŸ”§ [vLLM DeepSeek Recipe](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html)\n",
        "- ğŸ—ï¸ [NVIDIA vLLM Guide](https://docs.nvidia.com/ai-enterprise/deployment/vllm/)\n",
        "\n",
        "#### Code and Kernels\n",
        "- ğŸ’¾ [TileLang Sparse Kernels](https://github.com/deepseek-ai/TileLang)\n",
        "- âš¡ [FlashMLA Implementation](https://github.com/deepseek-ai/FlashMLA)\n",
        "- ğŸ§ª [DeepSeek Examples](https://github.com/deepseek-ai/DeepSeek-V3)\n",
        "\n",
        "#### Community\n",
        "- ğŸ“§ [NVIDIA Developer Forums](https://forums.developer.nvidia.com/)\n",
        "\n",
        "\n",
        "#### Acknowledgments\n",
        "\n",
        "**Author:** Jay Rodge, Senior Developer Advocate @ NVIDIA\n",
        "\n",
        "Special thanks to the DeepSeek and vLLM teams for their incredible work on these technologies."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepseek",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
