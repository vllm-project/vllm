steps:
- group: Abuild
  steps:
  - block: 'Run :docker: Build CPU arm64 image'
    depends_on: image-build
    key: block--docker--build-cpu-arm64-image
  - label: ':docker: Build CPU arm64 image'
    key: image-build-cpu-arm64
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - .buildkite/image_build/image_build_cpu_arm64.sh public.ecr.aws/q9t5s3a7 vllm-ci-test-repo
      123
    depends_on: block--docker--build-cpu-arm64-image
    soft_fail: false
  - label: ':docker: Build CPU image'
    key: image-build-cpu
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - .buildkite/image_build/image_build_cpu.sh public.ecr.aws/q9t5s3a7 vllm-ci-test-repo
      123
    soft_fail: false
    retry:
      automatic:
      - exit_status: -1
        limit: 2
      - exit_status: -10
        limit: 2
    env:
      DOCKER_BUILDKIT: '1'
  - label: ':docker: Build CUDA 11.8 image'
    key: image-build-cu118
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - .buildkite/image_build/image_build_cu118.sh public.ecr.aws/q9t5s3a7 vllm-ci-test-repo
      123
    soft_fail: false
    retry:
      automatic:
      - exit_status: -1
        limit: 2
      - exit_status: -10
        limit: 2
    env:
      DOCKER_BUILDKIT: '1'
  - label: ':docker: Build HPU image'
    key: image-build-hpu
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - .buildkite/image_build/image_build_hpu.sh public.ecr.aws/q9t5s3a7 vllm-ci-test-repo
      123
    soft_fail: true
    retry:
      automatic:
      - exit_status: -1
        limit: 2
      - exit_status: -10
        limit: 2
    env:
      DOCKER_BUILDKIT: '1'
  - label: ':docker: Build image'
    key: image-build
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - .buildkite/image_build/image_build.sh public.ecr.aws/q9t5s3a7 vllm-ci-test-repo
      123
    soft_fail: false
    retry:
      automatic:
      - exit_status: -1
        limit: 2
      - exit_status: -10
        limit: 2
    env:
      DOCKER_BUILDKIT: '1'
- group: Attention
  steps:
  - label: V1 attention (B200)
    agents:
      queue: gpu_1_queue
    commands:
    - VLLM_DISABLE_FLASHINFER_PREFILL=1 pytest -v -s v1/attention
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: V1 attention (H100)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s v1/attention
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_USE_DEEP_GEMM=0
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            node.kubernetes.io/instance-type: gpu-h100-sxm
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
- group: Basic Correctness
  steps:
  - label: Basic Correctness
    agents:
      queue: gpu_1_queue
    commands:
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - pytest -v -s basic_correctness/test_cumem.py
    - pytest -v -s basic_correctness/test_basic_correctness.py
    - pytest -v -s basic_correctness/test_cpu_offload.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Benchmarks
  steps:
  - label: Benchmarks
    agents:
      queue: gpu_1_queue
    commands:
    - bash scripts/run-benchmarks.sh
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Benchmarks CLI Test
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s benchmarks/
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: CUDA
  steps:
  - label: Cudagraph
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s v1/cudagraph/test_cudagraph_dispatch.py
    - pytest -v -s v1/cudagraph/test_cudagraph_mode.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Platform Tests (CUDA)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s cuda/test_cuda_context.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Compile
  steps:
  - block: Run Fusion E2E (2 GPUs)(B200)
    depends_on: image-build
    key: block-fusion-e2e-2-gpusb200
  - label: Fusion E2E (2 GPUs)(B200)
    agents:
      queue: gpu_4_queue
    commands:
    - nvidia-smi
    - pytest -v -s tests/compile/distributed/test_fusions_e2e.py
    depends_on: block-fusion-e2e-2-gpusb200
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Fusion and Compile Tests (B200)
    agents:
      queue: gpu_1_queue
    commands:
    - nvidia-smi
    - pytest -v -s tests/compile/test_fusion_attn.py
    - pytest -v -s tests/compile/test_silu_mul_quant_fusion.py
    - pytest -v -s tests/compile/distributed/test_fusion_all_reduce.py
    - pytest -v -s tests/compile/distributed/test_fusions_e2e.py::test_tp2_attn_quant_allreduce_rmsnorm
      -k 'True and not +quant_fp8 and not +rms_norm'
    - pytest -v -s tests/compile/fullgraph/test_full_graph.py::test_fp8_kv_scale_compile
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Distributed
  steps:
  - label: 2 Node Test (4 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d
      --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py | grep 'Same node
      test passed'
    - NUM_NODES=2 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10
      distributed/test_node_count.py | grep 'Node count test passed'
    - python3 ../examples/offline_inference/data_parallel.py --dp-size=2 --tp-size=1
      --node-size=2 --node-rank=0 --master-addr=192.168.10.10 --master-port=12345
      --enforce-eager --trust-remote-code
    - VLLM_MULTI_NODE=1 pytest -v -s distributed/test_multi_node_assignment.py
    - VLLM_MULTI_NODE=1 pytest -v -s distributed/test_pipeline_parallel.py
    - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d
      --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py | grep 'Same node
      test passed'
    - NUM_NODES=2 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10
      distributed/test_node_count.py | grep 'Node count test passed'
    - python3 ../examples/offline_inference/data_parallel.py --dp-size=2 --tp-size=1
      --node-size=2 --node-rank=1 --master-addr=192.168.10.10 --master-port=12345
      --enforce-eager --trust-remote-code
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Distributed (2 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - export NCCL_CUMEM_HOST_ENABLE=0
    - TP_SIZE=1 DP_SIZE=2 pytest -v -s v1/distributed/test_async_llm_dp.py
    - TP_SIZE=1 DP_SIZE=2 pytest -v -s v1/distributed/test_external_lb_dp.py
    - DP_SIZE=2 pytest -v -s v1/entrypoints/openai/test_multi_api_servers.py
    - pytest -v -s entrypoints/llm/test_collective_rpc.py
    - pytest -v -s ./compile/fullgraph/test_basic_correctness.py
    - pytest -v -s ./compile/test_wrapper.py
    - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py
      | grep 'Same node test passed'
    - VLLM_TEST_SAME_HOST=1 VLLM_TEST_WITH_DEFAULT_DEVICE_SET=1 torchrun --nproc-per-node=4
      distributed/test_same_node.py | grep 'Same node test passed'
    - pytest -v -s distributed/test_sequence_parallel.py
    - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s v1/shutdown
    - pytest -v -s v1/worker/test_worker_memory_snapshot.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Distributed Comm Ops
    agents:
      queue: gpu_4_queue
    commands:
    - pytest -v -s distributed/test_comm_ops.py
    - pytest -v -s distributed/test_shm_broadcast.py
    - pytest -v -s distributed/test_shm_buffer.py
    - pytest -v -s distributed/test_shm_storage.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Distributed NixlConnector PD accuracy (4 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - uv pip install --system -r /vllm-workspace/requirements/kv_connectors.txt
    - bash v1/kv_connector/nixl_integration/tp_config_sweep_accuracy_test.sh
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Distributed Tests (2 GPUs)(B200)
    depends_on: image-build
    key: block-distributed-tests-2-gpusb200
  - label: Distributed Tests (2 GPUs)(B200)
    agents:
      queue: gpu_4_queue
    commands:
    - pytest -v -s tests/distributed/test_context_parallel.py
    - pytest -v -s tests/distributed/test_nccl_symm_mem_allreduce.py
    - pytest -v -s tests/v1/distributed/test_dbo.py
    depends_on: block-distributed-tests-2-gpusb200
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Distributed Tests (2 GPUs)(H200)
    depends_on: image-build
    key: block-distributed-tests-2-gpush200
  - label: Distributed Tests (2 GPUs)(H200)
    agents:
      queue: gpu_4_queue
    commands:
    - pytest -v -s tests/compile/distributed/test_async_tp.py
    - pytest -v -s tests/compile/distributed/test_sequence_parallelism.py
    - pytest -v -s tests/compile/distributed/test_fusion_all_reduce.py
    - pytest -v -s tests/compile/distributed/test_fusions_e2e.py -k 'not Llama-4'
    - pytest -v -s tests/distributed/test_sequence_parallel.py
    - pytest -v -s tests/distributed/test_context_parallel.py
    - CUDA_VISIBLE_DEVICES=1,2 VLLM_ALL2ALL_BACKEND=deepep_high_throughput VLLM_USE_DEEP_GEMM=1
      VLLM_LOGGING_LEVEL=DEBUG python3 examples/offline_inference/data_parallel.py
      --model Qwen/Qwen1.5-MoE-A2.7B --tp-size=1  --dp-size=2 --max-model-len 2048
    - pytest -v -s tests/v1/distributed/test_dbo.py
    depends_on: block-distributed-tests-2-gpush200
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Distributed Tests (4 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - export NCCL_CUMEM_HOST_ENABLE=0
    - torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
    - PP_SIZE=2 torchrun --nproc-per-node=4 distributed/test_torchrun_example.py
    - TP_SIZE=4 torchrun --nproc-per-node=4 distributed/test_torchrun_example_moe.py
    - PP_SIZE=2 TP_SIZE=2 torchrun --nproc-per-node=4 distributed/test_torchrun_example_moe.py
    - DP_SIZE=4 ENABLE_EP=1 torchrun --nproc-per-node=4 distributed/test_torchrun_example_moe.py
    - TP_SIZE=2 DP_SIZE=2 ENABLE_EP=1 torchrun --nproc-per-node=4 distributed/test_torchrun_example_moe.py
    - python3 ../examples/offline_inference/data_parallel.py --enforce-eager
    - TP_SIZE=2 DP_SIZE=2 pytest -v -s v1/distributed/test_async_llm_dp.py
    - TP_SIZE=2 DP_SIZE=2 pytest -v -s v1/distributed/test_external_lb_dp.py
    - TP_SIZE=1 DP_SIZE=4 pytest -v -s v1/distributed/test_internal_lb_dp.py
    - TP_SIZE=1 DP_SIZE=4 pytest -v -s v1/distributed/test_hybrid_lb_dp.py
    - pytest -v -s v1/engine/test_engine_core_client.py::test_kv_cache_events_dp
    - pytest -v -s distributed/test_utils.py
    - pytest -v -s compile/fullgraph/test_basic_correctness.py
    - pytest -v -s distributed/test_pynccl.py
    - pytest -v -s distributed/test_events.py
    - pytest -v -s distributed/test_symm_mem_allreduce.py
    - pushd ../examples/offline_inference
    - VLLM_ALLOW_INSECURE_SERIALIZATION=1 python3 rlhf.py
    - VLLM_ALLOW_INSECURE_SERIALIZATION=1 RAY_DEDUP_LOGS=0 python3 rlhf_colocate.py
    - popd
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Distributed Tests (4 GPUs)(A100)
    depends_on: image-build
    key: block-distributed-tests-4-gpusa100
  - label: Distributed Tests (4 GPUs)(A100)
    agents:
      queue: gpu_4_queue
    commands:
    - pytest -v -s distributed/test_custom_all_reduce.py
    - torchrun --nproc_per_node=2 distributed/test_ca_buffer_sharing.py
    - TARGET_TEST_SUITE=A100 pytest basic_correctness/ -v -s -m 'distributed(num_gpus=2)'
    - pytest -v -s -x lora/test_mixtral.py
    depends_on: block-distributed-tests-4-gpusa100
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          priorityClassName: ci
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_WORKER_MULTIPROC_METHOD=spawn
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  - label: Distributed Tests (8 GPUs)(H100)
    agents:
      queue: gpu_1_queue
    commands:
    - export NCCL_CUMEM_HOST_ENABLE=0
    - torchrun --nproc-per-node=8 ../examples/offline_inference/torchrun_dp_example.py
      --tp-size=2 --pp-size=1 --dp-size=4 --enable-ep
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_USE_DEEP_GEMM=0
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            node.kubernetes.io/instance-type: gpu-h100-sxm
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  - label: Pipeline + Context Parallelism (4 GPUs))
    agents:
      queue: gpu_4_queue
    commands:
    - pytest -v -s distributed/test_pp_cudagraph.py
    - pytest -v -s distributed/test_pipeline_parallel.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: E2E Integration
  steps:
  - block: Run DeepSeek V2-Lite Accuracy
    depends_on: image-build
    key: block-deepseek-v2-lite-accuracy
  - label: DeepSeek V2-Lite Accuracy
    agents:
      queue: gpu_4_queue
    commands:
    - bash .buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh
      0.25 200 8010
    depends_on: block-deepseek-v2-lite-accuracy
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_USE_DEEP_GEMM=0
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            node.kubernetes.io/instance-type: gpu-h100-sxm
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  - block: Run Prime-RL Integration (2 GPUs)
    depends_on: image-build
    key: block-prime-rl-integration-2-gpus
  - label: Prime-RL Integration (2 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - bash .buildkite/scripts/run-prime-rl-test.sh
    depends_on: block-prime-rl-integration-2-gpus
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Qwen3-30B-A3B-FP8-block Accuracy
    depends_on: image-build
    key: block-qwen3-30b-a3b-fp8-block-accuracy
  - label: Qwen3-30B-A3B-FP8-block Accuracy
    agents:
      queue: gpu_4_queue
    commands:
    - bash .buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep_eplb.sh
      0.8 200 8020
    depends_on: block-qwen3-30b-a3b-fp8-block-accuracy
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_USE_DEEP_GEMM=0
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            node.kubernetes.io/instance-type: gpu-h100-sxm
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
- group: Engine
  steps:
  - label: Engine
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s engine test_sequence.py test_config.py test_logger.py test_vllm_port.py
    - pytest -v -s tokenization
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: V1 e2e + engine
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s v1/e2e
    - pytest -v -s v1/engine
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Entrypoints
  steps:
  - label: Entrypoints Integration (API Server)
    agents:
      queue: gpu_1_queue
    commands:
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - PYTHONPATH=/vllm-workspace pytest -v -s entrypoints/openai/test_collective_rpc.py
    - pytest -v -s entrypoints/openai --ignore=entrypoints/openai/test_chat_with_tool_reasoning.py
      --ignore=entrypoints/openai/test_oot_registration.py --ignore=entrypoints/openai/test_tensorizer_entrypoint.py
      --ignore=entrypoints/openai/correctness/ --ignore=entrypoints/openai/test_collective_rpc.py
      --ignore=entrypoints/openai/tool_parsers/
    - pytest -v -s entrypoints/test_chat_utils.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Entrypoints Integration (LLM)
    agents:
      queue: gpu_1_queue
    commands:
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - pytest -v -s entrypoints/llm --ignore=entrypoints/llm/test_generate.py --ignore=entrypoints/llm/test_collective_rpc.py
    - pytest -v -s entrypoints/llm/test_generate.py
    - pytest -v -s entrypoints/offline_mode
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Entrypoints Integration (Pooling)
    agents:
      queue: gpu_1_queue
    commands:
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - pytest -v -s entrypoints/pooling
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Entrypoints Unit Tests
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s entrypoints/openai/tool_parsers
    - pytest -v -s entrypoints/ --ignore=entrypoints/llm --ignore=entrypoints/openai
      --ignore=entrypoints/offline_mode --ignore=entrypoints/test_chat_utils.py  --ignore=entrypoints/pooling
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Entrypoints V1
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s v1/entrypoints
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: OpenAI API Correctness
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -s entrypoints/openai/correctness/
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Expert Parallelism
  steps:
  - label: EPLB Algorithm
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s distributed/test_eplb_algo.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: EPLB Execution
    agents:
      queue: gpu_4_queue
    commands:
    - pytest -v -s distributed/test_eplb_execute.py
    - pytest -v -s distributed/test_eplb_spec_decode.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Kernels
  steps:
  - label: Kernels (B200)
    agents:
      queue: gpu_1_queue
    commands:
    - nvidia-smi
    - python3 examples/offline_inference/basic/chat.py
    - pytest -v -s tests/kernels/attention/test_attention_selector.py
    - pytest -v -s tests/kernels/attention/test_flashinfer.py -k 'not num_heads2'
    - pytest -v -s tests/kernels/attention/test_flashinfer_trtllm_attention.py
    - pytest -v -s tests/kernels/attention/test_cutlass_mla_decode.py
    - pytest -v -s tests/kernels/attention/test_flashinfer_mla_decode.py
    - pytest -v -s tests/kernels/quantization/test_cutlass_scaled_mm.py -k 'fp8'
    - pytest -v -s tests/kernels/quantization/test_nvfp4_quant.py
    - pytest -v -s tests/kernels/quantization/test_silu_mul_nvfp4_quant.py
    - pytest -v -s tests/kernels/quantization/test_nvfp4_scaled_mm.py
    - pytest -v -s tests/kernels/quantization/test_flashinfer_scaled_mm.py
    - pytest -v -s tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py
    - pytest -v -s tests/kernels/quantization/test_nvfp4_qutlass.py
    - pytest -v -s tests/kernels/quantization/test_mxfp4_qutlass.py
    - pytest -v -s tests/kernels/moe/test_nvfp4_moe.py
    - pytest -v -s tests/kernels/moe/test_ocp_mx_moe.py
    - pytest -v -s tests/kernels/moe/test_flashinfer.py
    - pytest -v -s tests/kernels/moe/test_cutedsl_moe.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Kernels Attention Test %N
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s kernels/attention --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Kernels Core Operation Test
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s kernels/core kernels/test_top_k_per_row.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Kernels DeepGEMM Test (H100)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s kernels/quantization/test_block_fp8.py -k deep_gemm
    - pytest -v -s kernels/moe/test_deepgemm.py
    - pytest -v -s kernels/moe/test_batched_deepgemm.py
    - pytest -v -s kernels/attention/test_deepgemm_attention.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_USE_DEEP_GEMM=0
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            node.kubernetes.io/instance-type: gpu-h100-sxm
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  - label: Kernels Mamba Test
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s kernels/mamba
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Kernels MoE Test %N
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s kernels/moe --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Kernels Quantization Test %N
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s kernels/quantization --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: LM Eval
  steps:
  - block: Run LM Eval Large Models (4 GPUs)(A100)
    depends_on: image-build
    key: block-lm-eval-large-models-4-gpusa100
  - label: LM Eval Large Models (4 GPUs)(A100)
    agents:
      queue: gpu_4_queue
    commands:
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large.txt
      --tp-size=4
    depends_on: block-lm-eval-large-models-4-gpusa100
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          priorityClassName: ci
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_WORKER_MULTIPROC_METHOD=spawn
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  - block: Run LM Eval Large Models (4 GPUs)(H100)
    depends_on: image-build
    key: block-lm-eval-large-models-4-gpush100
  - label: LM Eval Large Models (4 GPUs)(H100)
    agents:
      queue: gpu_4_queue
    commands:
    - export VLLM_USE_DEEP_GEMM=0
    - pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
      --tp-size=4
    depends_on: block-lm-eval-large-models-4-gpush100
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_USE_DEEP_GEMM=0
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large-hopper.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            node.kubernetes.io/instance-type: gpu-h100-sxm
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
  - label: LM Eval Small Models
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -s -v evals/gsm8k/test_gsm8k_correctness.py --config-list-file=configs/models-small.txt
      --tp-size=1
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run LM Eval Small Models (B200)
    depends_on: image-build
    key: block-lm-eval-small-models-b200
  - label: LM Eval Small Models (B200)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -s -v evals/gsm8k/test_gsm8k_correctness.py --config-list-file=configs/models-blackwell.txt
      --tp-size=1
    depends_on: block-lm-eval-small-models-b200
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: LoRA
  steps:
  - label: LoRA %N
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s lora \ --shard-id=$$BUILDKITE_PARALLEL_JOB \ --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
      \ --ignore=lora/test_chatglm3_tp.py \ --ignore=lora/test_llama_tp.py \ --ignore=lora/test_llm_with_multi_loras.py
      \ --ignore=lora/test_olmoe_tp.py \ --ignore=lora/test_deepseekv2_tp.py \ --ignore=lora/test_gptoss_tp.py
      \ --ignore=lora/test_qwen3moe_tp.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: LoRA TP (Distributed)
    agents:
      queue: gpu_4_queue
    commands:
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - pytest -v -s -x lora/test_chatglm3_tp.py
    - pytest -v -s -x lora/test_llama_tp.py
    - pytest -v -s -x lora/test_llm_with_multi_loras.py
    - pytest -v -s -x lora/test_olmoe_tp.py
    - pytest -v -s -x lora/test_gptoss_tp.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Miscellaneous
  steps:
  - label: Async Engine, Inputs, Utils, Worker
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s -m 'not cpu_test' multimodal
    - pytest -v -s utils_
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Async Engine, Inputs, Utils, Worker, Config (CPU)
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - python3 standalone_tests/lazy_imports.py
    - pytest -v -s test_inputs.py
    - pytest -v -s test_outputs.py
    - pytest -v -s -m 'cpu_test' multimodal
    - pytest -v -s transformers_utils
    - pytest -v -s config
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Examples
    agents:
      queue: gpu_1_queue
    commands:
    - pip install tensorizer
    - python3 offline_inference/basic/generate.py --model facebook/opt-125m
    - python3 offline_inference/basic/generate.py --model meta-llama/Llama-2-13b-chat-hf
      --cpu-offload-gb 10
    - python3 offline_inference/basic/chat.py
    - python3 offline_inference/prefix_caching.py
    - python3 offline_inference/llm_engine_example.py
    - python3 offline_inference/audio_language.py --seed 0
    - python3 offline_inference/vision_language.py --seed 0
    - python3 offline_inference/vision_language_pooling.py --seed 0
    - python3 offline_inference/vision_language_multi_image.py --seed 0
    - python3 others/tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory
      /tmp/ --suffix v1 && python3 others/tensorize_vllm_model.py --model facebook/opt-125m
      deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors
    - python3 offline_inference/encoder_decoder_multimodal.py --model-type whisper
      --seed 0
    - python3 offline_inference/basic/classify.py
    - python3 offline_inference/basic/embed.py
    - python3 offline_inference/basic/score.py
    - python3 offline_inference/spec_decode.py --test --method eagle --num_spec_tokens
      3 --dataset-name hf --dataset-path philschmid/mt-bench --num-prompts 80 --temp
      0 --top-p 1.0 --top-k -1 --tp 1 --enable-chunked-prefill --max-model-len 2048
    - python3 offline_inference/spec_decode.py --test --method eagle3 --num_spec_tokens
      3 --dataset-name hf --dataset-path philschmid/mt-bench --num-prompts 80 --temp
      0 --top-p 1.0 --top-k -1 --tp 1 --enable-chunked-prefill --max-model-len 1536
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run GPT-OSS Eval (B200)
    depends_on: image-build
    key: block-gpt-oss-eval-b200
  - label: GPT-OSS Eval (B200)
    agents:
      queue: gpu_1_queue
    commands:
    - uv pip install --system 'gpt-oss[eval]==0.0.5'
    - pytest -s -v tests/evals/gpt_oss/test_gpqa_correctness.py --model openai/gpt-oss-20b
      --metric 0.58
    depends_on: block-gpt-oss-eval-b200
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Metrics, Tracing (2 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - pip install 'opentelemetry-sdk>=1.26.0' 'opentelemetry-api>=1.26.0' 'opentelemetry-exporter-otlp>=1.26.0'
      'opentelemetry-semantic-conventions-ai>=0.4.1'
    - pytest -v -s v1/tracing
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Python-only Installation
    agents:
      queue: gpu_1_queue
    commands:
    - bash standalone_tests/python_only_compile.sh
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Regression
    agents:
      queue: gpu_1_queue
    commands:
    - pip install modelscope
    - pytest -v -s test_regression.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: V1 Others
    agents:
      queue: gpu_1_queue
    commands:
    - uv pip install --system -r /vllm-workspace/requirements/kv_connectors.txt
    - pytest -v -s -m 'not cpu_test' v1/core
    - pytest -v -s v1/executor
    - pytest -v -s v1/kv_offload
    - pytest -v -s v1/sample
    - pytest -v -s v1/logits_processors
    - pytest -v -s v1/worker
    - pytest -v -s v1/spec_decode
    - pytest -v -s -m 'not cpu_test' v1/kv_connector/unit
    - pytest -v -s -m 'not cpu_test' v1/metrics
    - pytest -v -s v1/test_oracle.py
    - pytest -v -s v1/test_request.py
    - pytest -v -s v1/test_outputs.py
    - pip install -U git+https://github.com/robertgshaw2-redhat/lm-evaluation-harness.git@streaming-api
    - pytest -v -s entrypoints/openai/correctness/test_lmeval.py::test_lm_eval_accuracy_v1_engine
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: V1 Others (CPU)
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - pytest -v -s -m 'cpu_test' v1/core
    - pytest -v -s v1/structured_output
    - pytest -v -s v1/test_serial_utils.py
    - pytest -v -s -m 'cpu_test' v1/kv_connector/unit
    - pytest -v -s -m 'cpu_test' v1/metrics
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Model Executor
  steps:
  - label: Model Executor
    agents:
      queue: gpu_1_queue
    commands:
    - apt-get update && apt-get install -y curl libsodium23
    - export VLLM_WORKER_MULTIPROC_METHOD=spawn
    - pytest -v -s model_executor
    - pytest -v -s entrypoints/openai/test_tensorizer_entrypoint.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Models - Basic
  steps:
  - label: Basic Models Test (Other CPU)
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - pytest -v -s models/test_utils.py models/test_vision.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Basic Models Tests (Extra Initialization) %N
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s models/test_initialization.py \ -k 'not test_can_initialize_small_subset'
      \ --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT \ --shard-id=$$BUILDKITE_PARALLEL_JOB
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Basic Models Tests (Initialization)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s models/test_initialization.py::test_can_initialize_small_subset
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Basic Models Tests (Other)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s models/test_transformers.py models/test_registry.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Transformers Nightly Models
    depends_on: image-build
    key: block-transformers-nightly-models
  - label: Transformers Nightly Models
    agents:
      queue: gpu_1_queue
    commands:
    - pip install --upgrade git+https://github.com/huggingface/transformers
    - pytest -v -s tests/models/test_initialization.py -k 'not (Ultravox or Phi4Multimodal
      or MiniCPMO or Lfm2Moe or RobertaForSequenceClassification or Ovis2_5 or DeepseekOCR
      or KimiVL)'
    - pytest -v -s tests/models/test_transformers.py
    - pytest -v -s tests/models/multimodal/test_mapping.py
    - python3 examples/offline_inference/basic/chat.py
    - python3 examples/offline_inference/vision_language.py --model-type qwen2_5_vl
    - VLLM_WORKER_MULTIPROC_METHOD=spawn python3 examples/offline_inference/audio_language.py
      --model-type whisper
    depends_on: block-transformers-nightly-models
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Models - Distributed
  steps:
  - label: Distributed Model Tests (2 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - TARGET_TEST_SUITE=L4 pytest basic_correctness/ -v -s -m 'distributed(num_gpus=2)'
    - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s model_executor/model_loader/test_sharded_state_loader.py
    - pytest models/test_transformers.py -v -s -m 'distributed(num_gpus=2)'
    - pytest models/language -v -s -m 'distributed(num_gpus=2)'
    - pytest models/multimodal -v -s -m 'distributed(num_gpus=2)' --ignore models/multimodal/generation/test_whisper.py
    - VLLM_WORKER_MULTIPROC_METHOD=spawn pytest models/multimodal/generation/test_whisper.py
      -v -s -m 'distributed(num_gpus=2)'
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Models - Language
  steps:
  - block: Run Language Models Test (Extended Generation)
    depends_on: image-build
    key: block-language-models-test-extended-generation
  - label: Language Models Test (Extended Generation)
    agents:
      queue: gpu_1_queue
    commands:
    - uv pip install --system --no-build-isolation 'git+https://github.com/state-spaces/mamba@v2.2.5'
    - uv pip install --system --no-build-isolation 'git+https://github.com/Dao-AILab/causal-conv1d@v1.5.2'
    - pytest -v -s models/language/generation -m '(not core_model) and (not hybrid_model)'
    depends_on: block-language-models-test-extended-generation
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Language Models Test (Extended Pooling)
    depends_on: image-build
    key: block-language-models-test-extended-pooling
  - label: Language Models Test (Extended Pooling)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s models/language/pooling -m 'not core_model'
    depends_on: block-language-models-test-extended-pooling
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Language Models Test (MTEB)
    depends_on: image-build
    key: block-language-models-test-mteb
  - label: Language Models Test (MTEB)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s models/language/pooling_mteb_test
    depends_on: block-language-models-test-mteb
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Language Models Test (PPL)
    depends_on: image-build
    key: block-language-models-test-ppl
  - label: Language Models Test (PPL)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s models/language/generation_ppl_test
    depends_on: block-language-models-test-ppl
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Language Models Tests (Extra Standard) %N
    agents:
      queue: gpu_1_queue
    commands:
    - pip freeze | grep -E 'torch'
    - pytest -v -s models/language -m 'core_model and slow_test' \ --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
      \ --shard-id=$$BUILDKITE_PARALLEL_JOB
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Language Models Tests (Hybrid) %N
    agents:
      queue: gpu_1_queue
    commands:
    - uv pip install --system --no-build-isolation 'git+https://github.com/state-spaces/mamba@v2.2.5'
    - uv pip install --system --no-build-isolation 'git+https://github.com/Dao-AILab/causal-conv1d@v1.5.2'
    - pytest -v -s models/language/generation \ -m hybrid_model \ --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
      \ --shard-id=$$BUILDKITE_PARALLEL_JOB
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Language Models Tests (Standard)
    agents:
      queue: gpu_1_queue
    commands:
    - pip freeze | grep -E 'torch'
    - pytest -v -s models/language -m 'core_model and (not slow_test)'
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Models - Multimodal
  steps:
  - block: Run Custom Models
    depends_on: image-build
    key: block-custom-models
  - label: Custom Models
    agents:
      queue: gpu_1_queue
    commands:
    - echo 'Testing custom models...'
    depends_on: block-custom-models
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Multi-Modal Accuracy Eval (Small Models)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-mm-small.txt
      --tp-size=1
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Multi-Modal Models (Extended) 1
    depends_on: image-build
    key: block-multi-modal-models-extended-1
  - label: Multi-Modal Models (Extended) 1
    agents:
      queue: gpu_1_queue
    commands:
    - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
    - pytest -v -s models/multimodal -m 'not core_model' --ignore models/multimodal/generation/test_common.py
      --ignore models/multimodal/processing
    depends_on: block-multi-modal-models-extended-1
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Multi-Modal Models (Extended) 2
    depends_on: image-build
    key: block-multi-modal-models-extended-2
  - label: Multi-Modal Models (Extended) 2
    agents:
      queue: gpu_1_queue
    commands:
    - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
    - pytest -v -s models/multimodal/generation/test_common.py -m 'split(group=0)
      and not core_model'
    depends_on: block-multi-modal-models-extended-2
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Multi-Modal Models (Extended) 3
    depends_on: image-build
    key: block-multi-modal-models-extended-3
  - label: Multi-Modal Models (Extended) 3
    agents:
      queue: gpu_1_queue
    commands:
    - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
    - pytest -v -s models/multimodal/generation/test_common.py -m 'split(group=1)
      and not core_model'
    depends_on: block-multi-modal-models-extended-3
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Multi-Modal Models (Standard)
    agents:
      queue: gpu_1_queue
    commands:
    - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
    - pip freeze | grep -E 'torch'
    - pytest -v -s models/multimodal -m core_model --ignore models/multimodal/generation/test_whisper.py
      --ignore models/multimodal/processing
    - cd .. && VLLM_WORKER_MULTIPROC_METHOD=spawn pytest -v -s tests/models/multimodal/generation/test_whisper.py
      -m core_model
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Multi-Modal Processor
    agents:
      queue: gpu_1_queue
    commands:
    - pip install git+https://github.com/TIGER-AI-Lab/Mantis.git
    - pytest -v -s models/multimodal/processing
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Plugins
  steps:
  - label: Plugin Tests (2 GPUs)
    agents:
      queue: gpu_4_queue
    commands:
    - pip install -e ./plugins/vllm_add_dummy_platform
    - pytest -v -s plugins_tests/test_platform_plugins.py
    - pip uninstall vllm_add_dummy_platform -y
    - pip install -e ./plugins/prithvi_io_processor_plugin
    - pytest -v -s plugins_tests/test_io_processor_plugins.py
    - pip uninstall prithvi_io_processor_plugin -y
    - pip install -e ./plugins/vllm_add_dummy_stat_logger
    - pytest -v -s plugins_tests/test_stats_logger_plugins.py
    - pip uninstall dummy_stat_logger -y
    - pytest -v -s plugins_tests/test_scheduler_plugins.py
    - pip install -e ./plugins/vllm_add_dummy_model
    - pytest -v -s distributed/test_distributed_oot.py
    - pytest -v -s entrypoints/openai/test_oot_registration.py
    - pytest -v -s models/test_oot_registration.py
    - pytest -v -s plugins/lora_resolvers
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: PyTorch
  steps:
  - label: PyTorch Compilation Unit Tests
    agents:
      queue: gpu_1_queue
    commands:
    - find compile/ -maxdepth 1 -name 'test_*.py' -exec pytest -s -v {} \\;
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: PyTorch Fullgraph
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s compile/fullgraph/test_full_graph.py -k 'not test_fp8_kv_scale_compile'
    - pytest -v -s compile/distributed/test_fusions_e2e.py -k 'TRITON and not +quant_fp8
      and not Llama-4'
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: PyTorch Fullgraph Smoke Test
    agents:
      queue: gpu_1_queue
    commands:
    - find compile/fullgraph/ -name 'test_*.py' -not -name 'test_full_graph.py' -exec
      pytest -s -v {} \\;
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Pytorch Nightly Dependency Override Check
    agents:
      queue: gpu_1_queue
    commands:
    - bash standalone_tests/pytorch_nightly_dependency.sh
    depends_on:
    - image-build
    soft_fail: true
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Quantization
  steps:
  - label: Quantization
    agents:
      queue: gpu_1_queue
    commands:
    - uv pip install --system torchao==0.13.0 --index-url https://download.pytorch.org/whl/cu129
    - VLLM_TEST_FORCE_LOAD_FORMAT=auto pytest -v -s quantization/ --ignore quantization/test_blackwell_moe.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: Quantized MoE Test (B200)
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -s -v tests/quantization/test_blackwell_moe.py
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Samplers
  steps:
  - label: Samplers Test
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s samplers
    - VLLM_USE_FLASHINFER_SAMPLER=1 pytest -v -s samplers
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Tool use
  steps:
  - label: OpenAI-Compatible Tool Use
    agents:
      queue: gpu_1_queue
    commands:
    - pytest -v -s -m 'not cpu_test' tool_use
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - label: OpenAI-Compatible Tool Use (CPU)
    agents:
      queue: cpu_queue_premerge_us_east_1
    commands:
    - pytest -v -s -m 'cpu_test' tool_use
    depends_on:
    - image-build
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
- group: Weight Loading
  steps:
  - block: Run Weight Loading Multiple GPU
    depends_on: image-build
    key: block-weight-loading-multiple-gpu
  - label: Weight Loading Multiple GPU
    agents:
      queue: gpu_4_queue
    commands:
    - bash weight_loading/run_model_weight_loading_test.sh -c weight_loading/models.txt
    depends_on: block-weight-loading-multiple-gpu
    soft_fail: false
    plugins:
    - docker#v5.2.0:
        image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
        always_pull: true
        propagate_environment: true
        gpus: all
        environment:
        - VLLM_USAGE_SOURCE=ci-test
        - NCCL_CUMEM_HOST_ENABLE=0
        - HF_HOME=/fsx/hf_cache
        - HF_TOKEN
        - CODECOV_TOKEN
        volumes:
        - /dev/shm:/dev/shm
        - /fsx/hf_cache:/fsx/hf_cache
        mount_buildkite_agent: true
  - block: Run Weight Loading Multiple GPU - Large Models
    depends_on: image-build
    key: block-weight-loading-multiple-gpu---large-models
  - label: Weight Loading Multiple GPU - Large Models
    agents:
      queue: gpu_4_queue
    commands:
    - bash weight_loading/run_model_weight_loading_test.sh -c weight_loading/models-large.txt
    depends_on: block-weight-loading-multiple-gpu---large-models
    soft_fail: false
    plugins:
    - kubernetes:
        podSpec:
          priorityClassName: ci
          containers:
          - image: public.ecr.aws/q9t5s3a7:vllm-ci-test-repo:123
            command:
            - bash
            - -c
            - (command nvidia-smi || true) && export VLLM_ALLOW_DEPRECATED_BEAM_SEARCH=1
              && cd /vllm-workspace/.buildkite/lm-eval-harness && export VLLM_WORKER_MULTIPROC_METHOD=spawn
              && pytest -s -v test_lm_eval_correctness.py --config-list-file=configs/models-large.txt
              --tp-size=4
            resources:
              limits:
                nvidia.com/gpu: 4
            volumeMounts:
            - name: devshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /root/.cache/huggingface
            env:
            - name: VLLM_USAGE_SOURCE
              value: ci-test
            - name: NCCL_CUMEM_HOST_ENABLE
              value: '0'
            - name: HF_HOME
              value: /root/.cache/huggingface
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token-secret
                  key: token
          nodeSelector:
            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
          volumes:
          - name: devshm
            emptyDir:
              medium: Memory
          - name: hf-cache
            hostPath:
              path: /mnt/hf-cache
              type: DirectoryOrCreate
