# For hf script, without -t option (tensor parallel size).
# bash .buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh -m meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 -l 250 -t 8 -f 5
model_name: "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
tasks:
# - name: "gsm8k"
#   metrics:
#   - name: "exact_match,strict-match"
#     value: 0.95
#   - name: "exact_match,flexible-extract"
#     value: 0.95
- name: "mmlu_pro"
  metrics:
  - name: "exact_match,custom-extract"
    value: 0.80
limit: 100 # will run on 250 * 14 subjects = 3500 samples
num_fewshot: 5
max_model_len: 8192
gen_kwargs: "temperature=0,top_p=1,top_k=0,max_gen_toks=5632,until=<|ENDANSWER|>"
apply_chat_template: true
fewshot_as_multiturn: true
rtol: 0.05
vllm_args:
  # enable_expert_parallel: true
  gpu_memory_utilization: 0.80
env_vars:
  VLLM_ROCM_USE_AITER: 1
  VLLM_ROCM_USE_AITER_MHA: 1
  VLLM_ROCM_USE_AITER_MLA: 1
  VLLM_ROCM_USE_AITER_MOE: 1
