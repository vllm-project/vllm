# For vllm script, with -t option (tensor parallel size).
# bash ./run-lm-eval-gsm-vllm-baseline.sh -m deepseek-ai/DeepSeek-V3 -b 32 -l 250 -f 8
model_name: "deepseek-ai/DeepSeek-V3"
backend: "vllm"
tasks:
- name: "gsm8k"
  metrics:
  - name: "exact_match,strict-match"
    value: 0.893
  - name: "exact_match,flexible-extract"
    value: 0.893
limit: 50
num_fewshot: 8
trust_remote_code: True
# TODO(zhewenl): we should increase bath_size and seq_len when we have MI300X or other large GPUs.
max_model_len: 1024
batch_size: 1
gpu_memory_utilization: 0.98
