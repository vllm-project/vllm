steps:
  - label: "QwenMoE + h200"
    gpu: h200
    num_gpus: 2
    commands:
      - CUDA_VISIBLE_DEVICES=1,2 VLLM_LOGGING_LEVEL=DEBUG python3 /vllm-workspace/examples/offline_inference/data_parallel.py --model Qwen/Qwen1.5-MoE-A2.7B --tp-size=1  --dp-size=2 --max-model-len 2048

  - label: "QwenMoE + h200 + DeepGEMM"
    gpu: h200
    num_gpus: 2
    commands:
      - CUDA_VISIBLE_DEVICES=1,2 VLLM_USE_DEEP_GEMM=1 VLLM_LOGGING_LEVEL=DEBUG python3 /vllm-workspace/examples/offline_inference/data_parallel.py --model Qwen/Qwen1.5-MoE-A2.7B --tp-size=1  --dp-size=2 --max-model-len 2048

  - label: "Sanity Check: QwenMoE + h200 + DeepEP"
    gpu: h200
    num_gpus: 2
    commands:
      - CUDA_VISIBLE_DEVICES=1,2 VLLM_ALL2ALL_BACKEND=deepep_high_throughput VLLM_LOGGING_LEVEL=DEBUG python3 /vllm-workspace/examples/offline_inference/data_parallel.py --model Qwen/Qwen1.5-MoE-A2.7B --tp-size=1  --dp-size=2 --max-model-len 2048

  - label: "Sanity Check: QwenMoE + h200 + pplx"
    gpu: h200
    num_gpus: 2
    commands:
      - CUDA_VISIBLE_DEVICES=1,2 VLLM_ALL2ALL_BACKEND=pplx VLLM_LOGGING_LEVEL=DEBUG python3 /vllm-workspace/examples/offline_inference/data_parallel.py --model Qwen/Qwen1.5-MoE-A2.7B --tp-size=1  --dp-size=2 --max-model-len 2048

  - label: "Sanity Check: QwenMoE + EP=4"
    gpu: h200
    num_gpus: 4
    commands:
      - CUDA_VISIBLE_DEVICES=1,2,3,4 VLLM_LOGGING_LEVEL=DEBUG python3 /vllm-workspace/examples/offline_inference/data_parallel.py --model Qwen/Qwen1.5-MoE-A2.7B --tp-size=1  --dp-size=4 --max-model-len 2048
