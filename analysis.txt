Q: 你是一名大模型推理架构技术专家，擅长分析复杂的数据流。请你分析 @vllm/v1/core/sched/scheduler.py ， @vllm/v1/core/sched/async_scheduler.py 在 
@vllm/v1/engine/core.py 中的调度逻辑，加上 @vllm/v1/worker/gpu_model_runner.py 这一 worker 进程，请你分析在 step 步 之间，每个请求的 kv cache 
是如何分配的？最终是如何在 attention 计算中被使用的？ 

A: vLLM V1 KV Cache 分配与使用分析

  概览

  让我分析 vLLM V1 中 KV Cache 从调度到 Attention 计算的完整数据流。

  1. 调度层：KV Cache 块分配

  1.1 Scheduler.schedule() 中的分配逻辑

  在 scheduler.py:193-735 的 schedule() 方法中，KV Cache 分配发生在两个主要场景：

  场景一：RUNNING 请求的增量分配 (scheduler.py:280-290)

  # 为 RUNNING 请求分配新的 KV 块
  new_blocks = self.kv_cache_manager.allocate_slots(
      request,
      num_new_tokens,  # 本步需要计算的新 token 数
      num_lookahead_tokens=self.num_lookahead_tokens,  # speculative decoding 预留
  )

  场景二：WAITING 请求的首次分配 (scheduler.py:556-564)

  new_blocks = self.kv_cache_manager.allocate_slots(
      request,
      num_new_tokens + num_external_computed_tokens,  # 新 token + 外部计算的 token
      num_new_local_computed_tokens,  # 本地已缓存的 token 数
      new_computed_blocks,  # 已计算的块（prefix cache 命中）
      num_lookahead_tokens=effective_lookahead_tokens,
      delay_cache_blocks=load_kv_async,  # 异步 KV 加载时延迟缓存
      num_encoder_tokens=num_encoder_tokens,  # encoder-decoder 模型的 cross-attention
  )

  1.2 Block 信息传递到 Worker

  分配完成后，block IDs 被封装到 SchedulerOutput 中：

  # scheduler.py:666-680
  new_reqs_data = [
      NewRequestData.from_request(
          req,
          req_to_new_blocks[req.request_id].get_block_ids(),  # 块 ID 列表
          req._all_token_ids,
      )
      for req in scheduled_new_reqs
  ]

  # scheduler.py:818-819 (CachedRequestData)
  new_block_ids.append(
      req_to_new_blocks[req_id].get_block_ids(allow_none=True)
  )

  2. 步间状态更新

  2.1 AsyncScheduler 的特殊处理

  async_scheduler.py:13-41 中，AsyncScheduler._update_after_schedule() 在父类基础上增加了异步调度的 placeholder 管理：

  # 计算完成后，为下一步预留 placeholder
  if request.num_computed_tokens == request.num_tokens + request.num_output_placeholders + cur_num_spec_tokens:
      request.num_output_placeholders += 1 + cur_num_spec_tokens
      request.spec_token_ids = [-1] * self.num_spec_tokens

  2.2 num_computed_tokens 的更新时机

  调度后立即更新 (scheduler.py:750-764)：
  def _update_after_schedule(self, scheduler_output):
      for req_id, num_scheduled_token in num_scheduled_tokens.items():
          request = self.requests[req_id]
          # 立即推进 num_computed_tokens
          request.num_computed_tokens += num_scheduled_token

  投机解码拒绝时回退 (scheduler.py:1070-1080)：
  if scheduled_spec_token_ids:
      num_rejected = num_draft_tokens - num_accepted
      if request.num_computed_tokens > 0:
          request.num_computed_tokens -= num_rejected  # 回退被拒绝的 token

  3. Worker 层：Block Table 构建

  3.1 GPUModelRunner._update_states()

  在 gpu_model_runner.py:709-993 中，Worker 接收 SchedulerOutput 并更新本地状态：

  # 新请求：初始化 block_ids
  req_state = CachedRequestState(
      ...
      block_ids=new_req_data.block_ids,  # 从 scheduler 传来的块 ID
      num_computed_tokens=new_req_data.num_computed_tokens,
  )

  # 缓存请求：追加新块
  if new_block_ids is not None:
      for block_ids, new_ids in zip(req_state.block_ids, new_block_ids):
          block_ids.extend(new_ids)

  3.2 Block Table 提交到 GPU

  gpu_model_runner.py:1441-1535 中准备 Attention 的输入：

  # 提交 block table 到 GPU
  self.input_batch.block_table.commit_block_table(num_reqs)

  # 计算 slot mapping（token 到 KV cache 槽位的映射）
  self.input_batch.block_table.compute_slot_mapping(req_indices, positions_np)
  self.input_batch.block_table.commit_slot_mapping(total_num_scheduled_tokens)

  4. Attention 计算中的 KV Cache 使用

  4.1 CommonAttentionMetadata 构建

  gpu_model_runner.py:1778-1797 构建 Attention 元数据：

  common_attn_metadata = CommonAttentionMetadata(
      query_start_loc=query_start_loc,      # 每个请求 query 的起始位置
      seq_lens=seq_lens,                     # 每个请求的序列长度
      block_table_tensor=blk_table_tensor,   # 块表：[num_reqs, max_blocks]
      slot_mapping=slot_mapping,             # 槽位映射：token -> KV cache 槽位
      ...
  )

  4.2 关键张量说明

  | 张量                 | 形状                             | 用途                               |
  |--------------------|--------------------------------|----------------------------------|
  | block_table_tensor | [num_reqs, max_blocks_per_req] | 每个请求分配的物理块 ID                    |
  | slot_mapping       | [total_tokens]                 | 当前 step 每个 token 写入的 KV cache 槽位 |
  | seq_lens           | [num_reqs]                     | 每个请求的累计序列长度                      |

  4.3 Attention 计算流程

  1. 模型前向 → 产生当前 step 的 K, V
           ↓
  2. reshape_and_cache(K, V, kv_cache, slot_mapping)
     - 使用 slot_mapping 将新 K, V 写入对应槽位
           ↓
  3. attention_kernel(Q, kv_cache, block_table, seq_lens)
     - 使用 block_table 索引历史 KV
     - 使用 seq_lens 确定有效范围

  5. 完整数据流图

  ┌─────────────────────────────────────────────────────────────────┐
  │                         EngineCore.step()                        │
  └─────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │                    Scheduler.schedule()                          │
  │  ┌─────────────────────────────────────────────────────────┐    │
  │  │  KVCacheManager.allocate_slots()                        │    │
  │  │  - 计算需要的新块数                                       │    │
  │  │  - 从 free_block_queue 分配物理块                        │    │
  │  │  - 返回 KVCacheBlocks (包含 block_ids)                   │    │
  │  └─────────────────────────────────────────────────────────┘    │
  │                              │                                   │
  │                              ▼                                   │
  │  ┌─────────────────────────────────────────────────────────┐    │
  │  │  _update_after_schedule()                               │    │
  │  │  - 更新 request.num_computed_tokens                     │    │
  │  └─────────────────────────────────────────────────────────┘    │
  └─────────────────────────────────────────────────────────────────┘
                                  │
              SchedulerOutput (含 block_ids, num_scheduled_tokens)
                                  │
                                  ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │                  GPUModelRunner.execute_model()                  │
  │  ┌─────────────────────────────────────────────────────────┐    │
  │  │  _update_states()                                       │    │
  │  │  - 更新 CachedRequestState.block_ids                    │    │
  │  │  - 追加 new_block_ids 到现有列表                         │    │
  │  └─────────────────────────────────────────────────────────┘    │
  │                              │                                   │
  │                              ▼                                   │
  │  ┌─────────────────────────────────────────────────────────┐    │
  │  │  _prepare_inputs()                                      │    │
  │  │  - commit_block_table(): 同步 block_ids 到 GPU          │    │
  │  │  - compute_slot_mapping(): 计算 token→slot 映射         │    │
  │  └─────────────────────────────────────────────────────────┘    │
  │                              │                                   │
  │                              ▼                                   │
  │  ┌─────────────────────────────────────────────────────────┐    │
  │  │  _build_attention_metadata()                            │    │
  │  │  - 构建 CommonAttentionMetadata                         │    │
  │  │  - builder.build() 生成后端特定的 AttentionMetadata     │    │
  │  └─────────────────────────────────────────────────────────┘    │
  │                              │                                   │
  │                              ▼                                   │
  │  ┌─────────────────────────────────────────────────────────┐    │
  │  │  Model Forward                                          │    │
  │  │  - Attention Layer 使用 block_table 读取历史 KV        │    │
  │  │  - 使用 slot_mapping 写入新 KV                         │    │
  │  └─────────────────────────────────────────────────────────┘    │
  └─────────────────────────────────────────────────────────────────┘
                                  │
                                  ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │                Scheduler.update_from_output()                    │
  │  - 处理投机解码的接受/拒绝                                        │
  │  - 调整 num_computed_tokens（若有拒绝）                           │
  │  - 释放已完成请求的 KV cache 块                                   │
  └─────────────────────────────────────────────────────────────────┘

  6. 关键设计要点

  1. 延迟分配：块只在实际需要时分配，支持 chunked prefill
  2. Prefix Caching：get_computed_blocks() 复用已缓存的 KV blocks
  3. 异步调度：AsyncScheduler 使用 num_output_placeholders 预留空间，允许调度与执行重叠
  4. 投机解码兼容：通过 num_lookahead_tokens 预留额外槽位，拒绝时通过 num_computed_tokens -= num_rejected 回退

Q: 了解，现在请你分析 ngram gpu 投机解码与 async scheduler 的适配中，针对每个输入的 scheduler output 里各个请求对应的 draft token ids 
的预先处理。请你判断我下面的理解是否正确：\
\
第 N 步 Scheduler， engine 通过 @vllm/v1/engine/core.py 中的 step_with_batch_queue 函数使 async scheduler 进行调度，对当前批次内每个请求预先分配第 N  步 
execute model 需要用到的 num_draft_tokens 数量的投机解码 placeholder；\
第 N 步 Worker，接受 Scheduler Output，并根据各项参数构建每个请求的 kv cache 位置以及 positions，这其中包括了第 N-1 步 propose 出来的 draft token 
ids，以及一个主模型 sampled token ids，之后经过一次 model 

A: 你的理解整体框架是正确的，但有一些细节需要澄清和修正。让我详细分析 ngram GPU 投机解码与 async scheduler 的适配流程：

  你的理解分析

  ✅ 正确的部分

  1. AsyncScheduler 预分配 placeholder - 确实在 _update_after_schedule 中预留空间
  2. Worker 构建 KV cache 位置 - 包含上一步的 draft tokens
  3. Drafter 结果缓存在 gpu_model_runner - _draft_token_ids 字段
  4. Scheduler 根据验证结果回退 - 调整 num_computed_tokens

  ⚠️ 需要修正/澄清的部分

  1. Draft tokens 的来源时序：第 N 步使用的是第 N-1 步 propose 的 draft tokens，但这些 tokens 在 async 模式下是通过特殊机制传递的
  2. 验证结果的传递路径：valid tokens 不是直接传给 scheduler，而是通过 ModelRunnerOutput 间接传递

  详细流程分析

  让我读取更多关键代码来验证：

● Read(vllm/v1/worker/gpu_model_runner.py)
  ⎿  Read 500 lines

● Read(vllm/v1/spec_decode/ngram_proposer_gpu.py)
  ⎿  Read 525 lines

● 让我继续查看 propose_draft_token_ids 函数和更多相关代码：

● Read(vllm/v1/worker/gpu_model_runner.py)
  ⎿  Read 400 lines

● 现在让我查看 _update_states 中关于 draft tokens 的处理：

● Read(vllm/v1/worker/gpu_model_runner.py)
  ⎿  Read 100 lines

● 现在我对整个流程有了清晰的理解。让我给出完整的分析：

  ---
  你的理解分析及修正

  整体评价：大方向正确，但有几处细节需要修正

  ---
  详细流程（以第 N 步为例）

  第 N 步的完整时序图

  ┌────────────────────────────────────────────────────────────────────────────┐
  │                              第 N-1 步（上一步）                             │
  ├────────────────────────────────────────────────────────────────────────────┤
  │  Worker 端:                                                                 │
  │  1. sample_tokens() 完成后，调用 propose_draft_token_ids()                  │
  │  2. NgramProposerGPU.propose() 生成 draft_tokens                           │
  │  3. 结果缓存到 self._draft_token_ids (GPU tensor)                          │
  │  4. self._is_empty_draft_tokens 标记哪些请求的 draft 无效                   │
  │  5. prev_sampled_token_ids 缓存采样结果                                     │
  │  6. prev_req_id_to_index 缓存请求顺序映射                                   │
  └────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                              第 N 步 Scheduler                              │
  ├────────────────────────────────────────────────────────────────────────────┤
  │  AsyncScheduler.schedule() + _update_after_schedule():                      │
  │                                                                             │
  │  1. 遍历每个 RUNNING 请求                                                   │
  │  2. 检查: num_computed_tokens == num_tokens + num_output_placeholders       │
  │           + cur_num_spec_tokens（上次调度的 spec tokens）                   │
  │  3. 如果条件满足，为第 N 步预留 placeholder:                                │
  │     request.num_output_placeholders += 1 + cur_num_spec_tokens             │
  │     request.spec_token_ids = [-1] * self.num_spec_tokens                   │
  │                                                                             │
  │  输出 SchedulerOutput:                                                      │
  │  - scheduled_spec_decode_tokens: {req_id: spec_token_ids}                  │
  │    ⚠️ 此处的 spec_token_ids 是 [-1, -1, ...] 占位符！                       │
  │  - num_scheduled_tokens: 包含 1（本步新token） + num_spec_tokens            │
  └────────────────────────────────────────────────────────────────────────────┘
                                      │
                      SchedulerOutput（含 placeholder spec_token_ids）
                                      │
                                      ▼
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                              第 N 步 Worker                                 │
  ├────────────────────────────────────────────────────────────────────────────┤
  │  execute_model() → _update_states():                                        │
  │                                                                             │
  │  1. 从 scheduler_output.scheduled_spec_decode_tokens 获取 spec_token_ids   │
  │     ⚠️ 此时仍是 [-1, -1, ...] 占位符                                        │
  │                                                                             │
  │  2. 检查 _draft_token_ids 是否有效（第 N-1 步生成的）                       │
  │     - 若 _draft_token_ids is None → draft_tokens_invalid = True            │
  │     - 若 _is_empty_draft_tokens[prev_idx] = True → draft_tokens_invalid     │
  │                                                                             │
  │  3. 若 draft_tokens_invalid 且有 spec_tokens:                               │
  │     scheduler_output.total_num_scheduled_tokens -= num_spec_tokens          │
  │     scheduler_output.num_scheduled_tokens[req_id] -= num_spec_tokens        │
  │     scheduler_output.scheduled_spec_decode_tokens.pop(req_id)               │
  │     ⚠️ 这里修改的是 deepcopy 后的 scheduler_output！                        │
  │                                                                             │
  │  4. _prepare_input_ids():                                                   │
  │     - 从 prev_sampled_token_ids 获取第 N-1 步采样的 token                  │
  │     - 从 _draft_token_ids 获取第 N-1 步 propose 的 draft tokens            │
  │     - 通过 scatter 操作填充到 input_ids GPU tensor                         │
  │                                                                             │
  │  输入组成（第 N 步 model forward）:                                         │
  │  - 第 N-1 步采样的 verified token (1个)                                    │
  │  - 第 N-1 步 propose 的 draft tokens (k个)                                 │
  │  - 共 1+k 个 token 参与计算                                                │
  └────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                          第 N 步 Model Forward                              │
  ├────────────────────────────────────────────────────────────────────────────┤
  │  1. 计算所有输入 token 的 hidden states                                     │
  │  2. 计算 logits（包含 1+k 个位置的 logits）                                │
  │  3. Rejection Sampler 验证 draft tokens                                    │
  │     - 接受的 token: valid_sampled_token_ids                                │
  │     - 拒绝后只保留到第一个被拒绝位置 + bonus token                         │
  └────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                          第 N 步 Drafter (propose)                          │
  ├────────────────────────────────────────────────────────────────────────────┤
  │  NgramProposerGPU.propose():                                                │
  │                                                                             │
  │  1. prepare_next_token_ids_padded(): 准备下一个 token 位置                  │
  │     - 使用本步采样结果更新 token_ids_gpu_tensor                            │
  │     - 计算 valid_sampled_tokens_count                                      │
  │                                                                             │
  │  2. propose(): 执行 ngram 匹配                                             │
  │     - 输入: 更新后的 token_ids_gpu_tensor                                  │
  │     - 输出: draft_tokens [batch_size, k]                                   │
  │            is_empty_draft_tokens [batch_size]                              │
  │                                                                             │
  │  3. 结果缓存:                                                              │
  │     self._draft_token_ids = draft_tokens （供第 N+1 步使用）               │
  │     self._is_empty_draft_tokens = is_empty_draft_tokens                    │
  │                                                                             │
  │  4. _copy_valid_sampled_token_count():                                     │
  │     异步拷贝 valid_sampled_tokens_count 到 CPU                             │
  │     供第 N+1 步 _update_states 使用                                        │
  └────────────────────────────────────────────────────────────────────────────┘
                                      │
                        ModelRunnerOutput（含 sampled_token_ids）
                                      │
                                      ▼
  ┌────────────────────────────────────────────────────────────────────────────┐
  │                      第 N 步 Scheduler 收尾                                 │
  ├────────────────────────────────────────────────────────────────────────────┤
  │  update_from_output():                                                      │
  │                                                                             │
  │  1. 获取 generated_token_ids = sampled_token_ids[req_index]                │
  │                                                                             │
  │  2. 如果有 scheduled_spec_token_ids (第 N-1 步的 draft tokens):            │
  │     num_draft_tokens = len(scheduled_spec_token_ids)                       │
  │     num_accepted = len(generated_token_ids) - 1  (减去 bonus)              │
  │     num_rejected = num_draft_tokens - num_accepted                         │
  │                                                                             │
  │  3. 调整 num_computed_tokens:                                               │
  │     request.num_computed_tokens -= num_rejected                            │
  │                                                                             │
  │  4. 调整 num_output_placeholders (async scheduler):                        │
  │     request.num_output_placeholders -= num_rejected                        │
  │                                                                             │
  │  ⚠️ 注意: KV cache 的"回退"不是真正删除，而是逻辑上的回退                  │
  │     - num_computed_tokens 减少后，下一步会重新分配那些位置                  │
  │     - KV cache blocks 本身不会被释放（仍然分配给该请求）                    │
  └────────────────────────────────────────────────────────────────────────────┘

  ---
  关键修正点

  1. ❌ "第 N 步分配的 draft token placeholder"

  你的理解：Scheduler 为第 N 步 execute model 分配 draft tokens placeholder

  修正：Scheduler 分配的 placeholder（num_output_placeholders）是为异步调度的流水线预留的，不是为 draft tokens 预留 KV cache。实际的 draft tokens 来自第 N-1 
  步 propose 的结果，存储在 _draft_token_ids 中。

  2. ❌ "验证结果传递给 scheduler"

  你的理解：valid draft token ids 结果传递给 scheduler

  修正：Worker 不直接传递 valid draft token ids 给 Scheduler。传递的是 sampled_token_ids（通过 ModelRunnerOutput），Scheduler 通过比较：
  - len(generated_token_ids) - 实际接受的 token 数（含 bonus）
  - len(scheduled_spec_token_ids) - 调度的 draft tokens 数

  来推断有多少 draft tokens 被拒绝。

  3. ⚠️ "回退 KV cache"

  你的理解：回退 KV cache

  修正：这里的"回退"是逻辑上的回退（num_computed_tokens -= num_rejected），而不是物理释放 KV cache blocks。已写入 KV cache 的数据仍然存在，但下一步调度时会从
  num_computed_tokens 位置开始重新计算，覆盖那些被拒绝位置的 KV。

  ---
  NgramGPU 与 AsyncScheduler 特殊适配

  关键数据流

  # gpu_model_runner.py:957-982
  if self.use_async_scheduling:
      req_state.prev_num_draft_len = num_spec_tokens

      # 检查 draft tokens 是否有效
      draft_tokens_invalid = self._draft_token_ids is None
      if not draft_tokens_invalid and self._is_empty_draft_tokens is not None:
          prev_idx = self.input_batch.prev_req_id_to_index.get(req_id)
          if prev_idx is not None:
              draft_tokens_invalid = self._is_empty_draft_tokens[prev_idx].item()

      # 若无效，从本步调度中移除
      if num_spec_tokens and draft_tokens_invalid:
          scheduler_output.total_num_scheduled_tokens -= num_spec_tokens
          scheduler_output.num_scheduled_tokens[req_id] -= num_spec_tokens
          scheduler_output.scheduled_spec_decode_tokens.pop(req_id, None)

  这段代码的作用是：当 ngram GPU proposer 返回空的 draft tokens（is_empty_draft_tokens = True）时，动态修改 scheduler_output，避免为无效的 draft tokens 分配
  KV cache 和计算资源。

  为什么需要 deepcopy(scheduler_output)？

  # gpu_model_runner.py:2947-2953
  if (
      self.use_async_scheduling
      and self.num_spec_tokens
      and self._draft_token_ids is None
      or self.speculative_config.method == "ngram_gpu"
  ):
      scheduler_output = deepcopy(scheduler_output)

  因为 Worker 会修改 scheduler_output 的内容（移除无效的 draft tokens），必须用 deepcopy 避免影响 EngineCore 进程中的原始数据。

  ---
  总结：你的理解 vs 实际流程

  | 你的理解                                          | 实际流程                                                               | 正确性     |
  |-----------------------------------------------|--------------------------------------------------------------------|---------|
  | 第 N 步 Scheduler 预分配 draft tokens placeholder  | Scheduler 分配 num_output_placeholders 用于异步流水线，draft tokens 来自 N-1 步 | ⚠️ 部分正确 |
  | 第 N 步 Worker 使用 N-1 步的 draft tokens           | ✅ 正确，从 _draft_token_ids 获取                                         | ✅ 正确    |
  | Model forward + sampler 得到 valid token ids    | ✅ 正确，Rejection Sampler 验证并输出                                       | ✅ 正确    |
  | Drafter propose 结果缓存在 gpu_model_runner        | ✅ 正确，_draft_token_ids 和 _is_empty_draft_tokens                     | ✅ 正确    |
  | Valid draft token ids 传递给 Scheduler           | 传递的是 sampled_token_ids，Scheduler 通过对比推断拒绝数                         | ❌ 细节不对  |
  | Scheduler 更新 num_computed_tokens 并回退 KV cache | 只是逻辑回退（num_computed_tokens -= num_rejected），不释放物理 blocks           | ⚠️ 部分正确 |