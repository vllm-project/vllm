ARG CUDA_VERSION=12.8.1
ARG PYTHON_VERSION=3.13

ARG BUILD_BASE_IMAGE=vannevarlabs.jfrog.io/vl-general/vannevarlabs.com/pytorch:latest-dev
ARG FINAL_BASE_IMAGE=vannevarlabs.jfrog.io/vl-general/vannevarlabs.com/pytorch:latest-dev

ARG VL_PYPI_USER

# PyTorch provides its own indexes for standard and nightly builds
ARG PYTORCH_CUDA_INDEX_BASE_URL=https://download.pytorch.org/whl
ARG PYTORCH_CUDA_NIGHTLY_INDEX_BASE_URL=https://download.pytorch.org/whl/nightly

# PIP supports multiple authentication schemes, including keyring
# By parameterizing the PIP_KEYRING_PROVIDER variable and setting it to
# disabled by default, we allow third-party to use keyring authentication for
# their private Python indexes, while not changing the default behavior which
# is no authentication.
#
# Reference: https://pip.pypa.io/en/stable/topics/authentication/#keyring-support
ARG PIP_KEYRING_PROVIDER=disabled
ARG UV_KEYRING_PROVIDER=${PIP_KEYRING_PROVIDER}

# Flag enables built-in KV-connector dependency libs into docker images
ARG INSTALL_KV_CONNECTORS=true

#################### BASE BUILD IMAGE ####################
FROM ${BUILD_BASE_IMAGE} AS base
ARG CUDA_VERSION
ARG PYTHON_VERSION
ARG TARGETPLATFORM
ARG PYTORCH_CUDA_INDEX_BASE_URL
ARG INSTALL_KV_CONNECTORS=true
ARG VL_PYPI_USER

WORKDIR /workspace

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500
ENV UV_INDEX_STRATEGY="unsafe-best-match"
# Use copy mode to avoid hardlink failures with Docker cache mounts
ENV UV_LINK_MODE=copy

# Add build and runtime dependencies
COPY requirements/common.txt requirements/common.txt
COPY requirements/cuda.txt requirements/cuda.txt

USER root
RUN update-alternatives --force --install /usr/local/bin/gcc gcc /usr/local/bin/gcc-12 110 --slave /usr/local/bin/g++ g++ /usr/local/bin/g++-12
RUN <<EOF
gcc --version
EOF

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
RUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

# install build and runtime dependencies
# NOTE: alt --index-url https://${VL_PYPI_USER}:${VL_PYPI_PASSWORD}@vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=ARTY_NETRC,target=/root/.netrc,required \
    uv pip install --system -r requirements/cuda.txt \
    --index-url https://vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple \
    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')
USER nonroot

# cuda arch list used by torch
# can be useful for both `dev` and `test`
# explicitly set the list to avoid issues with torch 2.2
# see https://github.com/pytorch/pytorch/pull/123243
ARG torch_cuda_arch_list='7.0 7.5 8.0 8.9 9.0 10.0 12.0'
ENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}
#################### BASE BUILD IMAGE ####################

#################### WHEEL BUILD IMAGE ####################
FROM base AS build
ARG TARGETPLATFORM

ARG PIP_INDEX_URL UV_INDEX_URL
ARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL
ARG PYTORCH_CUDA_INDEX_BASE_URL
ARG VL_PYPI_USER

# install build dependencies
COPY requirements/build.txt requirements/build.txt

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500
ENV UV_INDEX_STRATEGY="unsafe-best-match"
# Use copy mode to avoid hardlink failures with Docker cache mounts
ENV UV_LINK_MODE=copy

USER root
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=ARTY_NETRC,target=/root/.netrc,required \
    uv pip install --system -r requirements/build.txt \
    --index-url https://vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple \
    --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')
USER nonroot

COPY . .

# max jobs used by Ninja to build extensions
ARG max_jobs=2
ENV MAX_JOBS=${max_jobs}
# number of threads used by nvcc
ARG nvcc_threads=8
ENV NVCC_THREADS=$nvcc_threads

# Flag to control whether to use pre-built vLLM wheels
ARG VLLM_USE_PRECOMPILED=""
ARG VLLM_MAIN_CUDA_VERSION=""

ARG vllm_target_device="cuda"
ENV VLLM_TARGET_DEVICE=${vllm_target_device}
ENV CCACHE_DIR=/root/.cache/ccache

USER root
RUN --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=.git,target=.git  \
    # Clean any existing CMake artifacts
    rm -rf .deps && \
    mkdir -p .deps && \
    export VLLM_USE_PRECOMPILED="${VLLM_USE_PRECOMPILED}" && \
    export VLLM_DOCKER_BUILD_CONTEXT=1 && \
    export CC=/usr/local/bin/gcc-12 && \
    export CXX=/usr/local/bin/g++-12 && \
    export NVCC_PREPEND_FLAGS='-ccbin=/usr/local/bin/g++-12' && \
    export NVCC_APPEND_FLAGS='-ccbin=/usr/local/bin/g++-12' && \
    python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 
USER nonroot
#################### EXTENSION Build IMAGE ####################

#################### vLLM installation IMAGE ####################
# image with vLLM installed
FROM ${FINAL_BASE_IMAGE} AS vllm-base
ARG CUDA_VERSION
ARG PYTHON_VERSION
ARG INSTALL_KV_CONNECTORS=false
WORKDIR /vllm-workspace
ARG TARGETPLATFORM
ARG VL_PYPI_USER

# ARG GDRCOPY_CUDA_VERSION=12.8
# Keep in line with FINAL_BASE_IMAGE
# TODO: update this OS version for Alpine linux
# ARG GDRCOPY_OS_VERSION=Ubuntu22_04

SHELL ["/bin/bash", "-c"]

USER root
RUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\.//g') && \
    echo "export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}" >> /etc/profile
USER nonroot

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500
ENV UV_INDEX_STRATEGY="unsafe-best-match"
# Use copy mode to avoid hardlink failures with Docker cache mounts
ENV UV_LINK_MODE=copy

COPY ./vllm/collect_env.py .

# Workaround for https://github.com/openai/triton/issues/2507 and
# https://github.com/pytorch/pytorch/issues/107960 -- hopefully
# this won't be needed for future versions of this docker image
# or future versions of triton.
USER root
RUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/

# Install vllm wheel first, so that torch etc will be installed.
RUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \
    --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=ARTY_NETRC,target=/root/.netrc,required \
    uv pip install --system dist/*.whl --verbose \
        --index-url https://vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple \
        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')

RUN --mount=type=cache,target=/root/.cache/uv \
. /etc/profile && \
uv pip list
USER nonroot

# Even when we build Flashinfer with AOT mode, there's still
# some issues w.r.t. JIT compilation. Therefore we need to
# install build dependencies for JIT compilation.
# TODO: Remove this once FlashInfer AOT wheel is fixed
COPY requirements/build.txt requirements/build.txt
# COPY tools/install_gdrcopy.sh install_gdrcopy.sh
COPY tools/ep_kernels/install_python_libraries.sh install_python_libraries.sh

# ENV CUDA_HOME=/usr/local/cuda

USER root
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=ARTY_NETRC,target=/root/.netrc,required \
    uv pip install --system -r requirements/build.txt \
        --index-url https://vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple \
        --extra-index-url ${PYTORCH_CUDA_INDEX_BASE_URL}/cu$(echo $CUDA_VERSION | cut -d. -f1,2 | tr -d '.')

# NOTE: deepseek-ai packages have been removed due to national security concerns

# NOTE: disabling for now due to build errors. This will prevent us from using
# ExpertParallel and disaggregated serving
# RUN git clone --branch v2.5.1 https://github.com/NVIDIA/gdrcopy.git && \
#     cd gdrcopy && \
#     make prefix=/usr/local/gdrcopy CUDA=/usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2) all install && \
#     ./insmod.sh

# RUN set -eux; \
# case "${TARGETPLATFORM}" in \
#     linux/arm64) UUARCH="aarch64" ;; \
#     linux/amd64) UUARCH="x64" ;; \
#     *) echo "Unsupported TARGETPLATFORM: ${TARGETPLATFORM}" >&2; exit 1 ;; \
# esac; \
# ./install_gdrcopy.sh "${GDRCOPY_OS_VERSION}" "${GDRCOPY_CUDA_VERSION}" "${UUARCH}"; \
# rm ./install_gdrcopy.sh


# Install EP kernels from perplexity(pplx-kernels)
# NOTE: DeepEP from DeepSeek has been removed due to national security concerns
ENV CMAKE_EXTRA_ARGS="-DNVSHMEM_IBGDA_SUPPORT=OFF"
RUN export TORCH_CUDA_ARCH_LIST="${TORCH_CUDA_ARCH_LIST:-9.0a+PTX}" \
    export CUDA_HOME=/usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2) && \
    export PATH="${CUDA_HOME}/bin:${PATH}" && \
    export LD_LIBRARY_PATH="${CUDA_HOME}/lib64" && \
    export CC=/usr/local/bin/gcc-12 && \
    export CXX=/usr/local/bin/g++-12 && \
    export NVCC_PREPEND_FLAGS='-ccbin=/usr/local/bin/g++-12' && \
    export NVCC_APPEND_FLAGS='-ccbin=/usr/local/bin/g++-12' && \
    bash install_python_libraries.sh
USER nonroot
#################### vLLM installation IMAGE ####################

#################### OPENAI API SERVER ####################
# base openai image with additional requirements, for any subsequent openai-style images
FROM vllm-base AS vllm-openai-base
ARG TARGETPLATFORM
ARG INSTALL_KV_CONNECTORS=true

ARG PIP_INDEX_URL UV_INDEX_URL
ARG PIP_EXTRA_INDEX_URL UV_EXTRA_INDEX_URL

# This timeout (in seconds) is necessary when installing some dependencies via uv since it's likely to time out
# Reference: https://github.com/astral-sh/uv/pull/1694
ENV UV_HTTP_TIMEOUT=500

COPY requirements/kv_connectors.txt requirements/kv_connectors.txt

# install additional dependencies for openai api server
USER root
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=ARTY_NETRC,target=/root/.netrc,required \
    if [ "$INSTALL_KV_CONNECTORS" = "true" ]; then \
        uv pip install \
        --index-url https://vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple \
        --system -r requirements/kv_connectors.txt; \
    fi; \
    if [ "$TARGETPLATFORM" = "linux/arm64" ]; then \
        BITSANDBYTES_VERSION="0.42.0"; \
    else \
        BITSANDBYTES_VERSION="0.46.1"; \
    fi; \
    uv pip install \
    --index-url https://vannevarlabs.jfrog.io/artifactory/api/pypi/vl-python-pypi/simple \
    --system \
    accelerate hf_transfer "bitsandbytes>=${BITSANDBYTES_VERSION}" 'timm>=1.0.17' boto3 runai-model-streamer runai-model-streamer[s3]
USER nonroot

ENV VLLM_USAGE_SOURCE=production-docker-image

FROM vllm-openai-base AS vllm-openai

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
#################### OPENAI API SERVER ####################
