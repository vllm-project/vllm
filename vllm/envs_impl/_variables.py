# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

"""All env var declarations with type annotations and defaults.

Do NOT import from this module directly. Use ``import vllm.envs_impl as envs``.
"""

import os
import tempfile
from pathlib import Path
from typing import TYPE_CHECKING, Annotated, Literal

from pydantic.fields import FieldInfo

if not TYPE_CHECKING:
    from pydantic import Field
    from pydantic.functional_validators import BeforeValidator

    from vllm.envs_impl.utils import parse_list, parse_path
else:
    from typing import Any

    def Field(*args, **kwargs) -> Any: ...  # type: ignore[misc]

    class BeforeValidator:  # type: ignore[misc]
        def __init__(self, fn): ...

    def parse_list(value: str, separator: str = ","): ...

    def parse_path(value: str): ...


def disable_compile_cache() -> bool:
    return bool(int(os.getenv("VLLM_DISABLE_COMPILE_CACHE", "0")))


def use_aot_compile() -> bool:
    from vllm.model_executor.layers.batch_invariant import (
        vllm_is_batch_invariant,
    )
    from vllm.utils.torch_utils import is_torch_equal_or_newer

    default_value = (
        "1"
        if is_torch_equal_or_newer("2.10.0.dev") and not disable_compile_cache()
        else "0"
    )

    return (
        not vllm_is_batch_invariant()
        and os.environ.get("VLLM_USE_AOT_COMPILE", default_value) == "1"
    )


# ── Parse helpers (used as BeforeValidator arguments) ──────────────────────────


def _get_vllm_port(env_port: str) -> int:
    try:
        return int(env_port)
    except ValueError as err:
        from urllib.parse import urlparse

        parsed = urlparse(env_port)
        if parsed.scheme:
            raise ValueError(
                f"VLLM_PORT '{env_port}' appears to be a URI. "
                "This may be caused by a Kubernetes service discovery issue, "
                "check the warning in: https://docs.vllm.ai/en/stable/serving/env_vars.html"
            ) from None
        raise ValueError(f"VLLM_PORT '{env_port}' must be a valid integer") from err


def _parse_optional_bool_int(value: str) -> bool | None:
    return bool(int(value))


def _parse_log_stats_interval(v: str) -> float:
    val = float(v)
    return val if val > 0.0 else 10.0


def _parse_mcp_labels(v: str) -> set[str]:
    valid = {"container", "code_interpreter", "web_search_preview"}
    values = {item.strip() for item in v.split(",") if item.strip()}
    invalid = values - valid
    if invalid:
        raise ValueError(
            f"Invalid VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS values: {invalid}. "
            f"Must be a subset of: {valid}"
        )
    return values


# ── Callable defaults ───────────────────────────────────────────────────────────


def _get_default_cache_root() -> str:
    cache_home = os.getenv("XDG_CACHE_HOME", os.path.expanduser("~/.cache"))
    return os.path.join(cache_home, "vllm")


def _get_xla_cache_path() -> str:
    cache_root = os.getenv("VLLM_CACHE_ROOT")
    if cache_root:
        return os.path.join(cache_root, "xla_cache")
    return os.path.join(_get_default_cache_root(), "xla_cache")


def _get_assets_cache_path() -> str:
    cache_root = os.getenv("VLLM_CACHE_ROOT")
    if cache_root:
        return os.path.join(cache_root, "assets")
    return os.path.join(_get_default_cache_root(), "assets")


def _get_dnt_fallback() -> bool:
    return os.getenv("DO_NOT_TRACK", "0") == "1"


def _get_aot_compile_default() -> bool:
    from vllm.model_executor.layers.batch_invariant import (
        vllm_is_batch_invariant,
    )
    from vllm.utils.torch_utils import is_torch_equal_or_newer

    return (
        not vllm_is_batch_invariant()
        and is_torch_equal_or_newer("2.10.0.dev")
        and not disable_compile_cache()
    )


def _parse_aot_compile(value: str) -> bool:
    from vllm.model_executor.layers.batch_invariant import (
        vllm_is_batch_invariant,
    )

    return not vllm_is_batch_invariant() and value == "1"


# ── Network and Communication ───────────────────────────────────────────────────

VLLM_HOST_IP: str = Field(
    default="",
    description=(
        "Used in distributed environment to determine the IP address of the "
        "current node, when the node has multiple network interfaces. "
        "If you are using multi-node inference, you should set this "
        "differently on each node."
    ),
)
VLLM_PORT: Annotated[int | None, BeforeValidator(_get_vllm_port)] = Field(
    default=None,
    description=(
        "Used in distributed environment to manually set the communication port. "
        "Note: if VLLM_PORT is set, and some code asks for multiple ports, "
        "the VLLM_PORT will be used as the first port, and the rest will be "
        "generated by incrementing the VLLM_PORT value."
    ),
)
VLLM_RPC_BASE_PATH: str = Field(
    default_factory=tempfile.gettempdir,
    description="Base path for RPC socket files. Defaults to system temp directory.",
)
VLLM_RPC_TIMEOUT: int = Field(default=10000, description="RPC timeout in milliseconds.")
VLLM_HTTP_TIMEOUT_KEEP_ALIVE: int = Field(
    default=5, description="HTTP keep-alive timeout in seconds."
)

# ── Feature Flags ───────────────────────────────────────────────────────────────

VLLM_USE_MODELSCOPE: bool = Field(
    default=False,
    description="If set, vllm will use ModelScope instead of Hugging Face Hub.",
)
VLLM_RINGBUFFER_WARNING_INTERVAL: int = Field(
    default=60, description="Interval in seconds for ring buffer warnings."
)
VLLM_NCCL_SO_PATH: str | None = Field(
    default=None, description="Path to NCCL shared library (.so file)."
)
LD_LIBRARY_PATH: str | None = Field(
    default=None, description="Standard LD_LIBRARY_PATH environment variable."
)

# ── Device and Performance ──────────────────────────────────────────────────────

VLLM_ROCM_SLEEP_MEM_CHUNK_SIZE: int = Field(
    default=256, description="ROCm memory chunk size for sleep operations (in MB)."
)
VLLM_ROCM_FP8_MFMA_PAGE_ATTN: bool = Field(
    default=False,
    description=(
        "(ROCm only) Enable FP8 MFMA (Matrix Fused Multiply-Add) for paged attention."
    ),
)
VLLM_ROCM_USE_AITER: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for data loading.",
)
VLLM_ROCM_USE_AITER_FP4_ASM_GEMM: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for FP4 ASM GEMM operations.",
)
VLLM_ROCM_USE_AITER_FP4BMM: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for FP4 BMM operations.",
)
VLLM_ROCM_USE_AITER_FP8BMM: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for FP8 BMM operations.",
)
VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for fusion shared experts.",
)
VLLM_ROCM_USE_AITER_LINEAR: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for linear layers.",
)
VLLM_ROCM_USE_AITER_MHA: bool = Field(
    default=False,
    description=(
        "(ROCm only) Use asynchronous iterators for multi-head attention layers."
    ),
)
VLLM_ROCM_USE_AITER_MLA: bool = Field(
    default=False, description="(ROCm only) Use asynchronous iterators for MLA layers."
)
VLLM_ROCM_USE_AITER_MOE: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for Mixture of Experts layers.",
)
VLLM_ROCM_USE_AITER_RMSNORM: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for RMSNorm layers.",
)
VLLM_ROCM_USE_AITER_TRITON_ROPE: bool = Field(
    default=False,
    description=(
        "(ROCm only) Use asynchronous iterators for Triton RoPE implementation."
    ),
)
VLLM_ROCM_USE_AITER_PAGED_ATTN: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for paged attention.",
)
VLLM_ROCM_USE_AITER_UNIFIED_ATTENTION: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for unified attention.",
)
VLLM_ROCM_USE_AITER_TRITON_GEMM: bool = Field(
    default=False,
    description="(ROCm only) Use asynchronous iterators for Triton GEMM operations.",
)
VLLM_ROCM_SHUFFLE_KV_CACHE_LAYOUT: bool = Field(
    default=False,
    description=(
        "(ROCm only) Shuffle key-value cache layout for better memory access patterns."
    ),
)
LOCAL_RANK: int = Field(
    default=0,
    description=(
        "Local rank of the process in the distributed setting, "
        "used to determine the GPU device id."
    ),
)
CUDA_VISIBLE_DEVICES: str | None = Field(
    default=None,
    description="Used to control the visible devices in the distributed setting.",
)

# ── Timeouts and Intervals ──────────────────────────────────────────────────────

VLLM_ENGINE_ITERATION_TIMEOUT_S: int = Field(
    default=60, description="Timeout for each iteration in the engine (in seconds)."
)
VLLM_ENGINE_READY_TIMEOUT_S: int = Field(
    default=600,
    description=(
        "Timeout in seconds for waiting for engine cores to become ready "
        "during startup. Default is 600 seconds (10 minutes)."
    ),
)

# ── Authentication and Security ─────────────────────────────────────────────────

VLLM_API_KEY: str | None = Field(
    default=None, description="API key for vLLM API server."
)
VLLM_DEBUG_LOG_API_SERVER_RESPONSE: bool = Field(
    default=False,
    description="Whether to log responses from API Server for debugging.",
)
S3_ACCESS_KEY_ID: str | None = Field(
    default=None,
    description="S3 access key ID, used for tensorizer to load model from S3.",
)
S3_SECRET_ACCESS_KEY: str | None = Field(
    default=None,
    description="S3 secret access key, used for tensorizer to load model from S3.",
)
S3_ENDPOINT_URL: str | None = Field(
    default=None,
    description="S3 endpoint URL, used for tensorizer to load model from S3.",
)

# ── Paths and Directories ───────────────────────────────────────────────────────

VLLM_MODEL_REDIRECT_PATH: Annotated[Path | None, BeforeValidator(parse_path)] = Field(
    default=None,
    description=(
        "Path to redirect model loading. "
        "Automatically expands ~ and environment variables."
    ),
)
VLLM_CACHE_ROOT: str = Field(
    default_factory=_get_default_cache_root,
    description=(
        "Root directory for vLLM cache files. "
        "Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set."
    ),
)
VLLM_CONFIG_ROOT: str = Field(
    default_factory=lambda: os.path.expanduser("~/.config/vllm"),
    description=(
        "Root directory for vLLM configuration files. Defaults to `~/.config/vllm`."
    ),
)

# ── Usage Statistics ────────────────────────────────────────────────────────────

VLLM_USAGE_STATS_SERVER: str = Field(
    default="https://stats.vllm.ai",
    description="URL of the usage stats collection server.",
)
VLLM_NO_USAGE_STATS: bool = Field(
    default=False, description="If set to 1, vllm will not send usage stats."
)
VLLM_DO_NOT_TRACK: Annotated[bool, BeforeValidator(lambda v: v.strip() == "1")] = Field(
    default_factory=_get_dnt_fallback,
    description=(
        "Honors DO_NOT_TRACK and VLLM_DO_NOT_TRACK environment variables. "
        "If set to 1, disables usage tracking. "
        "VLLM_DO_NOT_TRACK takes precedence over DO_NOT_TRACK."
    ),
)
VLLM_USAGE_SOURCE: str = Field(
    default="production", description="Source identifier for usage statistics."
)

# ── Logging Configuration ───────────────────────────────────────────────────────

VLLM_CONFIGURE_LOGGING: bool = Field(
    default=True,
    description=(
        "If set to 0, vllm will not configure logging. "
        "If set to 1, vllm will configure logging using the default configuration "
        "or the configuration file specified by VLLM_LOGGING_CONFIG_PATH."
    ),
)
VLLM_LOGGING_LEVEL: Annotated[str, BeforeValidator(str.upper)] = Field(
    default="INFO", description='Logging level for vLLM. Default is "INFO".'
)
VLLM_LOGGING_PREFIX: str = Field(
    default="",
    description="If set, VLLM_LOGGING_PREFIX will be prepended to all log messages.",
)
VLLM_LOGGING_STREAM: str = Field(
    default="ext://sys.stdout",
    description="Stream for logging output. Default is sys.stdout.",
)
VLLM_LOGGING_CONFIG_PATH: str | None = Field(
    default=None, description="Path to custom logging configuration file."
)
VLLM_LOGGING_COLOR: str = Field(
    default="auto",
    description=(
        "Controls colored logging output. "
        'Options: "auto" (default, colors when terminal), '
        '"1" (always use colors), "0" (never use colors).'
    ),
)
NO_COLOR: Annotated[bool, BeforeValidator(lambda _: True)] = Field(
    default=False, description="Standard unix flag for disabling ANSI color codes."
)
VLLM_LOG_STATS_INTERVAL: Annotated[
    float, BeforeValidator(_parse_log_stats_interval)
] = Field(
    default=10.0,
    description=(
        "Interval in seconds for logging stats. "
        "If set to a non-positive value, defaults to 10.0 seconds."
    ),
)

# ── Debugging and Tracing ───────────────────────────────────────────────────────

VLLM_PROCESS_NAME_PREFIX: str = Field(
    default="VLLM",
    description='Prefix for process names in logs and monitoring. Default is "VLLM".',
)
VLLM_TRACE_FUNCTION: int = Field(
    default=0,
    description="If set to 1, vllm will trace function calls. Useful for debugging.",
)

# ── CPU Backend ─────────────────────────────────────────────────────────────────

VLLM_CPU_KVCACHE_SPACE: int = Field(
    default=0, description="(CPU backend only) CPU key-value cache space in GB."
)
VLLM_CPU_OMP_THREADS_BIND: str = Field(
    default="", description="(CPU backend only) OpenMP thread binding configuration."
)
VLLM_CPU_NUM_OF_RESERVED_CPU: int | None = Field(
    default=None, description="(CPU backend only) Number of reserved CPU cores."
)
VLLM_CPU_SGL_KERNEL: bool = Field(
    default=False, description="(CPU backend only) Use SGLang kernel."
)

# ── XLA Backend ─────────────────────────────────────────────────────────────────

VLLM_XLA_CACHE_PATH: str = Field(
    default_factory=_get_xla_cache_path,
    description=(
        "Path for XLA compilation cache. Defaults to VLLM_CACHE_ROOT/xla_cache."
    ),
)
VLLM_XLA_CHECK_RECOMPILATION: bool = Field(
    default=False, description="Check for XLA recompilations and log warnings."
)
VLLM_XLA_USE_SPMD: bool = Field(
    default=False,
    description="Use XLA SPMD (Single Program Multiple Data) partitioning.",
)

# ── Kernel Config ───────────────────────────────────────────────────────────────

VLLM_USE_FLASHINFER_SAMPLER: Annotated[
    bool | None, BeforeValidator(_parse_optional_bool_int)
] = Field(default=None, description="If set, vllm will use flashinfer sampler.")
VLLM_FUSED_MOE_CHUNK_SIZE: int = Field(
    default=16 * 1024, description="Chunk size for fused MOE operations."
)
VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING: bool = Field(
    default=True, description="Enable chunking for fused MOE activations."
)

# ── Distributed Config ──────────────────────────────────────────────────────────

VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE: Literal["auto", "nccl", "shm"] = Field(
    default="auto",
    description="Channel type for Ray compiled DAG. Options: auto, nccl, shm.",
)
VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = Field(
    default=False, description="Use overlapped communication in Ray compiled DAG."
)
VLLM_USE_RAY_WRAPPED_PP_COMM: bool = Field(
    default=True, description="Use Ray-wrapped pipeline parallelism communication."
)
VLLM_PP_LAYER_PARTITION: str | None = Field(
    default=None, description="Pipeline parallelism layer partition strategy."
)
VLLM_WORKER_MULTIPROC_METHOD: Literal["fork", "spawn"] = Field(
    default="fork",
    description="Multiprocessing method for workers. Options: fork, spawn.",
)

# ── Media and Assets ────────────────────────────────────────────────────────────

VLLM_ASSETS_CACHE: str = Field(
    default_factory=_get_assets_cache_path,
    description="Cache directory for downloaded assets (images, videos, audio).",
)
VLLM_ASSETS_CACHE_MODEL_CLEAN: bool = Field(
    default=False, description="Clean model-specific assets from cache."
)
VLLM_IMAGE_FETCH_TIMEOUT: int = Field(
    default=5, description="Timeout in seconds for fetching images."
)
VLLM_VIDEO_FETCH_TIMEOUT: int = Field(
    default=30, description="Timeout in seconds for fetching videos."
)
VLLM_AUDIO_FETCH_TIMEOUT: int = Field(
    default=10, description="Timeout in seconds for fetching audio files."
)
VLLM_MEDIA_URL_ALLOW_REDIRECTS: bool = Field(
    default=True, description="Allow URL redirects when fetching media."
)
VLLM_MEDIA_LOADING_THREAD_COUNT: int = Field(
    default=8, description="Number of threads for loading media files."
)
VLLM_MAX_AUDIO_CLIP_FILESIZE_MB: int = Field(
    default=25, description="Maximum audio clip file size in MB."
)
VLLM_VIDEO_LOADER_BACKEND: str = Field(
    default="opencv", description="Backend for video loading. Default is opencv."
)
VLLM_MEDIA_CONNECTOR: str = Field(
    default="http", description="Connector type for media loading. Default is http."
)
VLLM_MM_HASHER_ALGORITHM: Annotated[
    Literal["blake3", "sha256", "sha512"], BeforeValidator(str.lower)
] = Field(
    default="blake3",
    description=(
        "Hash algorithm for multimodal content hashing. "
        '"blake3": Default, fast cryptographic hash (not FIPS 140-3 compliant). '
        '"sha256": FIPS 140-3 compliant, widely supported. '
        '"sha512": FIPS 140-3 compliant, faster on 64-bit systems. '
        "Use sha256 or sha512 for FIPS compliance in government/enterprise deployments."
    ),
)

# ── Installation Time Env Vars ──────────────────────────────────────────────────

VLLM_TARGET_DEVICE: Annotated[str, BeforeValidator(str.lower)] = Field(
    default="cuda",
    description="Target device of vLLM. Options: cuda (default), rocm, cpu.",
)
VLLM_MAIN_CUDA_VERSION: Annotated[
    str, BeforeValidator(lambda v: v.lower() if v else "12.9")
] = Field(
    default="12.9",
    description=(
        "Main CUDA version of vLLM. This follows PyTorch but can be overridden."
    ),
)
VLLM_FLOAT32_MATMUL_PRECISION: Literal["highest", "high", "medium"] = Field(
    default="highest",
    description=(
        "Controls PyTorch float32 matmul precision mode within vLLM workers. "
        "Valid options mirror torch.set_float32_matmul_precision."
    ),
)
MAX_JOBS: str | None = Field(
    default=None,
    description=(
        "Maximum number of compilation jobs to run in parallel. "
        "By default this is the number of CPUs."
    ),
)
NVCC_THREADS: str | None = Field(
    default=None,
    description=(
        "Number of threads to use for nvcc. By default this is 1. "
        "If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU."
    ),
)
VLLM_USE_PRECOMPILED: Annotated[
    bool, BeforeValidator(lambda v: v.strip().lower() in ("1", "true"))
] = Field(
    default_factory=lambda: os.getenv("VLLM_PRECOMPILED_WHEEL_LOCATION") is not None,
    description=(
        "If set, vllm will use precompiled binaries (*.so). "
        "Also enabled if VLLM_PRECOMPILED_WHEEL_LOCATION is set."
    ),
)
VLLM_SKIP_PRECOMPILED_VERSION_SUFFIX: bool = Field(
    default=False,
    description="If set, skip adding +precompiled suffix to version string.",
)
VLLM_DOCKER_BUILD_CONTEXT: bool = Field(
    default=False,
    description=(
        "Used to mark that setup.py is running in a Docker build context, "
        "in order to force the use of precompiled binaries."
    ),
)
VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = Field(
    default=False, description="Keep the server alive even if the engine dies."
)
CMAKE_BUILD_TYPE: Literal["Debug", "Release", "RelWithDebInfo"] | None = Field(
    default=None,
    description=(
        'CMake build type. If not set, defaults to "Debug" or "RelWithDebInfo".'
    ),
)
VERBOSE: bool = Field(
    default=False, description="Enable verbose output during installation."
)

# ── Model Configuration ─────────────────────────────────────────────────────────

VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = Field(
    default=False, description="Allow model max length to exceed default limits."
)

# ── Plugins ─────────────────────────────────────────────────────────────────────

VLLM_PLUGINS: Annotated[list[str] | None, BeforeValidator(parse_list)] = Field(
    default=None, description="List of vLLM plugins to load (comma-separated)."
)

# ── MCP (Model Context Protocol) Configuration ──────────────────────────────────

VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS: Annotated[
    set[str], BeforeValidator(_parse_mcp_labels)
] = Field(
    default_factory=set,
    description=(
        "Valid server labels for MCP tools. "
        "Example: VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS=container,code_interpreter. "
        "If the server_label of your MCP tool is not in this list, it will be ignored."
    ),
)

# ── LoRA Configuration ──────────────────────────────────────────────────────────

VLLM_LORA_RESOLVER_CACHE_DIR: str | None = Field(
    default=None, description="Cache directory for LoRA resolver."
)

# ── Compilation and Build Flags ─────────────────────────────────────────────────

VLLM_USE_AOT_COMPILE: Annotated[bool, BeforeValidator(_parse_aot_compile)] = Field(
    default_factory=_get_aot_compile_default,
    description=(
        "Enable/disable AOT compilation. Ensures compilation is done in warmup phase "
        "and reused in subsequent calls. Auto-enabled in torch >= 2.10.0 "
        "when compile cache is enabled."
    ),
)
VLLM_USE_BYTECODE_HOOK: bool = Field(
    default=True,
    description="Enable/disable bytecode in TorchCompileWithNoGuardsWrapper.",
)
VLLM_FORCE_AOT_LOAD: bool = Field(
    default=False,
    description=(
        "Force vllm to always load AOT compiled models from disk. "
        "Failure to load will result in a hard error when enabled. "
        "Ignored when VLLM_USE_AOT_COMPILE is disabled."
    ),
)
VLLM_USE_MEGA_AOT_ARTIFACT: bool = Field(
    default=False, description="Use mega AOT artifact for AOT compilation."
)
VLLM_USE_STANDALONE_COMPILE: bool = Field(
    default=True,
    description=(
        "Enable/disable Inductor standalone compile. "
        "In torch <= 2.7 ignored; in torch >= 2.9 enabled by default."
    ),
)
VLLM_DISABLE_COMPILE_CACHE: Annotated[bool, BeforeValidator(lambda v: bool(int(v)))] = (
    Field(default=False, description="Disable torch.compile cache.")
)
VLLM_COMPILE_CACHE_SAVE_FORMAT: Literal["binary", "unpacked"] = Field(
    default="binary",
    description=(
        "Format for saving torch.compile cache artifacts. "
        '"binary": saves as binary file (multiprocess safe). '
        '"unpacked": saves as directory structure (for inspection/debugging, '
        "NOT multiprocess safe)."
    ),
)

# ── Additional Device and Hardware Settings ─────────────────────────────────────

CUDA_HOME: str | None = Field(
    default=None,
    description=(
        "Path to cudatoolkit home directory "
        "(should contain bin, include, and lib directories)."
    ),
)
VLLM_CUDART_SO_PATH: str | None = Field(
    default=None,
    description=(
        "Path to CUDART shared library. Useful when find_loaded_library() doesn't work."
    ),
)
VLLM_NCCL_INCLUDE_PATH: str | None = Field(
    default=None, description="Path to NCCL include directory."
)
VLLM_USE_NCCL_SYMM_MEM: bool = Field(
    default=False, description="Use NCCL symmetric memory for communication."
)
VLLM_DISABLE_PYNCCL: bool = Field(
    default=False,
    description="Disable pynccl (using torch.distributed instead).",
)
VLLM_ROCM_CUSTOM_PAGED_ATTN: bool = Field(
    default=True,
    description="(ROCm only) Custom paged attention kernel for MI3* cards.",
)
VLLM_ROCM_FP8_PADDING: bool = Field(
    default=True, description="(ROCm only) Pad the FP8 weights to 256 bytes for ROCm."
)
VLLM_ROCM_MOE_PADDING: bool = Field(
    default=True, description="(ROCm only) Pad the weights for the MOE kernel."
)
VLLM_ROCM_USE_SKINNY_GEMM: bool = Field(
    default=True, description="(ROCm only) Use skinny GEMM kernels."
)
VLLM_ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16: bool = Field(
    default=True,
    description=(
        "(ROCm only) Custom quick allreduce kernel for MI3* cards. "
        "Due to lack of bfloat16 asm instruction, if set to 1, "
        "input is converted to fp16."
    ),
)
VLLM_ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB: int | None = Field(
    default=None,
    description=(
        "(ROCm only) Maximum allowed number of data bytes (MB) for custom "
        "quick allreduce communication. Data exceeding this size will use "
        "either custom allreduce or RCCL."
    ),
)
VLLM_ROCM_QUICK_REDUCE_QUANTIZATION: Literal["FP", "INT8", "INT6", "INT4", "NONE"] = (
    Field(
        default="NONE",
        description=(
            "(ROCm only) Custom quick allreduce kernel quantization level "
            "for MI3* cards. Recommended for large models."
        ),
    )
)
VLLM_ENABLE_CUDAGRAPH_GC: bool = Field(
    default=False, description="Enable CUDA graph garbage collection."
)
VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = Field(
    default=128,
    description=(
        "Maximum number of requests to handle in a single asyncio task when processing "
        "per-token outputs in the V1 AsyncLLM interface."
    ),
)

# ── TPU and XLA Settings ────────────────────────────────────────────────────────

VLLM_TPU_BUCKET_PADDING_GAP: int = Field(
    default=0,
    description=(
        "Gap between padding buckets for the forward pass. "
        "If set to 8, forward pass will run with [16, 24, 32, ...]."
    ),
)
VLLM_TPU_MOST_MODEL_LEN: int | None = Field(
    default=None, description="Most common model length for TPU bucketing."
)
VLLM_TPU_USING_PATHWAYS: Annotated[
    bool, BeforeValidator(lambda v: v.strip().lower() in ("1", "true"))
] = Field(
    default_factory=lambda: "proxy" in os.getenv("JAX_PLATFORMS", "").lower(),
    description="Whether using Pathways for TPU.",
)

# ── Additional Distributed Settings ────────────────────────────────────────────

VLLM_ALLREDUCE_USE_SYMM_MEM: bool = Field(
    default=True, description="Whether to use PyTorch symmetric memory for allreduce."
)
VLLM_DP_RANK: int = Field(
    default=0, description="Rank of the process in the data parallel setting."
)
VLLM_DP_RANK_LOCAL: int = Field(
    default=-1,
    description=(
        "Local rank of the process in the data parallel setting. "
        "Defaults to VLLM_DP_RANK when not set."
    ),
)
VLLM_DP_SIZE: int = Field(
    default=1, description="World size of the data parallel setting."
)
VLLM_DP_MASTER_IP: str = Field(
    default="",
    description="IP address of the master node in the data parallel setting.",
)
VLLM_DP_MASTER_PORT: int = Field(
    default=0, description="Port of the master node in the data parallel setting."
)
VLLM_MOE_DP_CHUNK_SIZE: int = Field(
    default=256,
    description=(
        "Quantum of tokens that can be dispatched from a DP rank in MoE "
        "with Data-Parallel, Expert-Parallel and Batched All-to-All "
        "dispatch/combine kernels."
    ),
)
VLLM_ENABLE_MOE_DP_CHUNK: bool = Field(
    default=True, description="Enable MOE data parallel chunking."
)
VLLM_RANDOMIZE_DP_DUMMY_INPUTS: bool = Field(
    default=False,
    description="Randomize inputs during dummy runs when using Data Parallel.",
)
VLLM_RAY_PER_WORKER_GPUS: float = Field(
    default=1.0,
    description=(
        "Number of GPUs per worker in Ray. If set to a fraction, "
        "allows Ray to schedule multiple actors on a single GPU."
    ),
)
VLLM_RAY_BUNDLE_INDICES: str = Field(
    default="",
    description=(
        "Bundle indices for Ray. If set, controls precisely which indices are used for "
        "the Ray bundle for every worker. Format: comma-separated list of integers."
    ),
)
VLLM_RAY_DP_PACK_STRATEGY: Literal["strict", "fill", "span"] = Field(
    default="strict",
    description=(
        "Strategy to pack the data parallel ranks for Ray. "
        '"fill": for DP master node, allocate exactly '
        "data-parallel-size-local DP ranks. "
        '"strict": allocate exactly data-parallel-size-local DP ranks '
        "to each picked node. "
        '"span": allocate one DP rank over as many nodes '
        "as required for set world_size."
    ),
)
VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB: str | None = Field(
    default="{}", description="FlashInfer allreduce fusion thresholds in MB."
)

# ── Additional Kernel Settings ──────────────────────────────────────────────────

VLLM_USE_TRITON_AWQ: bool = Field(
    default=False, description="Use Triton implementations of AWQ."
)
VLLM_SKIP_P2P_CHECK: bool = Field(
    default=True,
    description=(
        "Skip P2P check. We assume drivers can report p2p status correctly. "
        "If the program hangs when using custom allreduce, set to False to verify p2p."
    ),
)
VLLM_DISABLED_KERNELS: Annotated[list[str] | None, BeforeValidator(parse_list)] = Field(
    default=None,
    description=(
        "List of quantization kernels to disable (comma-separated). "
        "Used for testing and performance comparisons. "
        "Currently only affects MPLinearKernel selection."
    ),
)
VLLM_USE_DEEP_GEMM: bool = Field(
    default=True, description="Allow use of DeepGemm kernels for fused moe ops."
)
VLLM_MOE_USE_DEEP_GEMM: bool = Field(
    default=True,
    description=(
        "Allow use of DeepGemm specifically for MoE fused ops (overrides only MoE)."
    ),
)
VLLM_USE_DEEP_GEMM_E8M0: bool = Field(
    default=True,
    description="Use E8M0 scaling when DeepGEMM is used on Blackwell GPUs.",
)
VLLM_USE_DEEP_GEMM_TMA_ALIGNED_SCALES: bool = Field(
    default=True, description="Create TMA-aligned scale tensor when DeepGEMM is used."
)
VLLM_DEEP_GEMM_WARMUP: Literal["skip", "full", "relax"] = Field(
    default="relax",
    description=(
        "DeepGemm warmup strategy. Warmup JITs required kernels before model execution "
        "but increases startup time. "
        '"skip": Skip warmup. '
        '"full": Warmup by running all possible gemm shapes. '
        '"relax": Select gemm shapes based on heuristics.'
    ),
)
VLLM_USE_FUSED_MOE_GROUPED_TOPK: bool = Field(
    default=True, description="Use fused grouped_topk for MoE expert selection."
)
VLLM_BLOCKSCALE_FP8_GEMM_FLASHINFER: bool = Field(
    default=False,
    description=(
        "Allow use of FlashInfer FP8 block-scale GEMM for linear layers. "
        "Uses TensorRT-LLM kernels and requires SM90+ (Hopper)."
    ),
)
VLLM_USE_FLASHINFER_MOE_FP16: bool = Field(
    default=False,
    description="Allow use of FlashInfer MoE kernels for FP16 fused moe ops.",
)
VLLM_USE_FLASHINFER_MOE_FP8: bool = Field(
    default=False,
    description="Allow use of FlashInfer MoE kernels for FP8 fused moe ops.",
)
VLLM_USE_FLASHINFER_MOE_FP4: bool = Field(
    default=False,
    description="Allow use of FlashInfer CUTLASS kernels for FP4 fused moe ops.",
)
VLLM_USE_FLASHINFER_MOE_INT4: bool = Field(
    default=False,
    description="Allow use of FlashInfer MxInt4 MoE kernels for fused moe ops.",
)
VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8: bool = Field(
    default=False,
    description="Use FlashInfer MXFP8 (activation) x MXFP4 (weight) MoE backend.",
)
VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS: bool = Field(
    default=False,
    description=(
        "Use FlashInfer CUTLASS backend for MXFP8 (activation) x MXFP4 (weight) MoE. "
        "Separate from TRTLLMGEN path controlled by "
        "VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8."
    ),
)
VLLM_USE_FLASHINFER_MOE_MXFP4_BF16: bool = Field(
    default=False,
    description="Use FlashInfer BF16 (activation) x MXFP4 (weight) MoE backend.",
)
VLLM_FLASHINFER_MOE_BACKEND: Literal["throughput", "latency", "masked_gemm"] = Field(
    default="latency", description="FlashInfer MoE backend type."
)
VLLM_FLASHINFER_WORKSPACE_BUFFER_SIZE: int = Field(
    default=394 * 1024 * 1024,
    description="Workspace buffer size for the FlashInfer backend (in bytes).",
)
VLLM_HAS_FLASHINFER_CUBIN: bool = Field(
    default=False, description="Whether FlashInfer CUBIN kernels are available."
)
VLLM_USE_FBGEMM: bool = Field(default=False, description="Use FBGEMM kernels.")
VLLM_MARLIN_USE_ATOMIC_ADD: bool = Field(
    default=False, description="Use atomicAdd reduce in GPTQ/AWQ Marlin kernel."
)
VLLM_MARLIN_INPUT_DTYPE: Literal["int8", "fp8"] | None = Field(
    default=None, description="Activation dtype for Marlin kernel."
)
VLLM_MXFP4_USE_MARLIN: bool | None = Field(
    default=None, description="Use Marlin kernel in MXFP4 quantization method."
)

# ── Additional Debugging Settings ───────────────────────────────────────────────

VLLM_PATTERN_MATCH_DEBUG: str | None = Field(
    default=None,
    description=(
        "Debug pattern matching inside custom passes. "
        "Should be set to the fx.Node name (e.g. 'getitem_34' or 'scaled_mm_3')."
    ),
)
VLLM_DEBUG_DUMP_PATH: str | None = Field(
    default=None,
    description=(
        "Dump fx graphs to the given directory. "
        "Overrides CompilationConfig.debug_dump_path if set."
    ),
)
VLLM_DEBUG_WORKSPACE: bool = Field(
    default=False, description="Logging of workspace resize operations."
)
VLLM_DEBUG_MFU_METRICS: bool = Field(
    default=False, description="Debug logging for --enable-mfu-metrics."
)
VLLM_GC_DEBUG: str = Field(default="", description="Garbage collection debug settings.")
VLLM_DISABLE_LOG_LOGO: bool = Field(
    default=False, description="Disable logging of vLLM logo at server startup."
)
VLLM_CUSTOM_SCOPES_FOR_PROFILING: bool = Field(
    default=False,
    description="Add optional custom scopes for profiling. Disable to avoid overheads.",
)
VLLM_NVTX_SCOPES_FOR_PROFILING: bool = Field(
    default=False, description="Add NVTX scopes for profiling."
)

# ── MOE and Quantization Settings ───────────────────────────────────────────────

VLLM_DEEPEPLL_NVFP4_DISPATCH: bool = Field(
    default=False,
    description=(
        "Use DeepEPLL kernels for NVFP4 quantization and dispatch method. "
        "Only supported on Blackwell GPUs."
    ),
)
VLLM_DEEPEP_BUFFER_SIZE_MB: int = Field(
    default=1024,
    description="Size in MB of the buffers (NVL and RDMA) used by DeepEP.",
)
VLLM_DEEPEP_HIGH_THROUGHPUT_FORCE_INTRA_NODE: bool = Field(
    default=False,
    description=(
        "Force DeepEP to use intranode kernel for inter-node communication "
        "in high throughput mode. "
        "Useful for higher prefill throughput on systems supporting "
        "multi-node nvlink (e.g. GB200)."
    ),
)
VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL: bool = Field(
    default=False,
    description=(
        "Allow DeepEP to use MNNVL (multi-node nvlink) for internode_ll kernel. "
        "Turn on for better latency on GB200-like systems."
    ),
)
VLLM_USE_NVFP4_CT_EMULATIONS: bool = Field(
    default=False, description="Use NVFP4 CT emulations."
)
VLLM_NVFP4_GEMM_BACKEND: str | None = Field(
    default=None, description="NVFP4 GEMM backend selection."
)
VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE: int = Field(
    default=163840,
    description=(
        "Maximum number of tokens per expert supported by the NVFP4 MoE "
        "CUTLASS Kernel. Used to create a buffer for the blockscale tensor "
        "of activations NVFP4 Quantization."
    ),
)
VLLM_MOE_ROUTING_SIMULATION_STRATEGY: str | None = Field(
    default=None, description="MOE routing simulation strategy."
)

# ── Additional Attention and Memory Settings ─────────────────────────────────────

VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE: bool = Field(
    default=True,
    description=(
        "Allow chunked local attention with Hybrid KV cache in Llama4 models. "
        "Currently disabled by default due to latency regression."
    ),
)
VLLM_KV_CACHE_LAYOUT: Literal["NHD", "HND"] | None = Field(
    default=None, description="KV cache memory layout. Options: NHD, HND."
)
VLLM_KV_EVENTS_USE_INT_BLOCK_HASHES: bool = Field(
    default=True, description="Use integer block hashes for KV cache events."
)
VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME: str = Field(
    default="VLLM_OBJECT_STORAGE_SHM_BUFFER",
    description="Shared memory buffer name for object storage.",
)
VLLM_MLA_DISABLE: bool = Field(
    default=False, description="Disable the MLA attention optimizations."
)

# ── Scale Constants ─────────────────────────────────────────────────────────────

Q_SCALE_CONSTANT: int = Field(
    default=200,
    description="Divisor for dynamic query scale factor calculation for FP8 KV Cache.",
)
K_SCALE_CONSTANT: int = Field(
    default=200,
    description="Divisor for dynamic key scale factor calculation for FP8 KV Cache.",
)
V_SCALE_CONSTANT: int = Field(
    default=100,
    description="Divisor for dynamic value scale factor calculation for FP8 KV Cache.",
)

# ── Additional Runtime Settings ─────────────────────────────────────────────────

VLLM_ALLOW_INSECURE_SERIALIZATION: bool = Field(
    default=False,
    description=(
        "Allow insecure serialization using pickle. "
        "Useful for environments where it is deemed safe to use the insecure method."
    ),
)
VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = Field(
    default=False,
    description="Allow loading or unloading LoRA adapters at runtime.",
)
VLLM_CI_USE_S3: bool = Field(
    default=False,
    description="Use S3 path for model loading in CI via RunAI Streamer.",
)
VLLM_LORA_RESOLVER_HF_REPO_LIST: str | None = Field(
    default=None,
    description="Comma-separated Hugging Face repo IDs for LoRA resolver.",
)
VLLM_LORA_DISABLE_PDL: bool = Field(default=False, description="Disable PDL for LoRA.")
VLLM_COMPUTE_NANS_IN_LOGITS: bool = Field(
    default=False,
    description=(
        "Enable checking whether generated logits contain NaNs, indicating "
        "corrupted output. Useful for debugging but may add compute overhead."
    ),
)
VLLM_DBO_COMM_SMS: int = Field(
    default=20,
    description=(
        "Number of SMs to allocate for communication kernels when running DBO. "
        "The rest will be allocated to compute."
    ),
)
VLLM_DISABLE_SHARED_EXPERTS_STREAM: bool = Field(
    default=False, description="Disable shared experts stream."
)
VLLM_SHARED_EXPERTS_STREAM_TOKEN_THRESHOLD: int = Field(
    default=256, description="Token threshold for shared experts stream."
)
VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING: bool = Field(
    default=True, description="Enable Inductor coordinate descent tuning."
)
VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE: bool = Field(
    default=True, description="Enable Inductor max autotune."
)
VLLM_ENABLE_RESPONSES_API_STORE: bool = Field(
    default=False, description="Enable responses API store."
)
VLLM_ENABLE_V1_MULTIPROCESSING: bool = Field(
    default=True, description="Enable multiprocessing in LLM for the V1 code path."
)
VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS: int = Field(
    default=300, description="Timeout in seconds for model execution."
)
VLLM_GPT_OSS_HARMONY_SYSTEM_INSTRUCTIONS: bool = Field(
    default=False, description="Enable GPT OSS Harmony system instructions."
)
VLLM_LOG_BATCHSIZE_INTERVAL: float = Field(
    default=-1.0, description="Interval for logging batch size statistics."
)
VLLM_LOG_MODEL_INSPECTION: bool = Field(
    default=False, description="Enable logging of model inspection details."
)
VLLM_LOOPBACK_IP: str = Field(default="", description="Loopback IP address.")
VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT: int = Field(
    default=480, description="Timeout for aborting requests in Mooncake."
)
VLLM_MOONCAKE_BOOTSTRAP_PORT: int = Field(
    default=8998,
    description="Port used for Mooncake handshake between remote agents.",
)
VLLM_MORIIO_CONNECTOR_READ_MODE: bool = Field(
    default=False, description="Moriio connector read mode."
)
VLLM_MORIIO_NUM_WORKERS: int = Field(default=1, description="Number of Moriio workers.")
VLLM_MORIIO_POST_BATCH_SIZE: int = Field(
    default=-1, description="Moriio POST batch size."
)
VLLM_MORIIO_QP_PER_TRANSFER: int = Field(
    default=1, description="Moriio queue pairs per transfer."
)
VLLM_MQ_MAX_CHUNK_BYTES_MB: int = Field(
    default=16, description="Maximum chunk size in MB for message queue."
)
VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = Field(
    default=256,
    description=(
        "Threshold for msgspec to use 'zero copy' for "
        "serialization/deserialization of tensors. "
        "Tensors below this limit are encoded into the msgpack buffer, tensors above "
        "are sent separately."
    ),
)
VLLM_NIXL_ABORT_REQUEST_TIMEOUT: int = Field(
    default=480, description="Timeout for aborting requests in NIXL."
)
VLLM_NIXL_SIDE_CHANNEL_HOST: str = Field(
    default="localhost",
    description="IP address used for NIXL handshake between remote agents.",
)
VLLM_NIXL_SIDE_CHANNEL_PORT: int = Field(
    default=5600,
    description="Port used for NIXL handshake between remote agents.",
)
VLLM_SERVER_DEV_MODE: bool = Field(
    default=False,
    description=(
        "Run vLLM in development mode, enabling additional endpoints for "
        "developing and debugging (e.g. `/reset_prefix_cache`)."
    ),
)
VLLM_SLEEP_WHEN_IDLE: bool = Field(
    default=False, description="Put workers to sleep when idle."
)
VLLM_TEST_FORCE_LOAD_FORMAT: str | None = Field(
    default=None, description="Force a specific load format for testing."
)
VLLM_TEST_FORCE_FP8_MARLIN: bool = Field(
    default=False,
    description=(
        "Force FP8 Marlin to be used for FP8 quantization "
        "regardless of hardware support for FP8 compute."
    ),
)
VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY: bool = Field(
    default=False,
    description="Automatically retry on JSON parsing errors in tool calls.",
)
VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS: int = Field(
    default=1, description="Timeout in seconds for tool parse regex operations."
)
VLLM_TUNED_CONFIG_FOLDER: str | None = Field(
    default=None, description="Folder containing tuned configuration files."
)
VLLM_USE_EXPERIMENTAL_PARSER_CONTEXT: bool = Field(
    default=False, description="Use experimental parser context."
)
VLLM_USE_V2_MODEL_RUNNER: bool = Field(
    default=False, description="Enable V2 model runner."
)
VLLM_V1_USE_OUTLINES_CACHE: bool = Field(
    default=False,
    description=(
        "Use the outlines cache for V1. "
        "This cache is unbounded and on disk, so it's not safe "
        "in environments with potentially malicious users."
    ),
)
VLLM_XGRAMMAR_CACHE_MB: int = Field(
    default=512,
    description=(
        "Cache size in MB used by the xgrammar compiler. "
        "Default of 512 MB should be enough for roughly 1000 JSON schemas."
    ),
)

# Collect all field definitions for use in __init__.py.
# This runs at import time after all FieldInfo objects are in module globals.
_fields: dict[str, FieldInfo] = {
    k: v
    for k, v in globals().items()
    if not k.startswith("_") and k.isupper() and isinstance(v, FieldInfo)
}
