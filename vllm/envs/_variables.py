# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

"""Environment variable declarations for vLLM.

This module contains all environment variable declarations with type annotations
and default values. These are the single source of truth for environment variables.

WARNING: Do NOT import from this module directly (except under TYPE_CHECKING).
         Always use `import vllm.envs as envs` or `from vllm import envs`.
"""

import os
import tempfile
from pathlib import Path
from typing import TYPE_CHECKING, Literal

# Only import utilities at runtime, not during type checking
# This avoids circular imports since __init__.py imports this module
if not TYPE_CHECKING:
    from vllm.envs.utils import (
        env_default_factory,
        env_factory,
        env_set_with_choices,
        env_with_choices,
        parse_list,
        parse_path,
    )
else:
    # Provide type stubs for type checking
    def env_default_factory(x): ...
    def env_factory(x, y): ...
    def env_with_choices(a, b, c, **kwargs): ...
    def env_set_with_choices(a, b, c, **kwargs): ...
    def parse_path(x): ...
    def parse_list(x): ...

# ================== Network and Communication ==================

VLLM_HOST_IP: str = ""
"""
Used in distributed environment to determine the IP address of the current node,
when the node has multiple network interfaces.
If you are using multi-node inference, you should set this differently on each node.
"""


def _get_vllm_port(env_port: str) -> int:
    """Parse VLLM_PORT environment variable with validation."""
    try:
        return int(env_port)
    except ValueError as err:
        from urllib.parse import urlparse

        parsed = urlparse(env_port)
        if parsed.scheme:
            raise ValueError(
                f"VLLM_PORT '{env_port}' appears to be a URI. "
                "This may be caused by a Kubernetes service discovery issue, "
                "check the warning in: https://docs.vllm.ai/en/stable/serving/env_vars.html"
            ) from None
        raise ValueError(f"VLLM_PORT '{env_port}' must be a valid integer") from err


VLLM_PORT: int | None = env_factory(None, _get_vllm_port)
"""
Used in distributed environment to manually set the communication port.

Note: if VLLM_PORT is set, and some code asks for multiple ports, the VLLM_PORT
will be used as the first port, and the rest will be generated by incrementing
the VLLM_PORT value.
"""

VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()
"""Base path for RPC socket files. Defaults to system temp directory."""

VLLM_RPC_TIMEOUT: int = 10000
"""RPC timeout in milliseconds."""

VLLM_HTTP_TIMEOUT_KEEP_ALIVE: int = 5
"""HTTP keep-alive timeout in seconds."""

# ================== Feature Flags ==================

VLLM_USE_MODELSCOPE: bool = False
"""If set, vllm will use ModelScope instead of Hugging Face Hub."""

VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60
"""Interval in seconds for ring buffer warnings."""

VLLM_NCCL_SO_PATH: str | None = None
"""Path to NCCL shared library (.so file)."""

LD_LIBRARY_PATH: str | None = None
"""Standard LD_LIBRARY_PATH environment variable."""

# ================== Device and Performance ==================

VLLM_ROCM_SLEEP_MEM_CHUNK_SIZE: int = 256
"""ROCm memory chunk size for sleep operations (in MB)."""

VLLM_V1_USE_PREFILL_DECODE_ATTENTION: bool = False
"""Use separate attention implementations for prefill and decode phases in V1."""

VLLM_FLASH_ATTN_VERSION: int | None = None
"""FlashAttention version to use. If None, auto-detected."""

LOCAL_RANK: int = 0
"""Local rank of the process in the distributed setting, used to determine the GPU device id."""

CUDA_VISIBLE_DEVICES: str | None = None
"""Used to control the visible devices in the distributed setting."""

# ================== Timeouts and Intervals ==================

VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60
"""Timeout for each iteration in the engine (in seconds)."""

VLLM_ENGINE_READY_TIMEOUT_S: int = 600
"""
Timeout in seconds for waiting for engine cores to become ready during startup.
Default is 600 seconds (10 minutes).
"""

# ================== Authentication and Security ==================

VLLM_API_KEY: str | None = None
"""API key for vLLM API server."""

VLLM_DEBUG_LOG_API_SERVER_RESPONSE: bool = False
"""Whether to log responses from API Server for debugging."""

S3_ACCESS_KEY_ID: str | None = None
"""S3 access key ID, used for tensorizer to load model from S3."""

S3_SECRET_ACCESS_KEY: str | None = None
"""S3 secret access key, used for tensorizer to load model from S3."""

S3_ENDPOINT_URL: str | None = None
"""S3 endpoint URL, used for tensorizer to load model from S3."""

# ================== Paths and Directories ==================

VLLM_MODEL_REDIRECT_PATH: Path | None = None
"""Path to redirect model loading. Automatically expands ~ and environment variables."""


def _get_default_cache_root() -> str:
    """Get the default cache root directory."""
    cache_home = os.getenv("XDG_CACHE_HOME", os.path.expanduser("~/.cache"))
    return os.path.join(cache_home, "vllm")


VLLM_CACHE_ROOT: str = env_default_factory(_get_default_cache_root)
"""
Root directory for vLLM cache files.
Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set.
"""

VLLM_CONFIG_ROOT: str = env_default_factory(
    lambda: os.path.expanduser("~/.config/vllm")
)
"""Root directory for vLLM configuration files. Defaults to `~/.config/vllm`."""

# ================== Usage Statistics ==================

VLLM_USAGE_STATS_SERVER: str = "https://stats.vllm.ai"
"""URL of the usage stats collection server."""

VLLM_NO_USAGE_STATS: bool = False
"""If set to 1, vllm will not send usage stats."""

VLLM_DISABLE_FLASHINFER_PREFILL: bool = False
"""Disable FlashInfer for prefill operations."""


def _get_dnt_value() -> bool:
    """Check VLLM_DO_NOT_TRACK first, then fall back to DO_NOT_TRACK."""
    # VLLM_DO_NOT_TRACK takes precedence
    vllm_dnt = os.getenv("VLLM_DO_NOT_TRACK")
    if vllm_dnt is not None:
        return vllm_dnt == "1"
    # Fallback to standard DO_NOT_TRACK
    return os.getenv("DO_NOT_TRACK", "0") == "1"


VLLM_DO_NOT_TRACK: bool = env_default_factory(_get_dnt_value)
"""
Honors DO_NOT_TRACK and VLLM_DO_NOT_TRACK environment variables.
If set to 1, disables usage tracking.
VLLM_DO_NOT_TRACK takes precedence over DO_NOT_TRACK.
"""

VLLM_USAGE_SOURCE: str = "production"
"""Source identifier for usage statistics."""

# ================== Logging Configuration ==================

VLLM_CONFIGURE_LOGGING: bool = True
"""
If set to 0, vllm will not configure logging.
If set to 1, vllm will configure logging using the default configuration
or the configuration file specified by VLLM_LOGGING_CONFIG_PATH.
"""

VLLM_LOGGING_LEVEL: str = env_factory("INFO", lambda x: x.upper())
"""Logging level for vLLM. Default is "INFO"."""

VLLM_LOGGING_PREFIX: str = ""
"""If set, VLLM_LOGGING_PREFIX will be prepended to all log messages."""

VLLM_LOGGING_STREAM: str = "ext://sys.stdout"
"""Stream for logging output. Default is sys.stdout."""

VLLM_LOGGING_CONFIG_PATH: str | None = None
"""Path to custom logging configuration file."""

VLLM_LOGGING_COLOR: str = "auto"
"""
Controls colored logging output.
Options: "auto" (default, colors when terminal), "1" (always use colors), "0" (never use colors).
"""

NO_COLOR: bool = env_factory(False, lambda x: True)
"""Standard unix flag for disabling ANSI color codes."""

VLLM_LOG_STATS_INTERVAL: float = env_factory(
    10.0, lambda x: val if (val := float(x)) > 0.0 else 10.0
)
"""
Interval in seconds for logging stats.
If set to a non-positive value, defaults to 10.0 seconds.
"""

# ================== Debugging and Tracing ==================

VLLM_TRACE_FUNCTION: int = 0
"""
If set to 1, vllm will trace function calls.
Useful for debugging.
"""

# ================== Attention Backend ==================


def _get_attention_backend_choices() -> list[str]:
    """Get available attention backend choices dynamically."""
    try:
        from vllm.v1.attention.backends.registry import AttentionBackendEnum

        return list(AttentionBackendEnum.__members__.keys())
    except ImportError:
        # Fallback list if the module is not available
        return [
            "TORCH_SDPA",
            "FLASH_ATTN",
            "FLASHINFER",
            "FLASHMLA",
            "FLASH_ATTN_MLA",
            "FLASHINFER_MLA",
            "CUTLASS_MLA",
        ]


VLLM_ATTENTION_BACKEND: str | None = env_with_choices(
    "VLLM_ATTENTION_BACKEND", None, _get_attention_backend_choices
)
"""
Backend for attention computation.
Example options: "TORCH_SDPA", "FLASH_ATTN", "FLASHINFER", etc.
All possible options are loaded dynamically from AttentionBackendEnum.
"""


def _parse_optional_bool_int(value: str) -> bool | None:
    """Parse optional boolean from int string (used for VLLM_USE_FLASHINFER_SAMPLER)."""
    return bool(int(value))


# ================== CPU Backend ==================

VLLM_CPU_KVCACHE_SPACE: int = 0
"""(CPU backend only) CPU key-value cache space in GB."""

VLLM_CPU_OMP_THREADS_BIND: str = ""
"""(CPU backend only) OpenMP thread binding configuration."""

VLLM_CPU_NUM_OF_RESERVED_CPU: int | None = None
"""(CPU backend only) Number of reserved CPU cores."""

VLLM_CPU_SGL_KERNEL: bool = False
"""(CPU backend only) Use SGLang kernel."""

# ================== XLA Backend ==================


def _get_xla_cache_path() -> str:
    """Get XLA cache path, dependent on VLLM_CACHE_ROOT."""
    cache_root = os.getenv("VLLM_CACHE_ROOT")
    if cache_root:
        return os.path.join(cache_root, "xla_cache")
    return os.path.join(_get_default_cache_root(), "xla_cache")


VLLM_XLA_CACHE_PATH: str = env_default_factory(_get_xla_cache_path)
"""Path for XLA compilation cache. Defaults to VLLM_CACHE_ROOT/xla_cache."""

VLLM_XLA_CHECK_RECOMPILATION: bool = False
"""Check for XLA recompilations and log warnings."""

VLLM_XLA_USE_SPMD: bool = False
"""Use XLA SPMD (Single Program Multiple Data) partitioning."""

# ================== Kernel Config ==================

VLLM_USE_FLASHINFER_SAMPLER: bool | None = env_factory(
    None, _parse_optional_bool_int
)
"""If set, vllm will use flashinfer sampler."""

VLLM_FUSED_MOE_CHUNK_SIZE: int = 16 * 1024
"""Chunk size for fused MOE operations."""

VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING: bool = True
"""Enable chunking for fused MOE activations."""

# ================== Distributed Config ==================

VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE: Literal["auto", "nccl", "shm"] = "auto"
"""Channel type for Ray compiled DAG. Options: auto, nccl, shm."""

VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = False
"""Use overlapped communication in Ray compiled DAG."""

VLLM_USE_RAY_WRAPPED_PP_COMM: bool = True
"""Use Ray-wrapped pipeline parallelism communication."""

VLLM_PP_LAYER_PARTITION: str | None = None
"""Pipeline parallelism layer partition strategy."""

VLLM_WORKER_MULTIPROC_METHOD: Literal["fork", "spawn"] = "fork"
"""Multiprocessing method for workers. Options: fork, spawn."""

# ================== Media and Assets ==================


def _get_assets_cache_path() -> str:
    """Get assets cache path, dependent on VLLM_CACHE_ROOT."""
    cache_root = os.getenv("VLLM_CACHE_ROOT")
    if cache_root:
        return os.path.join(cache_root, "assets")
    return os.path.join(_get_default_cache_root(), "assets")


VLLM_ASSETS_CACHE: str = env_default_factory(_get_assets_cache_path)
"""Cache directory for downloaded assets (images, videos, audio)."""

VLLM_ASSETS_CACHE_MODEL_CLEAN: bool = False
"""Clean model-specific assets from cache."""

VLLM_IMAGE_FETCH_TIMEOUT: int = 5
"""Timeout in seconds for fetching images."""

VLLM_VIDEO_FETCH_TIMEOUT: int = 30
"""Timeout in seconds for fetching videos."""

VLLM_AUDIO_FETCH_TIMEOUT: int = 10
"""Timeout in seconds for fetching audio files."""

VLLM_MEDIA_URL_ALLOW_REDIRECTS: bool = True
"""Allow URL redirects when fetching media."""

VLLM_MEDIA_LOADING_THREAD_COUNT: int = 8
"""Number of threads for loading media files."""

VLLM_MAX_AUDIO_CLIP_FILESIZE_MB: int = 25
"""Maximum audio clip file size in MB."""

VLLM_VIDEO_LOADER_BACKEND: str = "opencv"
"""Backend for video loading. Default is opencv."""

VLLM_MEDIA_CONNECTOR: str = "http"
"""Connector type for media loading. Default is http."""

# ================== Installation Time Env Vars ==================

VLLM_TARGET_DEVICE: str = env_factory("cuda", lambda x: x.lower())
"""
Target device of vLLM.
Options: cuda (default), rocm, cpu.
"""

VLLM_MAIN_CUDA_VERSION: str = env_factory(
    "12.9", lambda x: x.lower() if x else "12.9"
)
"""
Main CUDA version of vLLM. This follows PyTorch but can be overridden.
"""

VLLM_FLOAT32_MATMUL_PRECISION: Literal["highest", "high", "medium"] = "highest"
"""
Controls PyTorch float32 matmul precision mode within vLLM workers.
Valid options mirror torch.set_float32_matmul_precision.
Automatically validated (case-insensitive).
"""

MAX_JOBS: str | None = None
"""
Maximum number of compilation jobs to run in parallel.
By default this is the number of CPUs.
"""

NVCC_THREADS: str | None = None
"""
Number of threads to use for nvcc.
By default this is 1.
If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.
"""


def _get_use_precompiled_default() -> bool:
    """Check for VLLM_USE_PRECOMPILED or VLLM_PRECOMPILED_WHEEL_LOCATION."""
    if os.getenv("VLLM_USE_PRECOMPILED", "0").strip().lower() in ("1", "true"):
        return True
    if os.getenv("VLLM_PRECOMPILED_WHEEL_LOCATION"):
        return True
    return False


VLLM_USE_PRECOMPILED: bool = env_default_factory(_get_use_precompiled_default)
"""
If set, vllm will use precompiled binaries (*.so).
Also enabled if VLLM_PRECOMPILED_WHEEL_LOCATION is set.
"""

VLLM_SKIP_PRECOMPILED_VERSION_SUFFIX: bool = False
"""If set, skip adding +precompiled suffix to version string."""

VLLM_DOCKER_BUILD_CONTEXT: bool = False
"""
Used to mark that setup.py is running in a Docker build context,
in order to force the use of precompiled binaries.
"""

VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False
"""Keep the server alive even if the engine dies."""

CMAKE_BUILD_TYPE: Literal["Debug", "Release", "RelWithDebInfo"] | None = None
"""CMake build type. If not set, defaults to "Debug" or "RelWithDebInfo"."""

VERBOSE: bool = False
"""Enable verbose output during installation."""

# ================== Model Configuration ==================

VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False
"""Allow model max length to exceed default limits."""

# ================== Plugins ==================


VLLM_PLUGINS: list[str] | None = env_factory(None, parse_list)
"""List of vLLM plugins to load (comma-separated)."""

# ================== MCP (Model Context Protocol) Configuration ==================

VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS: set[str] = env_default_factory(
    env_set_with_choices(
        "VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS",
        default=[],
        choices=["container", "code_interpreter", "web_search_preview"],
    )
)
"""
Valid server labels for MCP tools.
Example: VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS=container,code_interpreter
If the server_label of your MCP tool is not in this list, it will be completely ignored.
"""

# ================== LoRA Configuration ==================

VLLM_LORA_RESOLVER_CACHE_DIR: str | None = None
"""Cache directory for LoRA resolver."""

# ================== Deprecated Profiling Env Vars ==================
# Deprecated env variables for profiling, kept for backward compatibility
# See also vllm/config/profiler.py and `--profiler-config` argument

VLLM_TORCH_CUDA_PROFILE: str | None = None
"""(Deprecated) Torch CUDA profiling configuration."""

VLLM_TORCH_PROFILER_DIR: str | None = None
"""(Deprecated) Directory for torch profiler output."""

VLLM_TORCH_PROFILER_RECORD_SHAPES: str | None = None
"""(Deprecated) Record tensor shapes in profiler."""

VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY: str | None = None
"""(Deprecated) Profile memory usage."""

VLLM_TORCH_PROFILER_DISABLE_ASYNC_LLM: str | None = None
"""(Deprecated) Disable async LLM during profiling."""

VLLM_TORCH_PROFILER_WITH_STACK: str | None = None
"""(Deprecated) Include stack traces in profiler."""

VLLM_TORCH_PROFILER_WITH_FLOPS: str | None = None
"""(Deprecated) Profile FLOPs."""

# ... More variables would be added here to complete the migration ...

# ================== Internal: Default Values Store ==================
# This dict is used internally by the __getattr__ mechanism to access defaults
_defaults = {k: v for k, v in locals().items() if not k.startswith("_") and k.isupper()}
