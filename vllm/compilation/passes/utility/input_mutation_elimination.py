# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

import torch.fx

from vllm.logger import init_logger

from ..fx_utils import is_func
from ..vllm_inductor_pass import VllmInductorPass

logger = init_logger(__name__)

class InputMutationEliminationPass(VllmInductorPass):
    """
    This Pass is used to eliminate unnecessary
    in-place mutations (copy_) on graph input arguments generated by AOT Autograd.
    
    Scenario:
    When PyTorch compilation detects that the forward function
    modifies input parameters (e.g., inputs_embeds),
    it inserts an `aten.copy_` node at the end of the graph to
    write the new value back to the input tensor.
    In Sequence Parallel (SP) mode, the computation flow is sharded, 
    while input parameters are full-length (Replicated/Full). 
    This leads to Shape Mismatch errors in copy_ operations.
    
    Fix Strategy:
    If we find `aten.copy_(dst, src)` where `dst` is a Graph Input (placeholder),
    we assume this is generated by the compiler to maintain side effects.
    In inference scenarios, we do not need to preserve modifications to inputs,
    so we can directly remove this node and replace all uses of the `copy_` output with `src`.

    Example:
    auto_functionalized_321 = auto_functionalized(fused_add_rms_norm.default, ...)
    getitem_1922: "bf16[(s59//4), 5120]" = auto_functionalized_321[1]
    getitem_1923: "bf16[(s59//4), 5120]" = auto_functionalized_321[2]
    all_gather_default: "bf16[4*((s59//4)), 5120]" = all_gather.default(getitem_1922, ...)
    copy_: "bf16[s59, 5120]" = torch.ops.aten.copy_.default(arg1_1, getitem_1923)
    return (all_gather_default,)

    Can be replaced with:
    auto_functionalized_321 = auto_functionalized(fused_add_rms_norm.default, ...)
    getitem_1922: "bf16[(s59//4), 5120]" = auto_functionalized_321[1]
    getitem_1923: "bf16[(s59//4), 5120]" = auto_functionalized_321[2]
    all_gather_default: "bf16[4*((s59//4)), 5120]" = all_gather.default(getitem_1922, ...)
    return (all_gather_default,)
    """

    @VllmInductorPass.time_and_log
    def __call__(self, graph: torch.fx.Graph):
        count = 0

        for node in graph.nodes:
            # 1. Find aten.copy_ operations
            if is_func(node, torch.ops.aten.copy_.default):
                dst = node.args[0]
                src = node.args[1]

                # 2. Check if dst is a graph input parameter (op code is 'placeholder')
                if isinstance(dst, torch.fx.Node) and dst.op == "placeholder":
                    
                    # Get shapes from metadata
                    dst_val = dst.meta.get("val")
                    src_val = src.meta.get("val")
                    
                    dst_shape = dst_val.shape if dst_val is not None else None
                    src_shape = src_val.shape if src_val is not None else None
                    
                    # 3. Only remove if shapes are indeed different
                    if dst_shape is not None and src_shape is not None:
                        if dst_shape != src_shape:
                            logger.debug(f"Found input mutation copy_ (Shape Mismatch): {dst.name} ({dst_shape}) <- {src.name} ({src_shape})")
                            node.replace_all_uses_with(src)
                            graph.erase_node(node)
                            count += 1
                        else:
                            # Shapes look the same at the metadata level, skipping elimination.
                            logger.debug(f"Skipping copy_ elimination for {dst.name} <- {src.name} because metadata shapes match: {dst_shape}")
                    else:
                        logger.debug(f"Metadata missing for {dst.name} or {src.name}, Skipping...")

        if count > 0:
            logger.debug("Removed %d input mutation copy_ nodes (fixed SP shape mismatch).", count)
