# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project

import torch.fx

from vllm.logger import init_logger

from ..fx_utils import is_func
from ..vllm_inductor_pass import VllmInductorPass

logger = init_logger(__name__)


class InputMutationEliminationPass(VllmInductorPass):
    """
    This Pass is used to eliminate unnecessary
    in-place mutations (copy_) on graph input arguments generated by AOT Autograd.

    When PyTorch compilation detects that the forward function
    modifies input parameters (e.g., inputs_embeds),
    it inserts an `aten.copy_` node at the end of the graph to
    write the new value back to the input tensor.

    In Sequence Parallel (SP) mode, the computation flow is sharded,
    while input parameters are full-length.
    This can lead to Shape Mismatch errors in these compiler-inserted copy_
    operations when the sharded tensor being written back does not match the
    original full-length input shape.

    If we find `aten.copy_(dst, src)` where `dst` is a Graph
    Input (placeholder) and the recorded metadata shapes for `dst` and `src`
    differ, we assume this copy_ was inserted by the compiler solely to
    preserve input mutation side effects. For LLM inference forward in SP
    mode, such shape-mismatched write-backs are not required to preserve
    correctness and would fail due to the mismatch, so we remove the
    corresponding `copy_` node and replace all uses of its output with `src`.
    This pass does not eliminate all input mutations, only these specific
    shape-mismatch copy_ operations on graph inputs in SP mode.

    Example:
    auto_functionalized_321 = auto_functionalized(fused_add_rms_norm.default, ...)
    getitem_1922: "bf16[(s59//4), 5120]" = auto_functionalized_321[1]
    getitem_1923: "bf16[(s59//4), 5120]" = auto_functionalized_321[2]
    all_gather_default: "bf16[4*((s59//4)), 5120]" = all_gather(getitem_1922, ...)
    copy_: "bf16[s59, 5120]" = torch.ops.aten.copy_.default(arg1_1, getitem_1923)
    return (all_gather_default,)

    Can be replaced with:
    auto_functionalized_321 = auto_functionalized(fused_add_rms_norm.default, ...)
    getitem_1922: "bf16[(s59//4), 5120]" = auto_functionalized_321[1]
    getitem_1923: "bf16[(s59//4), 5120]" = auto_functionalized_321[2]
    all_gather_default: "bf16[4*((s59//4)), 5120]" = all_gather(getitem_1922, ...)
    return (all_gather_default,)
    """

    @VllmInductorPass.time_and_log
    def __call__(self, graph: torch.fx.Graph) -> None:
        count = 0

        for node in graph.nodes:
            # 1. Find aten.copy_ operations
            if is_func(node, torch.ops.aten.copy_.default):
                dst = node.args[0]
                src = node.args[1]

                # 2. Check if dst is a graph input parameter (op code is 'placeholder')
                if isinstance(dst, torch.fx.Node) and dst.op == "placeholder":
                    # Get shapes from metadata
                    dst_val = dst.meta.get("val")
                    src_val = src.meta.get("val")

                    dst_shape = dst_val.shape if dst_val is not None else None
                    src_shape = src_val.shape if src_val is not None else None

                    # 3. Only remove if shapes are indeed different
                    if dst_shape is not None and src_shape is not None:
                        if dst_shape != src_shape:
                            logger.debug(
                                "Found input mutation copy_ "
                                "(Shape Mismatch): %s (%s) <- %s (%s)",
                                dst.name,
                                dst_shape,
                                src.name,
                                src_shape,
                            )
                            node.replace_all_uses_with(src)
                            graph.erase_node(node)
                            count += 1
                        else:
                            # Shapes look the same at the metadata level,
                            # skipping elimination.
                            logger.debug(
                                "Skipping copy_ elimination for %s <- %s "
                                "because metadata shapes match: %s",
                                dst.name,
                                src.name,
                                dst_shape,
                            )
                    else:
                        logger.debug(
                            "Metadata missing for %s or %s, Skipping...",
                            dst.name,
                            src.name,
                        )

        if count > 0:
            logger.debug(
                "Removed %d input mutation copy_ nodes (fixed SP shape mismatch).",
                count,
            )
