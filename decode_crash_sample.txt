0208 08:14:43.504373 12556 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:14:53.504501 12556 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
0208 08:14:59.498706  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1119: vLLM route_chat called, use_discovery=false request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.498742  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1151: Using direct URL mode with VllmPDRouter's own routing logic request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.499940  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1175: Found 4 prefill workers, 1 decode workers from worker_registry request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500511  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:281: Updated consistent hash ring with 4 workers and 640 virtual nodes request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500539  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:593: CONSISTENT_HASH_DEBUG: Extracted hash key: request_hash:9554506712c316b0 request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500543  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:598: CONSISTENT_HASH_DEBUG: Hash key 'request_hash:9554506712c316b0' mapped to worker: https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500548  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:647: Consistent hash routing: key='request_hash:9554506712c316b0' -> worker='https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081' (index=0) request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500615  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:281: Updated consistent hash ring with 1 workers and 160 virtual nodes request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500631  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:593: CONSISTENT_HASH_DEBUG: Extracted hash key: request_hash:9554506712c316b0 request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500634  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:598: CONSISTENT_HASH_DEBUG: Hash key 'request_hash:9554506712c316b0' mapped to worker: https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081 request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500637  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:647: Consistent hash routing: key='request_hash:9554506712c316b0' -> worker='https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081' (index=0) request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:14:59.500642  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1227: Chat: Selected prefill=https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 [policy:consistent_hash], decode=https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081 [policy:consistent_hash] request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
[0;36m(ApiServer_0 pid=306)[0;0m WARNING 02-08 08:14:59 [protocol.py:53] The following fields were present in the request but ignored: {'return_hidden_states', 'stream_reasoning', 'separate_reasoning', 'no_stop_trim'}
[0;36m(EngineCore_DP0 pid=304)[0;0m INFO 02-08 08:14:59 [nixl_connector.py:1099] NIXL compatibility check passed (hash: 98f64d8e1600a363b1c7977f640ed099835460dca70f5fbce31070bc302474f4)
I0208 08:14:59.675677    3546 nixl_agent.cpp:1616] Loading remote metadata for agent: ae661371-f104-496a-b592-88816068a76c
[1770567299.676472] [twshared4216:304  :0]          select.c:652  UCX  ERROR   no active messages transport to <no debug data>: self/memory - no peer failure handler, tcp/eth0 - no route to 2803:6086:878:2fca:4a5f:12c4:400:0%929505860:65535, sysv/memory - no peer failure handler, posix/memory - no peer failure handler, cuda_copy/cuda - no am bcopy, cuda_ipc/cuda - no am bcopy,
W0208 08:14:59.676482    3546 ucx_utils.cpp:70] Unexpected UCX error: Destination is unreachable
E0208 08:14:59.676953    3546 nixl_agent.cpp:1639] loadRemoteMD: error loading connection info for backend 'UCX' with status NIXL_ERR_BACKEND
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258] NIXL transfer failure: handshake_setup_failed | Context: {'failure_type': 'handshake_setup_failed', 'request_id': None, 'engine_id': '9fee93dd-d148-4fee-9f9a-f9e2ea76b627_dp0', 'remote_engine_id': '6f24ac30-204b-4fee-b357-35b75fdda226_dp1'}
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1256, in done_callback
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     self._remote_agents[eid] = f.result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]                                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     raise self._exception
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     result = self.fn(*self.args, **self.kwargs)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1126, in _nixl_handshake
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     remote_agent_name = self.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]                         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1595, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     remote_agent_name = self.nixl_wrapper.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/nixl_cu13/_api.py", line 801, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]     agent_name = self.agent.loadRemoteMD(metadata)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1258] nixl_cu13._bindings.nixlBackendError: NIXL_ERR_BACKEND
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275] NIXL transfer failure: handshake_failed | Context: {'failure_type': 'handshake_failed', 'request_id': 'chatcmpl-___prefill_addr_[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_3154f42021d84cd382f1dba85e79f5ab-a7713eec', 'engine_id': '9fee93dd-d148-4fee-9f9a-f9e2ea76b627_dp0', 'remote_engine_id': '6f24ac30-204b-4fee-b357-35b75fdda226_dp1', 'remote_request_id': 'chatcmpl-___prefill_addr_[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_3154f42021d84cd382f1dba85e79f5ab-b6833ac9', 'remote_host': '2803:6086:0878:2fca:4a5f:12c4:0400:0000', 'remote_port': 5603, 'num_local_blocks': 12, 'num_remote_blocks': 12, 'local_block_ids_sample': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1271, in request_ready
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     f.result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     raise self._exception
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1256, in done_callback
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     self._remote_agents[eid] = f.result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]                                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     raise self._exception
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     result = self.fn(*self.args, **self.kwargs)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1126, in _nixl_handshake
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     remote_agent_name = self.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]                         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1595, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     remote_agent_name = self.nixl_wrapper.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/nixl_cu13/_api.py", line 801, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]     agent_name = self.agent.loadRemoteMD(metadata)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:14:59 [nixl_connector.py:1275] nixl_cu13._bindings.nixlBackendError: NIXL_ERR_BACKEND
[0;36m(EngineCore_DP0 pid=304)[0;0m WARNING 02-08 08:14:59 [scheduler.py:2116] Recovered from KV load failure: 1 request(s) rescheduled (768 tokens affected).
V0208 08:15:03.504666 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:38546 - "POST /v1/chat/completions HTTP/1.1" 200 OK
0208 08:15:07.063991  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:867: ðŸ“¥ Decode response status: 200 OK request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:15:07.064029  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:868: ðŸ“¥ Decode response headers: {"date": "Sun, 08 Feb 2026 16:14:59 GMT", "server": "uvicorn", "content-length": "3753", "content-type": "application/json"} request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:15:07.064086  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1247: Two-stage processing completed successfully request_id: "0af00000000000b9" method: "InferencePlatformSp_LlmPredictorGpu.predict"
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:38550 - "GET /metrics HTTP/1.1" 200 OK
0208 08:15:07.353850  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:15:07.354096  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=465, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=297, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=281, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.iteration_tokens_total_sum=1185, llm_service.vllm.request_params_max_tokens.p50.60=3500, llm_service.vllm.generation_tokens_total.sum.60=465, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=797, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.http_request_duration_highr_milliseconds.p90.60=7250, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=975, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=150, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=750, llm_service.vllm.request_prefill_kv_computed_tokens_count=1, llm_service.vllm.time_to_first_token_milliseconds.p99.60=747, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=975, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_prompt_tokens_avg=360, llm_service.vllm.request_prompt_tokens_count.engine_0=1, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=33, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p50.60=6250, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.request_params_max_tokens.p99.60=4970, llm_service.vllm.request_decode_time_milliseconds.p99.60=9950, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=9500, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.http_request_duration_highr_milliseconds_sum=7450, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=470, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=14, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=9950, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=720, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.request_params_max_tokens.p95.60=4850, llm_service.vllm.request_generation_tokens.p99.60=497, llm_service.vllm.request_params_n_count.engine_0=1, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=4970, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=9950, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.request_generation_tokens.p90.60=470, llm_service.vllm.prefix_cache_queries_total.engine_0=1440, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.finished_reason_stop=1, llm_service.vllm.request_prompt_tokens.p99.60=995, llm_service.vllm.request_generation_tokens.p50.60=350, llm_service.vllm.request_generation_tokens_count=1, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=9750, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=1, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=1, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=9500, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=725, llm_service.vllm.request_prefill_time_milliseconds_count=1, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=497, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=797, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=975, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=350, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=1, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=720, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=464, llm_service.vllm.request_prompt_tokens_count=1, llm_service.vllm.request_prompt_tokens.p50.60=750, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.inter_token_latency_milliseconds_sum=3419, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=7360, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=747, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=563, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=7500, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=7500, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.request_max_num_generation_tokens_count=1, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=1, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=950, llm_service.vllm.time_to_first_token_milliseconds.p50.60=625, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=350, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=261, llm_service.vllm.request_params_n_count=1, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_sum=16, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.prompt_tokens_total.sum.60=720, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=1185, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=1, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=24, llm_service.vllm.request_prompt_tokens_sum=720, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=522, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=785, llm_service.vllm.request_prefill_kv_computed_tokens_avg=360, llm_service.vllm.request_time_per_output_token_milliseconds_sum=7, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=9500, llm_service.vllm.request_params_max_tokens_sum.engine_0=4096, llm_service.vllm.request_queue_time_milliseconds.p95.60=285, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=7360, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_params_max_tokens_count.engine_0=1, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=1440, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=563, llm_service.vllm.request_inference_time_milliseconds.p95.60=9750, llm_service.vllm.inter_token_latency_milliseconds_avg=7, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=995, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=6838, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_sum=3419, llm_service.vllm.request_prefill_time_milliseconds_sum=261, llm_service.vllm.request_queue_time_milliseconds.p99.60=297, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=7450, llm_service.vllm.external_prefix_cache_queries_total.sum.60=720, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=785, llm_service.vllm.request_params_max_tokens_avg.engine_0=4096, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=9750, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=9750, llm_service.vllm.request_params_n_sum.engine_0=1, llm_service.vllm.request_inference_time_milliseconds_count=1, llm_service.vllm.time_to_first_token_milliseconds.p90.60=725, llm_service.vllm.e2e_request_latency_milliseconds_count=1, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=7, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=23, llm_service.vllm.request_generation_tokens.engine_0.p95.60=485, llm_service.vllm.request_max_num_generation_tokens.p90.60=470, llm_service.vllm.request_prefill_kv_computed_tokens_sum=720, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=770, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=33, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens.p95.60=485, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=522, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=950, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=17, llm_service.http_request_duration_highr_milliseconds_count=1, llm_service.vllm.request_max_num_generation_tokens_avg=232, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=1, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=485, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=950, llm_service.vllm.request_queue_time_milliseconds.p90.60=270, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=720, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=1, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=2, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=995, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.iteration_tokens_total_count=465, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.request_params_n_avg.engine_0=1, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=7401, llm_service.http_request_duration_highr_milliseconds_avg=7450, llm_service.vllm.request_generation_tokens.engine_0.p90.60=470, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=24, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=24, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.prompt_tokens_total.engine_0=720, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=150, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=7500, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=737, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=1, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=7401, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=9500, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.prefix_cache_queries_total=1440, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=7500, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens_count=1, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=285, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=995, llm_service.vllm.request_decode_time_milliseconds_count=1, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=23, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total=720, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=9500, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=17, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=650, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=14, llm_service.vllm.e2e_request_latency_milliseconds_sum=3700, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=650, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=465, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=1, llm_service.vllm.e2e_request_latency_milliseconds_avg=3700, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=7500, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.generation_tokens_total=465, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=1, llm_service.vllm.iteration_tokens_total_count.engine_0=465, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=720, llm_service.vllm.inter_token_latency_milliseconds.p99.60=24, llm_service.vllm.request_queue_time_milliseconds_avg=16, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=720, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=497, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=24, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=17, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=14, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_generation_tokens_sum=465, llm_service.vllm.request_inference_time_milliseconds_avg=3680, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=750, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=1, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=6838, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=24, llm_service.http_request_duration_highr_milliseconds.p99.60=7475, llm_service.vllm.generation_tokens_total.engine_0=465, llm_service.vllm.request_generation_tokens_avg.engine_0=465, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=720, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=485, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=9500, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens.p95.60=975, llm_service.vllm.request_prompt_tokens.p90.60=950, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=4700, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=9750, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.request_generation_tokens_sum.engine_0=465, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.iteration_tokens_total_avg=1, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_count=1, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=3500, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_params_max_tokens_sum=4096, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=625, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=24, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=720, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.request_params_max_tokens.p90.60=4700, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=9750, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=465, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=6838, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=9950, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=1, llm_service.vllm.inter_token_latency_milliseconds_count=464, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_max_tokens_avg=2048, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=7450, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=23, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=1, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=17, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=24, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.http_request_duration_highr_milliseconds.p95.60=7375, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=270, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=9950, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.request_decode_time_milliseconds_avg=3419, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=1, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=1, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=9950, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_count=1, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=7500, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=23, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=497, llm_service.vllm.time_to_first_token_milliseconds_avg=281, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.prefix_cache_queries_total.sum.60=1440, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=737, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=770, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_sum=465, llm_service.vllm.request_max_num_generation_tokens.p50.60=350, llm_service.vllm.request_generation_tokens_count.engine_0=1, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.prompt_tokens_total=720, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=750, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_sum=3680, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=4850, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=1, llm_service.vllm.request_generation_tokens_avg=232
0208 08:15:07.390671  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:15:07.393337  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:15:07.393348  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:15:07.393350  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:15:07.393353  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:15:07.393354  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:15:07.412064  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.07s
V0208 08:15:13.504825 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:15:23.504994 12556 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:15:33.505163 12556 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:15:43.505317 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:15:53.505493 12556 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:16:03.505664 12556 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_1 pid=307)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:50020 - "GET /metrics HTTP/1.1" 200 OK
0208 08:16:07.353956  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:16:07.354188  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0
0208 08:16:07.390768  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:16:07.393637  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:16:07.393650  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:16:07.393659  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:16:07.393661  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:16:07.393662  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:16:07.748076  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.41s
V0208 08:16:13.505821 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:16:23.505968 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:16:33.506159 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
I0208 08:16:33.515435  5694 Perftools.cpp:367] Perftools::getHeapProfile() - Profiler done, profile written to /tmp/service.138-17.heap
V0208 08:16:43.506341 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:16:53.506498 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:17:03.506670 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:51656 - "GET /metrics HTTP/1.1" 200 OK
0208 08:17:07.354039  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:17:07.354283  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.generation_tokens_total=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0
0208 08:17:07.390851  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:17:07.394144  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:17:07.394155  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:17:07.394158  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:17:07.394159  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:17:07.394161  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:17:07.488762  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.15s
V0208 08:17:13.506858 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:17:23.507005 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:17:33.507162 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:17:43.507325 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:17:53.507481 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:18:03.507633 12580 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:45404 - "GET /metrics HTTP/1.1" 200 OK
0208 08:18:07.354220  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:18:07.354473  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0
0208 08:18:07.391051  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:18:07.394504  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:18:07.394515  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:18:07.394518  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:18:07.394520  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:18:07.394521  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:18:07.489168  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.15s
V0208 08:18:13.507778 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:18:23.507945 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:18:33.508128 12634 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:18:43.508260 12634 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:18:53.508462 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:19:03.508643 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:46168 - "GET /metrics HTTP/1.1" 200 OK
0208 08:19:07.354476  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:19:07.354724  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0
0208 08:19:07.390274  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:19:07.393872  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:19:07.393883  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:19:07.393885  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:19:07.393887  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:19:07.393889  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:19:07.487086  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.15s
V0208 08:19:13.508818 12604 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:19:23.508972 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:19:33.509136 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:19:43.509290 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:19:53.509487 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:20:03.509640 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:35654 - "GET /metrics HTTP/1.1" 200 OK
0208 08:20:07.354225  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:20:07.354470  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_sum=0
0208 08:20:07.390103  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:20:07.393480  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:20:07.393490  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:20:07.393493  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:20:07.393494  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:20:07.393496  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:20:07.486682  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.15s
V0208 08:20:13.509823 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:20:23.509980 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:20:33.510128 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:20:43.510284 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:20:53.510435 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:21:03.510629 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_1 pid=307)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:36680 - "GET /metrics HTTP/1.1" 200 OK
0208 08:21:07.354244  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:21:07.354487  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.request_params_n_count=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0
0208 08:21:07.390092  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:21:07.393073  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:21:07.393083  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:21:07.393086  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:21:07.393088  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:21:07.393089  INFO tokio-runtime-worker ThreadId(139) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:21:07.488704  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.15s
V0208 08:21:13.510803 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:21:23.510947 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:21:33.511111 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:21:43.511305 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:21:53.511483 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:22:03.511643 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:54476 - "GET /metrics HTTP/1.1" 200 OK
0208 08:22:07.354039  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:22:07.354286  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0
0208 08:22:07.384047  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.04s
0208 08:22:07.391049  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:22:07.393886  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:22:07.393894  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:22:07.393897  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:22:07.393898  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:22:07.393900  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
V0208 08:22:13.511799 12658 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:22:23.511958 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:22:33.512116 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:22:43.512268 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:22:53.512431 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:23:03.512597 12693 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_1 pid=307)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:62450 - "GET /metrics HTTP/1.1" 200 OK
0208 08:23:07.353554  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:23:07.353788  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.vllm.request_params_n_count=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0
0208 08:23:07.390375  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:23:07.393824  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:23:07.393836  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:23:07.393838  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:23:07.393840  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:23:07.393841  INFO tokio-runtime-worker ThreadId(140) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:23:07.749215  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.41s
V0208 08:23:13.512770 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:23:23.512918 12693 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:23:33.513067 12657 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:23:43.513239 12693 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:23:53.513418 12693 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:24:03.513595 12693 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:33634 - "GET /metrics HTTP/1.1" 200 OK
0208 08:24:07.354024  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:677: Calculated 160 histogram percentile metrics
0208 08:24:07.354271  INFO tokio-runtime-worker ThreadId(140) lib::metrics_thread:219: Collected 524 service metrics: llm_service.vllm.external_prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length=0, llm_service.http_request_duration_milliseconds_count.handler_v1_chat_completions=0, llm_service.vllm.request_success_total.finished_reason_abort=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_descriptors_sum.engine_1=0, llm_service.vllm.prefix_cache_queries_total.engine_0.sum.60=0, llm_service.vllm.request_queue_time_milliseconds_sum=0, llm_service.vllm.iteration_tokens_total.engine_0.p95.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total_count=0, llm_service.vllm.prefix_cache_queries_total.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.engine_sleep_state=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_1=0, llm_service.vllm.prefix_cache_queries_total=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0=0, llm_service.vllm.request_generation_tokens_avg=0, llm_service.vllm.e2e_request_latency_milliseconds_avg=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum=0, llm_service.vllm.request_prompt_tokens_avg.engine_1=0, llm_service.vllm.iteration_tokens_total.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.iteration_tokens_total.engine_0.p90.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count=0, llm_service.vllm.request_params_max_tokens_count.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_0.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.p50.60=0, llm_service.vllm.mm_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_sum=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_inference_time_milliseconds_count=0, llm_service.vllm.request_queue_time_milliseconds_count=0, llm_service.vllm.request_success_total.finished_reason_error.sum.60=0, llm_service.vllm.nixl_bytes_transferred_count.engine_1=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.sum.60=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_0=0, llm_service.vllm.request_success_total.finished_reason_length.sum.60=0, llm_service.vllm.mm_cache_hits_total.sum.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_num_descriptors_count.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p95.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum.engine_0=0, llm_service.vllm.nixl_bytes_transferred_count.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_length.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_descriptors_avg.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_generation_tokens_count.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_success_total.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_bytes_transferred_avg.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_sum=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_1=0, llm_service.vllm.engine_sleep_state.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_0=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort=0, llm_service.vllm.request_generation_tokens_avg.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.iteration_tokens_total.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_params_n.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_params_max_tokens.engine_1.p90.60=0, llm_service.vllm.request_success_total.finished_reason_error=0, llm_service.vllm.request_params_n.engine_0.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_0=0, llm_service.vllm.nixl_num_descriptors_avg=0, llm_service.vllm.request_prompt_tokens.p50.60=0, llm_service.vllm.request_generation_tokens_sum.engine_0=0, llm_service.vllm.request_prompt_tokens.engine_0.p99.60=0, llm_service.http_request_duration_highr_milliseconds_avg=0, llm_service.vllm.request_prefill_time_milliseconds_avg=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens_avg.engine_0=0, llm_service.vllm.request_params_n_sum=0, llm_service.vllm.iteration_tokens_total_avg=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p99.60=0, llm_service.vllm.generation_tokens_total=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0.sum.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p95.60=0, llm_service.vllm.iteration_tokens_total_sum=0, llm_service.vllm.iteration_tokens_total.engine_1.p95.60=0, llm_service.http_response_size_bytes_sum.handler_v1_chat_completions=3753, llm_service.vllm.nixl_bytes_transferred.engine_1.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_1=0, llm_service.vllm.mm_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_sum=0, llm_service.vllm.e2e_request_latency_milliseconds.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_0=0, llm_service.vllm.request_generation_tokens_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_1=0, llm_service.vllm.nixl_num_descriptors_count=0, llm_service.vllm.mm_cache_queries_total.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.p99.60=0, llm_service.vllm.nixl_bytes_transferred.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p95.60=0, llm_service.vllm.request_decode_time_milliseconds_count=0, llm_service.vllm.request_prefill_time_milliseconds.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.request_success_total.finished_reason_abort.sum.60=0, llm_service.vllm.request_prompt_tokens_avg=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_0=0, llm_service.http_request_duration_highr_milliseconds_count=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p95.60=0, llm_service.vllm.request_prompt_tokens_count.engine_0=0, llm_service.http_request_duration_milliseconds_sum.handler_v1_chat_completions=0, llm_service.vllm.request_queue_time_milliseconds_avg.engine_1=0, llm_service.http_request_size_bytes_sum.handler_v1_chat_completions=3301, llm_service.vllm.inter_token_latency_milliseconds.p90.60=0, llm_service.vllm.request_decode_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_1.p99.60=0, llm_service.vllm.prompt_tokens_total.engine_0.sum.60=0, llm_service.vllm.cache_config_info=2, llm_service.vllm.generation_tokens_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p95.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error=0, llm_service.vllm.request_params_max_tokens_count.engine_0=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p95.60=0, llm_service.vllm.request_params_max_tokens.engine_0.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p50.60=0, llm_service.vllm.mm_cache_hits_total.engine_0=0, llm_service.vllm.iteration_tokens_total_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count=0, llm_service.vllm.inter_token_latency_milliseconds_sum.engine_0=0, llm_service.vllm.request_decode_time_milliseconds_avg.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds_count=0, llm_service.vllm.iteration_tokens_total.p50.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_1=0, llm_service.vllm.request_params_n.engine_1.p99.60=0, llm_service.vllm.request_params_n.p95.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p95.60=0, llm_service.vllm.prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.request_params_n.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_1=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds_sum=0, llm_service.vllm.request_params_n_count=0, llm_service.http_request_duration_highr_milliseconds_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_0=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p95.60=0, llm_service.http_response_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_prefill_kv_computed_tokens_sum.engine_0=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens_sum.engine_0=0, llm_service.vllm.inter_token_latency_milliseconds.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.time_to_first_token_milliseconds_avg=0, llm_service.vllm.nixl_num_kv_expired_reqs_total.engine_0=0, llm_service.vllm.external_prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_0=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p95.60=0, llm_service.http_request_duration_highr_milliseconds.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds_count.engine_0=0, llm_service.vllm.request_success_total.finished_reason_stop=0, llm_service.vllm.external_prefix_cache_hits_total=0, llm_service.vllm.time_to_first_token_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_params_max_tokens_avg.engine_1=0, llm_service.vllm.num_preemptions_total.engine_1.sum.60=0, llm_service.vllm.request_max_num_generation_tokens_sum.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens_sum.engine_1=0, llm_service.vllm.request_params_max_tokens_sum.engine_0=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total=0, llm_service.vllm.request_params_n_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_avg.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p95.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.nixl_bytes_transferred.p90.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p50.60=0, llm_service.vllm.prefix_cache_hits_total=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.nixl_post_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.request_params_n.p50.60=0, llm_service.vllm.num_preemptions_total.engine_0.sum.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_abort.sum.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_1.p99.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg=0, llm_service.vllm.request_params_n_avg.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds.p95.60=0, llm_service.vllm.num_requests_running.engine_0=0, llm_service.vllm.request_generation_tokens_count=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds.p90.60=0, llm_service.vllm.request_params_n_avg=0, llm_service.vllm.request_prefill_time_milliseconds_sum=0, llm_service.vllm.request_inference_time_milliseconds_sum=0, llm_service.vllm.request_success_total.engine_1.finished_reason_error.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.p50.60=0, llm_service.vllm.nixl_post_time_milliseconds.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens_sum=0, llm_service.vllm.nixl_num_descriptors.p95.60=0, llm_service.vllm.request_params_n.engine_1.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0=0, llm_service.vllm.nixl_num_descriptors.engine_0.p50.60=0, llm_service.http_request_duration_highr_milliseconds.p95.60=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_1.sum.60=0, llm_service.vllm.inter_token_latency_milliseconds_sum=0, llm_service.vllm.request_params_n.engine_1.p95.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p50.60=0, llm_service.vllm.request_params_n.engine_0.p50.60=0, llm_service.vllm.request_max_num_generation_tokens_count=0, llm_service.vllm.iteration_tokens_total_count.engine_1=0, llm_service.vllm.request_prefill_time_milliseconds_count.engine_0=0, llm_service.vllm.mm_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_prompt_tokens.p95.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p95.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_count=0, llm_service.vllm.prefix_cache_hits_total.engine_0=0, llm_service.vllm.nixl_bytes_transferred.p99.60=0, llm_service.vllm.iteration_tokens_total.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p99.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p90.60=0, llm_service.vllm.prompt_tokens_total.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens.engine_0.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum=0, llm_service.vllm.request_prefill_kv_computed_tokens_count.engine_0=0, llm_service.vllm.request_generation_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_num_descriptors.p90.60=0, llm_service.vllm.cache_config_info.engine_1=1, llm_service.vllm.nixl_post_time_milliseconds.engine_0.p50.60=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds.p99.60=0, llm_service.vllm.request_generation_tokens.p99.60=0, llm_service.vllm.request_params_max_tokens.p95.60=0, llm_service.vllm.nixl_num_descriptors_sum.engine_0=0, llm_service.vllm.nixl_post_time_milliseconds_count=0, llm_service.vllm.generation_tokens_total.engine_1=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds_avg=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions=0, llm_service.vllm.request_params_n_sum.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.nixl_num_descriptors.engine_0.p99.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p90.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.request_prefill_time_milliseconds.p90.60=0, llm_service.vllm.mm_cache_queries_total.sum.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error=0, llm_service.http_requests_total.status_2xx.handler_v1_chat_completions.sum.60=0, llm_service.vllm.nixl_num_failed_notifications_total=0, llm_service.vllm.request_prefill_time_milliseconds_count=0, llm_service.vllm.request_params_max_tokens.engine_0.p50.60=0, llm_service.vllm.request_generation_tokens.p50.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.p95.60=0, llm_service.vllm.request_generation_tokens.p95.60=0, llm_service.vllm.request_inference_time_milliseconds_sum.engine_1=0, llm_service.vllm.request_success_total.finished_reason_length=0, llm_service.vllm.time_to_first_token_milliseconds_count=0, llm_service.vllm.generation_tokens_total.engine_0=0, llm_service.vllm.nixl_bytes_transferred_avg=0, llm_service.vllm.request_inference_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.prefix_cache_queries_total.sum.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_0.sum.60=0, llm_service.vllm.num_preemptions_total.sum.60=0, llm_service.vllm.request_inference_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_n_sum.engine_1=0, llm_service.vllm.nixl_num_descriptors_avg.engine_1=0, llm_service.vllm.inter_token_latency_milliseconds.engine_0.p50.60=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p95.60=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_1=0, llm_service.vllm.nixl_bytes_transferred.p95.60=0, llm_service.vllm.nixl_num_descriptors.p99.60=0, llm_service.vllm.iteration_tokens_total_count.engine_0=0, llm_service.vllm.num_preemptions_total.engine_0=0, llm_service.vllm.request_success_total.engine_0.finished_reason_stop=0, llm_service.vllm.request_params_max_tokens.engine_1.p95.60=0, llm_service.vllm.time_to_first_token_milliseconds.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_avg.engine_0=0, llm_service.vllm.request_decode_time_milliseconds.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_avg.engine_1=0, llm_service.vllm.engine_sleep_state.engine_1=0, llm_service.vllm.request_params_max_tokens.engine_1.p50.60=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1.sum.60=0, llm_service.vllm.external_prefix_cache_queries_total.sum.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_1=0, llm_service.vllm.request_prompt_tokens_count=0, llm_service.vllm.e2e_request_latency_milliseconds_avg.engine_1=0, llm_service.vllm.request_params_max_tokens.p99.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_0=0, llm_service.vllm.request_params_n.p99.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p50.60=0, llm_service.vllm.request_queue_time_milliseconds_count.engine_1=0, llm_service.vllm.request_time_per_output_token_milliseconds.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.p95.60=0, llm_service.vllm.nixl_num_failed_notifications_total.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_0=0, llm_service.vllm.request_params_max_tokens_sum=0, llm_service.http_request_duration_milliseconds.handler_v1_chat_completions.p90.60=0, llm_service.vllm.request_params_max_tokens_count=0, llm_service.vllm.nixl_bytes_transferred_sum.engine_0=0, llm_service.vllm.cache_config_info.engine_0=1, llm_service.vllm.request_params_n.engine_1.p90.60=0, llm_service.vllm.request_prompt_tokens.engine_1.p50.60=0, llm_service.vllm.request_generation_tokens.engine_0.p50.60=0, llm_service.vllm.external_prefix_cache_queries_total=0, llm_service.vllm.request_prefill_kv_computed_tokens.p95.60=0, llm_service.vllm.request_prompt_tokens_sum=0, llm_service.vllm.request_generation_tokens.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens_sum=0, llm_service.vllm.request_generation_tokens.engine_1.p90.60=0, llm_service.vllm.request_generation_tokens.engine_0.p99.60=0, llm_service.vllm.mm_cache_hits_total=0, llm_service.vllm.request_generation_tokens.engine_1.p95.60=0, llm_service.vllm.nixl_post_time_milliseconds_avg.engine_1=0, llm_service.vllm.request_prompt_tokens.p90.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_avg=0, llm_service.vllm.nixl_bytes_transferred_count=0, llm_service.vllm.time_to_first_token_milliseconds_count.engine_0=0, llm_service.vllm.iteration_tokens_total.p90.60=0, llm_service.vllm.nixl_num_descriptors_sum=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p50.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_1=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p95.60=0, llm_service.vllm.mm_cache_queries_total=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg=0, llm_service.vllm.mm_cache_queries_total.engine_1=0, llm_service.vllm.request_success_total.engine_0.finished_reason_length=0, llm_service.vllm.request_max_num_generation_tokens_avg.engine_0=0, llm_service.vllm.kv_cache_usage_perc.engine_0=0, llm_service.vllm.time_to_first_token_milliseconds.engine_1.p90.60=0, llm_service.vllm.nixl_num_failed_notifications_total.sum.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum.engine_1=0, llm_service.vllm.request_queue_time_milliseconds_avg=0, llm_service.vllm.request_params_max_tokens_avg=0, llm_service.vllm.request_time_per_output_token_milliseconds_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_0.p95.60=0, llm_service.vllm.num_requests_waiting.engine_0=0, llm_service.vllm.request_queue_time_milliseconds.p99.60=0, llm_service.vllm.iteration_tokens_total_avg.engine_1=0, llm_service.vllm.request_decode_time_milliseconds_sum.engine_0=0, llm_service.vllm.prompt_tokens_total=0, llm_service.vllm.request_max_num_generation_tokens_count.engine_1=0, llm_service.vllm.request_max_num_generation_tokens.engine_1.p99.60=0, llm_service.vllm.nixl_num_descriptors.p50.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_abort=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1=0, llm_service.vllm.request_prompt_tokens.engine_1.p99.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_1=0, llm_service.vllm.external_prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p50.60=0, llm_service.vllm.generation_tokens_total.engine_0.sum.60=0, llm_service.vllm.request_max_num_generation_tokens.engine_0.p95.60=0, llm_service.vllm.request_prompt_tokens_count.engine_1=0, llm_service.vllm.request_params_max_tokens.p50.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_queue_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.external_prefix_cache_queries_total.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p90.60=0, llm_service.vllm.prefix_cache_hits_total.sum.60=0, llm_service.vllm.request_decode_time_milliseconds.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_sum=0, llm_service.vllm.request_time_per_output_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_inference_time_milliseconds_count.engine_0=0, llm_service.vllm.iteration_tokens_total.engine_0.p99.60=0, llm_service.vllm.request_generation_tokens.engine_0.p90.60=0, llm_service.vllm.mm_cache_queries_total.engine_0.sum.60=0, llm_service.http_request_size_bytes_count.handler_v1_chat_completions=1, llm_service.vllm.request_prompt_tokens.p99.60=0, llm_service.vllm.request_max_num_generation_tokens.p90.60=0, llm_service.vllm.iteration_tokens_total_sum.engine_1=0, llm_service.vllm.request_generation_tokens.engine_1.p50.60=0, llm_service.vllm.request_prefill_time_milliseconds_sum.engine_1=0, llm_service.vllm.time_to_first_token_milliseconds_avg.engine_0=0, llm_service.vllm.nixl_xfer_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.e2e_request_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_1.p99.60=0, llm_service.vllm.request_queue_time_milliseconds.p90.60=0, llm_service.vllm.request_max_num_generation_tokens.p50.60=0, llm_service.vllm.request_inference_time_milliseconds_avg=0, llm_service.vllm.request_max_num_generation_tokens.p99.60=0, llm_service.vllm.external_prefix_cache_hits_total.engine_1.sum.60=0, llm_service.http_request_duration_milliseconds_avg.handler_v1_chat_completions=0, llm_service.http_request_duration_highr_milliseconds.p99.60=0, llm_service.vllm.generation_tokens_total.sum.60=0, llm_service.vllm.nixl_post_time_milliseconds_count.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.p50.60=0, llm_service.vllm.request_decode_time_milliseconds.p50.60=0, llm_service.vllm.request_success_total.engine_1.finished_reason_stop=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p99.60=0, llm_service.vllm.time_to_first_token_milliseconds.p99.60=0, llm_service.vllm.request_params_max_tokens.p90.60=0, llm_service.vllm.inter_token_latency_milliseconds.engine_1.p90.60=0, llm_service.vllm.request_max_num_generation_tokens_avg=0, llm_service.vllm.request_params_max_tokens.engine_0.p90.60=0, llm_service.vllm.nixl_num_descriptors.engine_1.p99.60=0, llm_service.vllm.request_params_n_count.engine_1=0, llm_service.vllm.request_params_n_count.engine_0=0, llm_service.vllm.prompt_tokens_total.sum.60=0, llm_service.vllm.request_prompt_tokens.engine_0.p90.60=0, llm_service.vllm.request_prefill_kv_computed_tokens.p99.60=0, llm_service.vllm.num_preemptions_total.engine_1=0, llm_service.vllm.request_decode_time_milliseconds.engine_1.p95.60=0, llm_service.vllm.nixl_num_kv_expired_reqs_total=0, llm_service.vllm.request_prefill_time_milliseconds.p50.60=0, llm_service.vllm.time_to_first_token_milliseconds_sum.engine_0=0, llm_service.vllm.request_prefill_kv_computed_tokens.engine_0.p95.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds.p99.60=0, llm_service.vllm.num_preemptions_total=0, llm_service.vllm.request_params_n.engine_0.p99.60=0, llm_service.vllm.nixl_xfer_time_milliseconds_count.engine_1=0, llm_service.vllm.nixl_bytes_transferred_sum=0, llm_service.vllm.prefix_cache_hits_total.engine_1.sum.60=0, llm_service.vllm.request_prefill_time_milliseconds.engine_0.p99.60=0, llm_service.vllm.e2e_request_latency_milliseconds_count.engine_0=0, llm_service.vllm.nixl_num_failed_transfers_total.engine_1=0, llm_service.vllm.iteration_tokens_total.engine_1.p99.60=0, llm_service.vllm.mm_cache_hits_total.engine_1=0, llm_service.vllm.request_generation_tokens_avg.engine_1=0, llm_service.vllm.nixl_bytes_transferred.engine_0.p50.60=0, llm_service.vllm.request_inference_time_milliseconds.engine_1.p50.60=0, llm_service.vllm.request_decode_time_milliseconds_avg=0, llm_service.http_request_duration_highr_milliseconds.p90.60=0, llm_service.vllm.request_time_per_output_token_milliseconds_count=0, llm_service.vllm.nixl_xfer_time_milliseconds.p90.60=0, llm_service.vllm.request_success_total.engine_0.finished_reason_error.sum.60=0, llm_service.vllm.prefix_cache_hits_total.engine_0.sum.60=0, llm_service.vllm.request_params_max_tokens_sum.engine_1=0, llm_service.vllm.request_inference_time_milliseconds.p50.60=0, llm_service.vllm.inter_token_latency_milliseconds.p95.60=0
0208 08:24:07.390882  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:65: Querying SMC tier for workers: smc.rift_congc_kimi_prefill_9f504et
0208 08:24:07.393599  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:108: Discovered 4 enabled workers from SMC tier smc.rift_congc_kimi_prefill_9f504et
0208 08:24:07.393609  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2af7:5b40:12c4:0400:0000]:8081 (port 8081)
0208 08:24:07.393611  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 (port 8081)
0208 08:24:07.393613  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 (port 8081)
0208 08:24:07.393615  INFO tokio-runtime-worker ThreadId(110) router::smc_worker_discovery:114:   - https://[2803:6086:0878:2fca:4a5f:12c4:0400:0000]:8081 (port 8081)
0208 08:24:07.487223  INFO tokio-runtime-worker ThreadId(139) lib::metrics_thread:246: Periodic task to report metrics to ODS finished in 0.15s
V0208 08:24:13.513794 12693 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
0208 08:24:14.745791  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1119: vLLM route_chat called, use_discovery=false request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745825  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1151: Using direct URL mode with VllmPDRouter's own routing logic request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745897  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1175: Found 4 prefill workers, 1 decode workers from worker_registry request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745937  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::policies::consistent_hash:593: CONSISTENT_HASH_DEBUG: Extracted hash key: request_hash:fa71feea80638a1c request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745943  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::policies::consistent_hash:598: CONSISTENT_HASH_DEBUG: Hash key 'request_hash:fa71feea80638a1c' mapped to worker: https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745947  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::policies::consistent_hash:647: Consistent hash routing: key='request_hash:fa71feea80638a1c' -> worker='https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081' (index=3) request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745967  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::policies::consistent_hash:593: CONSISTENT_HASH_DEBUG: Extracted hash key: request_hash:fa71feea80638a1c request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745971  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::policies::consistent_hash:598: CONSISTENT_HASH_DEBUG: Hash key 'request_hash:fa71feea80638a1c' mapped to worker: https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081 request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745974  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::policies::consistent_hash:647: Consistent hash routing: key='request_hash:fa71feea80638a1c' -> worker='https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081' (index=0) request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.745977  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1227: Chat: Selected prefill=https://[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081 [policy:consistent_hash], decode=https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081 [policy:consistent_hash] request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753063  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1119: vLLM route_chat called, use_discovery=false request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753081  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1151: Using direct URL mode with VllmPDRouter's own routing logic request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753158  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1175: Found 4 prefill workers, 1 decode workers from worker_registry request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753188  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:593: CONSISTENT_HASH_DEBUG: Extracted hash key: request_hash:35795d1b4904338f request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753193  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:598: CONSISTENT_HASH_DEBUG: Hash key 'request_hash:35795d1b4904338f' mapped to worker: https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753197  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:647: Consistent hash routing: key='request_hash:35795d1b4904338f' -> worker='https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081' (index=1) request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753214  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:593: CONSISTENT_HASH_DEBUG: Extracted hash key: request_hash:35795d1b4904338f request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753218  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:598: CONSISTENT_HASH_DEBUG: Hash key 'request_hash:35795d1b4904338f' mapped to worker: https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081 request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753220  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::policies::consistent_hash:647: Consistent hash routing: key='request_hash:35795d1b4904338f' -> worker='https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081' (index=0) request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:14.753224  INFO tokio-runtime-worker ThreadId(139) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1227: Chat: Selected prefill=https://[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081 [policy:consistent_hash], decode=https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081 [policy:consistent_hash] request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
[0;36m(ApiServer_0 pid=306)[0;0m WARNING 02-08 08:24:14 [protocol.py:53] The following fields were present in the request but ignored: {'return_hidden_states', 'stream_reasoning', 'separate_reasoning', 'no_stop_trim'}
[0;36m(EngineCore_DP0 pid=304)[0;0m INFO 02-08 08:24:14 [nixl_connector.py:1099] NIXL compatibility check passed (hash: 98f64d8e1600a363b1c7977f640ed099835460dca70f5fbce31070bc302474f4)
I0208 08:24:14.899801    3546 nixl_agent.cpp:1616] Loading remote metadata for agent: 6676c900-d2f6-4fbe-a017-019a7555adc1
[1770567854.899937] [twshared4216:304  :0]          select.c:652  UCX  ERROR   no active messages transport to <no debug data>: self/memory - no peer failure handler, tcp/eth0 - no route to 2803:6086:878:2c68:1360:12c4:400:0%929505860:65535, sysv/memory - no peer failure handler, posix/memory - no peer failure handler, cuda_copy/cuda - no am bcopy, cuda_ipc/cuda - no am bcopy,
W0208 08:24:14.899949    3546 ucx_utils.cpp:70] Unexpected UCX error: Destination is unreachable
E0208 08:24:14.899981    3546 nixl_agent.cpp:1639] loadRemoteMD: error loading connection info for backend 'UCX' with status NIXL_ERR_BACKEND
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258] NIXL transfer failure: handshake_setup_failed | Context: {'failure_type': 'handshake_setup_failed', 'request_id': None, 'engine_id': '9fee93dd-d148-4fee-9f9a-f9e2ea76b627_dp0', 'remote_engine_id': 'ebd9582d-cb25-485d-8284-5f46a114fa4b_dp1'}
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1256, in done_callback
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     self._remote_agents[eid] = f.result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]                                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     raise self._exception
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     result = self.fn(*self.args, **self.kwargs)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1126, in _nixl_handshake
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     remote_agent_name = self.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]                         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1595, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     remote_agent_name = self.nixl_wrapper.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/nixl_cu13/_api.py", line 801, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]     agent_name = self.agent.loadRemoteMD(metadata)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1258] nixl_cu13._bindings.nixlBackendError: NIXL_ERR_BACKEND
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275] NIXL transfer failure: handshake_failed | Context: {'failure_type': 'handshake_failed', 'request_id': 'chatcmpl-___prefill_addr_[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_ad2a78d1e3fd4e09bf2c415e44d61568-8c45acf8', 'engine_id': '9fee93dd-d148-4fee-9f9a-f9e2ea76b627_dp0', 'remote_engine_id': 'ebd9582d-cb25-485d-8284-5f46a114fa4b_dp1', 'remote_request_id': 'chatcmpl-___prefill_addr_[2803:6086:0878:2c68:1360:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_ad2a78d1e3fd4e09bf2c415e44d61568-bd4a00d6', 'remote_host': '2803:6086:0878:2c68:1360:12c4:0400:0000', 'remote_port': 5605, 'num_local_blocks': 2, 'num_remote_blocks': 12, 'local_block_ids_sample': [32, 33]}
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1271, in request_ready
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     f.result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     raise self._exception
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1256, in done_callback
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     self._remote_agents[eid] = f.result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]                                ^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     return self.__get_result()
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     raise self._exception
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     result = self.fn(*self.args, **self.kwargs)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1126, in _nixl_handshake
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     remote_agent_name = self.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]                         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1595, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     remote_agent_name = self.nixl_wrapper.add_remote_agent(
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/nixl_cu13/_api.py", line 801, in add_remote_agent
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]     agent_name = self.agent.loadRemoteMD(metadata)
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=304)[0;0m ERROR 02-08 08:24:14 [nixl_connector.py:1275] nixl_cu13._bindings.nixlBackendError: NIXL_ERR_BACKEND
[0;36m(EngineCore_DP0 pid=304)[0;0m WARNING 02-08 08:24:14 [scheduler.py:2116] Recovered from KV load failure: 1 request(s) rescheduled (128 tokens affected).
[0;36m(ApiServer_1 pid=307)[0;0m WARNING 02-08 08:24:15 [protocol.py:53] The following fields were present in the request but ignored: {'separate_reasoning', 'stream_reasoning', 'return_hidden_states', 'no_stop_trim'}
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693] AsyncLLM output_handler failed.
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693] Traceback (most recent call last):
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 686, in output_handler
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]     logger_manager.record(
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/metrics/loggers.py", line 1264, in record
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]     logger.record(
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/metrics/loggers.py", line 1017, in record
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]     self.counter_connector_prefix_cache_hits[engine_idx].inc(
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]   File "/usr/local/lib/python3.12/dist-packages/prometheus_client/metrics.py", line 339, in inc
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693]     raise ValueError('Counters can only be incremented by non-negative amounts.')
[0;36m(ApiServer_0 pid=306)[0;0m ERROR 02-08 08:24:15 [async_llm.py:693] ValueError: Counters can only be incremented by non-negative amounts.
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     2803:6086:908:94e4:48a:12c4:400:0:39988 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
0208 08:24:15.056611  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:867: ðŸ“¥ Decode response status: 400 Bad Request request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:15.056637  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:868: ðŸ“¥ Decode response headers: {"date": "Sun, 08 Feb 2026 16:24:14 GMT", "server": "uvicorn", "content-length": "130", "content-type": "application/json"} request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:15.056669  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1247: Two-stage processing completed successfully request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:15.057270 ERROR tokio-runtime-worker ThreadId(110) request_info:handler: common::thrift_predictor_service:173: HTTP request failed: Router error 400 Bad Request: {"error":{"code":400,"message":"Counters can only be incremented by non-negative amounts.","param":null,"type":"BadRequestError"},"prompt_logprobs":null} request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:15.057289 ERROR tokio-runtime-worker ThreadId(110) request_info:handler: llm_predictor_gpu_services:1007: method: "InferencePlatformSp_LlmPredictorGpu.predict", exception: ex2(ServiceException { message: "HTTP request failed: Router error 400 Bad Request: {\"error\":{\"code\":400,\"message\":\"Counters can only be incremented by non-negative amounts.\",\"param\":null,\"type\":\"BadRequestError\"},\"prompt_logprobs\":null}" }), error: "ServiceException { message: \"HTTP request failed: Router error 400 Bad Request: {\\\"error\\\":{\\\"code\\\":400,\\\"message\\\":\\\"Counters can only be incremented by non-negative amounts.\\\",\\\"param\\\":null,\\\"type\\\":\\\"BadRequestError\\\"},\\\"prompt_logprobs\\\":null}\" }" request_id: "06a00000000000bf" method: "InferencePlatformSp_LlmPredictorGpu.predict"
[0;36m(EngineCore_DP1 pid=305)[0;0m INFO 02-08 08:24:15 [nixl_connector.py:1099] NIXL compatibility check passed (hash: 98f64d8e1600a363b1c7977f640ed099835460dca70f5fbce31070bc302474f4)
I0208 08:24:15.059728    3562 nixl_agent.cpp:1616] Loading remote metadata for agent: 7545bd52-a1e8-4eb6-ac8d-2908fcc14d0e
[1770567855.059874] [twshared4216:305  :0]          select.c:652  UCX  ERROR   no active messages transport to <no debug data>: self/memory - no peer failure handler, tcp/eth0 - no route to 2803:6086:878:2bf6:2f5f:12c4:400:0%743514692:65535, sysv/memory - no peer failure handler, posix/memory - no peer failure handler, cuda_copy/cuda - no am bcopy, cuda_ipc/cuda - no am bcopy,
W0208 08:24:15.059883    3562 ucx_utils.cpp:70] Unexpected UCX error: Destination is unreachable
E0208 08:24:15.059925    3562 nixl_agent.cpp:1639] loadRemoteMD: error loading connection info for backend 'UCX' with status NIXL_ERR_BACKEND
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258] NIXL transfer failure: handshake_setup_failed | Context: {'failure_type': 'handshake_setup_failed', 'request_id': None, 'engine_id': '9fee93dd-d148-4fee-9f9a-f9e2ea76b627_dp1', 'remote_engine_id': 'c1b5d540-772a-4942-98a3-0710b6637cee_dp0'}
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258] Traceback (most recent call last):
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1256, in done_callback
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     self._remote_agents[eid] = f.result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]                                ^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     return self.__get_result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     raise self._exception
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     result = self.fn(*self.args, **self.kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1126, in _nixl_handshake
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     remote_agent_name = self.add_remote_agent(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]                         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1595, in add_remote_agent
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     remote_agent_name = self.nixl_wrapper.add_remote_agent(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]   File "/usr/local/lib/python3.12/dist-packages/nixl_cu13/_api.py", line 801, in add_remote_agent
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]     agent_name = self.agent.loadRemoteMD(metadata)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1258] nixl_cu13._bindings.nixlBackendError: NIXL_ERR_BACKEND
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275] NIXL transfer failure: handshake_failed | Context: {'failure_type': 'handshake_failed', 'request_id': 'chatcmpl-___prefill_addr_[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_423600768a7a4baab10494ab3c672b2a-b9925e59', 'engine_id': '9fee93dd-d148-4fee-9f9a-f9e2ea76b627_dp1', 'remote_engine_id': 'c1b5d540-772a-4942-98a3-0710b6637cee_dp0', 'remote_request_id': 'chatcmpl-___prefill_addr_[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_423600768a7a4baab10494ab3c672b2a-ba73bced', 'remote_host': '2803:6086:0878:2bf6:2f5f:12c4:0400:0000', 'remote_port': 5606, 'num_local_blocks': 11, 'num_remote_blocks': 11, 'local_block_ids_sample': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275] Traceback (most recent call last):
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1271, in request_ready
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     f.result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     return self.__get_result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     raise self._exception
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1256, in done_callback
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     self._remote_agents[eid] = f.result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]                                ^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     return self.__get_result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     raise self._exception
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     result = self.fn(*self.args, **self.kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1126, in _nixl_handshake
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     remote_agent_name = self.add_remote_agent(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]                         ^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py", line 1595, in add_remote_agent
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     remote_agent_name = self.nixl_wrapper.add_remote_agent(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]   File "/usr/local/lib/python3.12/dist-packages/nixl_cu13/_api.py", line 801, in add_remote_agent
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]     agent_name = self.agent.loadRemoteMD(metadata)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:15 [nixl_connector.py:1275] nixl_cu13._bindings.nixlBackendError: NIXL_ERR_BACKEND
[0;36m(EngineCore_DP1 pid=305)[0;0m WARNING 02-08 08:24:15 [scheduler.py:2116] Recovered from KV load failure: 1 request(s) rescheduled (704 tokens affected).
WARNING 02-08 08:24:15 [coordinator.py:326] Received stats for out-of-order step (1, 5) from engine 3 (expected > (1, 6))
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     Shutting down
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     Waiting for application shutdown.
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     Application shutdown complete.
[0;36m(ApiServer_0 pid=306)[0;0m INFO:     Finished server process [306]
ERROR 02-08 08:24:16 [utils.py:290] Exception occurred while running API servers: Process ApiServer_0 (PID: 306) died with exit code None
ERROR 02-08 08:24:16 [utils.py:290] Traceback (most recent call last):
ERROR 02-08 08:24:16 [utils.py:290]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
ERROR 02-08 08:24:16 [utils.py:290]     raise RuntimeError(
ERROR 02-08 08:24:16 [utils.py:290] RuntimeError: Process ApiServer_0 (PID: 306) died with exit code None
INFO 02-08 08:24:16 [utils.py:293] Terminating remaining processes ...
[0;36m(ApiServer_1 pid=307)[0;0m INFO 02-08 08:24:16 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(ApiServer_1 pid=307)[0;0m INFO:     Shutting down
[0;36m(ApiServer_1 pid=307)[0;0m INFO:     Waiting for connections to close. (CTRL+C to force quit)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [dump_input.py:72] Dumping input data for V1 LLM engine (v0.15.0rc2.dev19+g2b569e501) with config: model='/data/local/models/Kimi-K2.5', speculative_config=None, tokenizer='/data/local/models/Kimi-K2.5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=8, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='kimi_k2', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=default, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '/root/.cache/vllm/torch_compile_cache/0ec188e7f0', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer', 'vllm::rocm_aiter_sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': False}, 'local_cache_dir': '/root/.cache/vllm/torch_compile_cache/0ec188e7f0/rank_0_1/backbone'},
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [dump_input.py:79] Dumping scheduler output for model execution: SchedulerOutput(scheduled_new_reqs=[], scheduled_cached_reqs=CachedRequestData(req_ids=['chatcmpl-___prefill_addr_[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_423600768a7a4baab10494ab3c672b2a-b9925e59'],resumed_req_ids=set(),new_token_ids_lens=[],all_token_ids_lens={},new_block_ids=[None],num_computed_tokens=[765],num_output_tokens=[67]), num_scheduled_tokens={chatcmpl-___prefill_addr_[2803:6086:0878:2bf6:2f5f:12c4:0400:0000]:8081___decode_addr_[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081_423600768a7a4baab10494ab3c672b2a-b9925e59: 1}, total_num_scheduled_tokens=1, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[12], finished_req_ids=[], free_encoder_mm_hashes=[], preempted_req_ids=[], has_structured_output_requests=false, pending_structured_output_tokens=false, num_invalid_spec_tokens=null, kv_connector_metadata=NixlConnectorMetadata(reqs_to_recv={}, reqs_to_save={}, reqs_to_send={}, reqs_in_batch=[], reqs_not_processed=[]), ec_connector_metadata=null)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [dump_input.py:81] Dumping scheduler stats: SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, step_counter=0, current_wave=0, kv_cache_usage=0.0008091160407255327, prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0, preempted_requests=0, preempted_queries=0, preempted_hits=0), connector_prefix_cache_stats=PrefixCacheStats(reset=False, requests=0, queries=0, hits=0, preempted_requests=0, preempted_queries=0, preempted_hits=0), kv_cache_eviction_events=[], spec_decoding_stats=None, kv_connector_stats=None, waiting_lora_adapters={}, running_lora_adapters={}, cudagraph_stats=None, perf_stats=None)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948] EngineCore encountered a fatal error.
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948] Traceback (most recent call last):
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     engine_core.run_busy_loop()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 1357, in run_busy_loop
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     executed = self._process_engine_step()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 999, in _process_engine_step
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     outputs, model_executed = self.step_fn()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]                               ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 490, in step_with_batch_queue
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     exec_model_fut.result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     return self.__get_result()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]            ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     raise self._exception
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/uniproc_executor.py", line 79, in collective_rpc
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     result = run_method(self.driver_worker, method, args, kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/serial_utils.py", line 461, in run_method
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     return func(*args, **kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/worker_base.py", line 365, in execute_model
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     return self.worker.execute_model(scheduler_output)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     return func(*args, **kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 630, in execute_model
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     output = self.model_runner.execute_model(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     return func(*args, **kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3354, in execute_model
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     ) = self._determine_batch_execution_and_padding(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3109, in _determine_batch_execution_and_padding
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     coordinate_batch_across_dp(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/dp_utils.py", line 230, in coordinate_batch_across_dp
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     _synchronize_dp_ranks(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/dp_utils.py", line 134, in _synchronize_dp_ranks
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     tensor = _run_ar(
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]              ^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/dp_utils.py", line 55, in _run_ar
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     dist.all_reduce(tensor, group=group)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     return func(*args, **kwargs)
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]            ^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py", line 2942, in all_reduce
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948]     work.wait()
[0;36m(EngineCore_DP1 pid=305)[0;0m ERROR 02-08 08:24:19 [core.py:948] RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:547] Connection closed by peer [2803:6086:908:94cf:939f:12c4:400:0]:39565
0208 08:24:21.813328  INFO tokio-runtime-worker ThreadId(110) request_info:handler: vllm_router_rs::routers::http::vllm_pd_router:1251: Two-stage processing failed: Network error: Decode request failed to https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081/v1/chat/completions: error sending request for url (https://[2803:6086:908:94e4:48a:12c4:400:0]:8081/v1/chat/completions) request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:21.814192 ERROR tokio-runtime-worker ThreadId(110) request_info:handler: common::thrift_predictor_service:173: HTTP request failed: Router error 500 Internal Server Error: Request processing failed: Network error: Decode request failed to https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081/v1/chat/completions: error sending request for url (https://[2803:6086:908:94e4:48a:12c4:400:0]:8081/v1/chat/completions) request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
0208 08:24:21.814210 ERROR tokio-runtime-worker ThreadId(110) request_info:handler: llm_predictor_gpu_services:1007: method: "InferencePlatformSp_LlmPredictorGpu.predict", exception: ex2(ServiceException { message: "HTTP request failed: Router error 500 Internal Server Error: Request processing failed: Network error: Decode request failed to https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081/v1/chat/completions: error sending request for url (https://[2803:6086:908:94e4:48a:12c4:400:0]:8081/v1/chat/completions)" }), error: "ServiceException { message: \"HTTP request failed: Router error 500 Internal Server Error: Request processing failed: Network error: Decode request failed to https://[2803:6086:0908:94e4:048a:12c4:0400:0000]:8081/v1/chat/completions: error sending request for url (https://[2803:6086:908:94e4:48a:12c4:400:0]:8081/v1/chat/completions)\" }" request_id: "06b00000000000c7" method: "InferencePlatformSp_LlmPredictorGpu.predict"
I0208 08:24:21.821740    2960 ucx_backend.cpp:594] shared thread 0x65861f80{engine: 0x4c7c9290, worker_ids: [0]} exiting
I0208 08:24:21.821859    2961 ucx_backend.cpp:885] dedicated thread 0x65862140{engine: 0x4c7c9290, worker_ids: [1]} exiting
I0208 08:24:21.821866    2963 ucx_backend.cpp:885] dedicated thread 0x65a607b0{engine: 0x4c7c9290, worker_ids: [3]} exiting
I0208 08:24:21.821870    2962 ucx_backend.cpp:885] dedicated thread 0x658622e0{engine: 0x4c7c9290, worker_ids: [2]} exiting
I0208 08:24:21.821867    2964 ucx_backend.cpp:885] dedicated thread 0x65a60fc0{engine: 0x4c7c9290, worker_ids: [4]} exiting
[rank1]:[W208 08:24:22.361436855 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
I0208 08:24:23.146702    2680 ucx_backend.cpp:594] shared thread 0x5d526060{engine: 0x674c6550, worker_ids: [0]} exiting
I0208 08:24:23.146827    2683 ucx_backend.cpp:885] dedicated thread 0x5d5263c0{engine: 0x674c6550, worker_ids: [2]} exiting
I0208 08:24:23.146836    2682 ucx_backend.cpp:885] dedicated thread 0x5d526220{engine: 0x674c6550, worker_ids: [1]} exiting
I0208 08:24:23.146831    2685 ucx_backend.cpp:885] dedicated thread 0x5d526560{engine: 0x674c6550, worker_ids: [3]} exiting
I0208 08:24:23.146829    2687 ucx_backend.cpp:885] dedicated thread 0x5d526d70{engine: 0x674c6550, worker_ids: [4]} exiting
[rank0]:[W208 08:24:23.346176061 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
V0208 08:24:23.513998 12730 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 108, in cmd
    run_multi_api_server(args)
  File "/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/cli/serve.py", line 282, in run_multi_api_server
    wait_for_completion_or_failure(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/utils.py", line 277, in wait_for_completion_or_failure
    raise RuntimeError(
RuntimeError: Process ApiServer_0 (PID: 306) died with exit code None
0208 08:24:29.677650 ERROR tokio-runtime-worker ThreadId(110) lib::podman_manager:164: Podman process (PID 1942) exited with code 1! Sending SIGTERM to trigger sidecar shutdown.
0208 08:24:29.678164  INFO tokio-runtime-worker ThreadId(110) sidecar:629: Received shutdown signal, stopping service...
I0208 08:24:29.678864   614 ServiceFrameworkLight.cpp:1321] Calling stop() on vllm_sidecar
0208 08:24:29.694235  INFO tokio-runtime-worker ThreadId(110) sidecar:635: Stopping metrics collection thread...
0208 08:24:29.694255  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:147: Stopping metrics collection thread...
0208 08:24:29.694282  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:190: Metrics thread received shutdown signal, stopping
0208 08:24:29.694286  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:197: Metrics collection thread stopped
0208 08:24:29.694763  INFO tokio-runtime-worker ThreadId(110) lib::metrics_thread:156: Metrics collection thread stopped
0208 08:24:29.694770  INFO tokio-runtime-worker ThreadId(110) router:786: Stopping worker refresh task...
0208 08:24:29.694780  INFO tokio-runtime-worker ThreadId(110) router:790: Worker refresh task stopped
0208 08:24:29.694782  INFO tokio-runtime-worker ThreadId(110) sidecar:656: Stopping vLLM container...
0208 08:24:29.694784  INFO tokio-runtime-worker ThreadId(110) lib::podman_manager:196: Stopping vLLM container...
0208 08:24:29.694787  INFO tokio-runtime-worker ThreadId(110) lib::podman_manager:201: Sending SIGTERM to podman process (PID: 1942)
0208 08:24:29.694797  WARN tokio-runtime-worker ThreadId(110) lib::podman_manager:213: Error waiting for podman process: No child processes (os error 10)
0208 08:24:29.697289  INFO tokio-runtime-worker ThreadId(140) vllm_router_rs::routers::http::pd_router:662: Prefill drain coordinator shutting down
I0208 08:24:29.818896   138 GlobalHypertraceDumper.cpp:71] Shutdown requested, destroying GlobalHypertraceDumper
I0208 08:24:29.899567   138 DownloadClient.cpp:43] Download Client is stopping
I0208 08:24:29.899601   138 ServiceFrameworkLight.cpp:1321] Calling stop() on PeerAdmin
I0208 08:24:30.921041   138 PeerService.cpp:402] PeerService shutdown begins, client manifold:oss_models:oss_models-key:HEDWIG_MANIFOLD_APP_ID_FAIR_LLM_LARGE_MODEL endpoint hedwig.download.peer.gen_ai:2803:6086:0908:94e4:048a:12c4:0400:0000:48049
I0208 08:24:30.921702   138 PeerService.cpp:410] PeerService shutdown ends
I0208 08:24:32.627093   138 PeerCounters.h:367] Peer counters destroyed for client id: manifold:oss_models:oss_models-key:HEDWIG_MANIFOLD_APP_ID_FAIR_LLM_LARGE_MODEL
I0208 08:24:32.627209   138 PeerConfig.cpp:122] Peer configs destroyed for client id: manifold:oss_models:oss_models-key:HEDWIG_MANIFOLD_APP_ID_FAIR_LLM_LARGE_MODEL
I0208 08:24:32.686156   138 ScubaLogger.cpp:557] S451887 just destroyed scubaLogger, whose use_count is now 2
E0208 08:24:33.081369   156 ForkUserCommandExec.cpp:121] ChildEvent loop: child's fd has been closed
I0208 08:24:33.081835   123 GuardMain.cpp:210] Application exited normally with status 0
V0208 08:24:33.514187   156 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
V0208 08:24:43.514401   156 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/act]
I0208 08:24:45.331606   156 QuorumStateWatcher.cpp:46] An event occurred while watching existing quorum state node: EventType: CHANGED
SessionState: CONNECTED
Path: /gang_coordinator/QuorumState/tsp_rcd/g_c39fada5-ab72-4fdf-aacd-d1a21624211b
V0208 08:24:45.331692   156 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [quorum_state_watcher/act]
I0208 08:24:45.342578   156 QuorumStateWatcher.cpp:37] Quorum state read from /gang_coordinator/QuorumState/tsp_rcd/g_c39fada5-ab72-4fdf-aacd-d1a21624211b, 57 byte(s)
V0208 08:24:45.342604   156 [GlobalCPUThread] ObservableState.h:35] Observable [quorum_state] is notifying [quorum_joiner:trigger]
V0208 08:24:45.342613   156 [GlobalCPUThread] ChangeNotifier.cpp:16] Triggering change notifier [quorum_joiner:trigger]
V0208 08:24:45.342624   156 [GlobalCPUThread] ChangeNotifier.cpp:40] Wait on change notifier [quorum_joiner:trigger] completed
V0208 08:24:45.342647   156 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [quorum_joiner/act]
I0208 08:24:45.342668   156 [GlobalCPUThread] QuorumJoiner.cpp:64] Running quorum joiner ...
I0208 08:24:45.342688   156 [GlobalCPUThread] QuorumJoiner.cpp:87] Quorum has been reset, stage: JOINED, current run id: 1, new run id: 2
I0208 08:24:45.363681   156 [GlobalCPUThread] QuorumJoiner.cpp:225] Unpublished self as a potential quorum member, runId=1, id=5, path=/gang_coordinator/QuorumParticipation/tsp_rcd/g_c39fada5-ab72-4fdf-aacd-d1a21624211b/m_1_0000000005
I0208 08:24:45.363708   156 [GlobalCPUThread] SimpleQuorumParticipation.cpp:81] Left quorum with run id 1, raising SIGTERM
I0208 08:24:45.363729   156 [GlobalCPUThread] MetadataPublisher.cpp:49] Publishing quorum metadata: [guard_in_quorum=0, guard_run_id=1, guard_rank=]
V0208 08:24:45.363838   156 [GlobalCPUThread] ChangeNotifier.cpp:25] Awaiting change notifier [quorum_joiner:trigger]
I0208 08:24:45.363875   211 UserCommandManager.cpp:65] Command is already stopping
I0208 08:24:48.082062   123 GuardMain.cpp:264] Stop reason: 2
I0208 08:24:48.082095   123 [guard_main] GuardStartup.cpp:187] Guard is stopping
I0208 08:24:48.082141   123 [guard_main] GuardStartup.cpp:51] Starting failsafe termination thread
I0208 08:24:48.082230   123 [guard_main] GuardStartup.cpp:192] Stopping actors...
I0208 08:24:48.082304   156 [GlobalCPUThread] ActorRunner.cpp:95] Stopping actor [guard_invariant_reporter]
V0208 08:24:48.083914 12732 [GlobalCPUThread] ActorRunner.cpp:121] Actor loop [guard_invariant_reporter] finished by cancellation
V0208 08:24:48.083940 12732 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [guard_invariant_reporter/teardown]
I0208 08:24:48.084025 12732 [GlobalCPUThread] ActorRunner.cpp:101] Actor [guard_invariant_reporter] has been successfully stopped.
I0208 08:24:48.084045 12732 [GlobalCPUThread] ActorRunner.cpp:95] Stopping actor [quorum_joiner]
V0208 08:24:48.084068 12732 [GlobalCPUThread] ChangeNotifier.cpp:36] Cancellation requested for change notifier [quorum_joiner:trigger]
V0208 08:24:48.084085 12732 [GlobalCPUThread] ActorRunner.cpp:121] Actor loop [quorum_joiner] finished by cancellation
V0208 08:24:48.084093 12732 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [quorum_joiner/teardown]
I0208 08:24:48.084120 12732 [GlobalCPUThread] ActorRunner.cpp:101] Actor [quorum_joiner] has been successfully stopped.
I0208 08:24:48.084138 12732 [GlobalCPUThread] ActorRunner.cpp:95] Stopping actor [quorum_state_watcher]
I0208 08:24:48.084198 12732 [GlobalCPUThread] ActorRunner.cpp:189] Call to [quorum_state_watcher/act] was canceled
I0208 08:24:48.084239 12732 [GlobalCPUThread] ActorRunner.cpp:127] Actor loop [quorum_state_watcher] finished normally
V0208 08:24:48.084259 12732 [GlobalCPUThread] ActorRunner.cpp:185] Invoking [quorum_state_watcher/teardown]
I0208 08:24:48.084294 12732 [GlobalCPUThread] ActorRunner.cpp:101] Actor [quorum_state_watcher] has been successfully stopped.
I0208 08:24:48.084316 12732 ServiceFrameworkLight.cpp:1321] Calling stop() on Dummy
I0208 08:24:48.085105 12732 [GlobalCPUThread] ServerModule.cpp:60] ContextProp Thrift Server Module removed
I0208 08:24:48.085131 12732 [GlobalCPUThread] ServiceTraceModule.cpp:69] ServiceTrace Module removed
I0208 08:24:48.085152 12732 [GlobalCPUThread] BackTraceModule.cpp:71] Backtrace Module removed
I0208 08:24:48.085171 12732 [GlobalCPUThread] ClientSideSamplingModule.cpp:57] ClientSideSamplingModule Module removed
I0208 08:24:49.116774   123 [guard_main] GuardStartup.cpp:204] Closing connection...
I0208 08:24:49.129622   123 [guard_main] Guard.cpp:93] Destroying guard
I0208 08:24:49.130443   123 GuardMain.cpp:293] CPU executor counters: numThreads=144, numActiveThreads=6, numPendingTasks=0
I0208 08:24:49.130468   123 GuardMain.cpp:298] Overriding exit code 0 with 89 due to process stop reason 2
I0208 08:24:49.130473   123 GuardMain.cpp:304] Gracefully exiting with code 89, bye bye.
I0208 08:24:49.201582   123 TUNStreamBasedTierUpdatesObserver.cpp:233] Destroying TUNStreamBasedTierUpdatesObserver
I0208 08:24:49 Stopping twtask-main.service with Service Result: [exit-code] Exit Code: [exited] Exit Status: [89]
I0208 08:24:49.285748 12910 ReportServiceExit.cpp:177] twtask-main.service successfully reported value 89 to file /var/twexitcode
I0208 08:24:49 12917 /usr/libexec/tw/halt:56 Unknown stop context /run/tw_exit_reason.9387604274597542263
I0208 08:24:49 Stopping twtask-pre-run-step-0.service with Service Result: [success] Exit Code: [exited] Exit Status: [0] Step Name: twtask-pre-run-step-0.service
[twinfra] I0208 08:24:49.000000 1990486 LxcAgentHelper.cpp:367] Exit code obtained from container: 89
[twinfra] I0208 08:24:49.000000 1990486 LxcAgentHelper.cpp:465] Your task has exited with non-zero exit code 89. See https://fburl.com/twtask_troubleshoot for additional details.
[twinfra] I0208 08:24:49.000000 1990486 AgentHelperMain.cpp:229] Container shutdown complete
