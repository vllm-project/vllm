# Attention Backend Feature Support

This document is auto-generated by `tools/pre_commit/generate_attention_backend_docs.py`.
It shows the feature support for each registered attention backend
based on the checks in `AttentionBackend.validate_configuration()`.

**Do not edit this file manually.** Run the following command to
regenerate it:

```bash
python tools/pre_commit/generate_attention_backend_docs.py
```

## Legend

| Column | Description |
|--------|-------------|
| **Dtypes** | Supported model data types (fp16, bf16, fp32) |
| **KV Cache Dtypes** | Supported KV cache data types (auto, fp8, fp8_e4m3, etc.) |
| **Block Sizes** | Supported KV cache block sizes (×N means multiples of N) |
| **Head Sizes** | Supported attention head sizes |
| **MLA** | Multi-head Latent Attention support (for DeepSeek-style models) |
| **Sink** | Attention sink support (for StreamingLLM) |
| **Sparse** | Sparse attention support |
| **MM Prefix** | Multimodal prefix full attention support |
| **Attention Types** | Supported attention patterns (Decoder, Encoder, Enc-Dec) |
| **Compute Cap.** | Required CUDA compute capability |

**Symbols:** ✓ = Supported, ✗ = Not supported


## Standard Attention Backends

| Backend | Dtypes | KV Cache Dtypes | Block Sizes | Head Sizes | MLA | Sink | Sparse | MM Prefix | Attention Types | Compute Cap. |
|---------|--------|-----------------|-------------|------------|-----|------|--------|-----------|-----------------|--------------|
| CPU_ATTN | fp16, bf16, fp32 | auto | ×1 | 32, 64, 80, 96, 112, 128, 160, 192, 224, 256 | ✗ | ✗ | ✗ | ✗ | All | Any |
| FLASHINFER | fp16, bf16 | auto, fp8, fp8_e4m3, fp8_e5m2 | 16, 32, 64 | 64, 128, 256 | ✗ | ✗ | ✗ | ✗ | Decoder | ≥7.5 |
| FLASH_ATTN | fp16, bf16 | auto | ×16 | Any | ✗ | ✗ | ✗ | ✗ | All | ≥8.0 |
| FLASH_ATTN_DIFFKV | fp16, bf16 | auto | ×1 | Any | ✗ | ✗ | ✗ | ✗ | Decoder | Any |
| FLEX_ATTENTION | fp16, bf16, fp32 | auto | ×1 | Any | ✗ | ✗ | ✗ | ✓ | Decoder, Encoder Only | Any |
| ROCM_AITER_FA | fp16, bf16 | auto | ×16 | 64, 128, 256 | ✗ | ✗ | ✗ | ✗ | Decoder | Any |
| ROCM_AITER_UNIFIED_ATTN | fp16, bf16 | auto | ×1 | Any | ✗ | ✗ | ✗ | ✗ | Decoder | Any |
| ROCM_ATTN | fp16, bf16, fp32 | auto | 16, 32, 544 | 32, 64, 96, 128, 160, 192, 224, 256 | ✗ | ✗ | ✗ | ✗ | Decoder | Any |
| TREE_ATTN | fp16, bf16 | auto | ×16 | 32, 64, 96, 128, 160, 192, 224, 256 | ✗ | ✗ | ✗ | ✗ | Decoder | Any |
| TRITON_ATTN | fp16, bf16, fp32 | auto, fp8, fp8_e4m3, fp8_e5m2 | ×16 | Any | ✗ | ✓ | ✗ | ✓ | All | Any |

## MLA (Multi-head Latent Attention) Backends

| Backend | Dtypes | KV Cache Dtypes | Block Sizes | Head Sizes | MLA | Sink | Sparse | MM Prefix | Attention Types | Compute Cap. |
|---------|--------|-----------------|-------------|------------|-----|------|--------|-----------|-----------------|--------------|
| CUTLASS_MLA | fp16, bf16 | auto, fp8, fp8_e4m3 | 128 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
| FLASHINFER_MLA | fp16, bf16 | auto, fp8, fp8_e4m3 | 32, 64 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
| FLASHMLA | fp16, bf16 | auto, fp8, fp8_e4m3 | 64 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | 9.x-10.x |
| FLASHMLA_SPARSE | bf16 | auto, fp8_ds_mla | 64 | 576 | ✓ | ✗ | ✓ | ✗ | Decoder | 9.x-10.x |
| FLASH_ATTN_MLA | fp16, bf16 | auto | ×16 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
| ROCM_AITER_MLA | fp16, bf16 | auto | 1 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
| ROCM_AITER_MLA_SPARSE | fp16, bf16 | auto | ×1 | 576 | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
| ROCM_AITER_TRITON_MLA | fp16, bf16 | auto | ×1 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
| TRITON_MLA | fp16, bf16 | auto | ×1 | Any | ✓ | ✗ | ✗ | ✗ | Decoder | Any |
