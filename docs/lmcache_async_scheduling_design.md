# LMCache å¼‚æ­¥è°ƒåº¦é€‚é…è®¾è®¡æ–‡æ¡£

## 1. èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 å½“å‰çŠ¶å†µ
- **Async Scheduling**: vLLM v1 æ”¯æŒå¼‚æ­¥è°ƒåº¦ï¼ˆAsyncSchedulerï¼‰ï¼Œå…è®¸åœ¨ token å®é™…ç”Ÿæˆä¹‹å‰é¢„å…ˆè°ƒåº¦å’Œåˆ†é… KV cache ç©ºé—´
- **LMCache é›†æˆ**: LMCache é€šè¿‡ `LMCacheConnectorV1` ä¸ vLLM é›†æˆï¼Œæ”¯æŒ KV cache çš„å¤–éƒ¨å­˜å‚¨ã€å…±äº«å’Œä¼ è¾“
- **å…¼å®¹æ€§é—®é¢˜**: ç›®å‰ LMCache ä¸»è¦é’ˆå¯¹åŒæ­¥è°ƒåº¦ï¼ˆSchedulerï¼‰è®¾è®¡ï¼Œå°šæœªå®Œå…¨é€‚é…å¼‚æ­¥è°ƒåº¦æœºåˆ¶

### 1.2 è®¾è®¡ç›®æ ‡
1. ä½¿ LMCache èƒ½å¤Ÿæ­£ç¡®å¤„ç†å¼‚æ­¥è°ƒåº¦ä¸­çš„ KV cache æ“ä½œ
2. ç¡®ä¿ `num_output_placeholders` æœºåˆ¶ä¸ LMCache çš„ç¼“å­˜é€»è¾‘å…¼å®¹
3. ä¿æŒå‘åå…¼å®¹æ€§ï¼Œä¸å½±å“ç°æœ‰çš„åŒæ­¥è°ƒåº¦åŠŸèƒ½
4. ä¼˜åŒ–æ€§èƒ½ï¼Œé¿å…ä¸å¿…è¦çš„é‡å¤ç¼“å­˜æ“ä½œ

## 2. æ ¸å¿ƒæ¦‚å¿µåˆ†æ

### 2.1 AsyncScheduler çš„å…³é”®æœºåˆ¶

```
Request Timeline in AsyncScheduler:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time    â”‚ Action                â”‚ num_computed â”‚ placeholdersâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ t0      â”‚ Schedule (prefill)    â”‚ 100          â”‚ 0           â”‚
â”‚ t1      â”‚ Schedule (decode)     â”‚ 101          â”‚ 1           â”‚  â† placeholder++
â”‚ t2      â”‚ Token generated       â”‚ 101          â”‚ 0           â”‚  â† placeholder--
â”‚         â”‚ cache_blocks(100)     â”‚              â”‚             â”‚  â† Cache real tokens
â”‚ t3      â”‚ Schedule (decode)     â”‚ 102          â”‚ 1           â”‚
â”‚ t4      â”‚ Token generated       â”‚ 102          â”‚ 0           â”‚
â”‚         â”‚ cache_blocks(101)     â”‚              â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‚¹**:
- `num_output_placeholders`: è¿½è¸ªå·²è°ƒåº¦ä½†æœªå®é™…ç”Ÿæˆçš„ token æ•°é‡
- **å¢åŠ æ—¶æœº**: åœ¨ `_update_after_schedule()` ä¸­ï¼Œå½“è¯·æ±‚å°†ç”Ÿæˆæ–° token æ—¶
- **å‡å°‘æ—¶æœº**: åœ¨ `_update_request_with_output()` ä¸­ï¼Œå½“ token å®é™…ç”Ÿæˆæ—¶
- **ç¼“å­˜æ—¶æœº**: åœ¨ `_update_request_with_output()` ä¸­ï¼Œä½¿ç”¨ `num_computed_tokens - num_output_placeholders` ä½œä¸ºå‚æ•°è°ƒç”¨ `cache_blocks()`

### 2.2 LMCache çš„ç¼“å­˜æœºåˆ¶

LMCache åœ¨ä»¥ä¸‹ä½ç½®è¿›è¡Œ KV cache æ“ä½œï¼š

1. **Worker ä¾§** (forward pass æœŸé—´):
   - `start_load_kv()`: å¼€å§‹å¼‚æ­¥åŠ è½½ KV cache
   - `wait_for_layer_load()`: ç­‰å¾…æŸå±‚ KV cache åŠ è½½å®Œæˆ
   - `save_kv_layer()`: å¼‚æ­¥ä¿å­˜æŸå±‚ KV cache
   - `wait_for_save()`: ç­‰å¾…æ‰€æœ‰ä¿å­˜æ“ä½œå®Œæˆ

2. **Scheduler ä¾§** (è°ƒåº¦æœŸé—´):
   - `get_num_new_matched_tokens()`: è·å–å¯ä»å¤–éƒ¨ç¼“å­˜åŠ è½½çš„ token æ•°é‡
   - `update_state_after_alloc()`: åˆ†é… blocks åæ›´æ–°çŠ¶æ€
   - `build_connector_meta()`: æ„å»ºè¿æ¥å™¨å…ƒæ•°æ®
   - `request_finished()`: è¯·æ±‚å®Œæˆæ—¶çš„æ¸…ç†

### 2.3 å…¼å®¹æ€§é—®é¢˜åˆ†æ

#### é—®é¢˜ 1: ç¼“å­˜è¾¹ç•Œä¸ä¸€è‡´
åœ¨ AsyncScheduler ä¸­:
```python
# AsyncScheduler._update_request_with_output (line 43-46)
if status_before_update == RequestStatus.RUNNING:
    self.kv_cache_manager.cache_blocks(
        request,
        request.num_computed_tokens - request.num_output_placeholders)
```

- ç¼“å­˜çš„æ˜¯ "å·²çœŸæ­£è®¡ç®—å®Œæˆçš„ tokens"ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ `num_computed_tokens`
- LMCache éœ€è¦çŸ¥é“å®é™…åº”è¯¥ç¼“å­˜åˆ°å“ªä¸ª token ä½ç½®

#### é—®é¢˜ 2: Prefix Caching ä¸ Placeholders
- AsyncScheduler ä¸­ `num_computed_tokens` åŒ…å«äº† placeholder tokens
- LMCache åœ¨æŸ¥è¯¢ç¼“å­˜æ—¶éœ€è¦åŒºåˆ†"çœŸæ­£è®¡ç®—çš„"å’Œ"é¢„åˆ†é…çš„"tokens

#### é—®é¢˜ 3: å…ƒæ•°æ®ä¼ é€’
- LMCache çš„ `build_connector_meta()` éœ€è¦çŸ¥é“æ¯ä¸ªè¯·æ±‚çš„å®é™…ç¼“å­˜ä½ç½®
- å½“å‰æ¥å£å¯èƒ½æ²¡æœ‰ä¼ é€’ `num_output_placeholders` ä¿¡æ¯

## 3. è®¾è®¡æ–¹æ¡ˆ

### 3.1 æ–¹æ¡ˆæ¦‚è§ˆ

**æ ¸å¿ƒæ€è·¯**: è®© LMCache æ„ŸçŸ¥ async scheduling çš„ placeholder æœºåˆ¶ï¼Œåœ¨æ‰€æœ‰ç›¸å…³æ“ä½œä¸­ä½¿ç”¨"å®é™…å·²è®¡ç®— tokens æ•°"è€Œä¸æ˜¯"è°ƒåº¦ tokens æ•°"ã€‚

### 3.2 ä¿®æ”¹ç‚¹æ¸…å•

#### ä¿®æ”¹ 1: æ‰©å±• Request å¯¹è±¡ä¿¡æ¯ä¼ é€’

åœ¨ LMCache connector çš„æ‰€æœ‰æ¥å£ä¸­ï¼Œéœ€è¦èƒ½å¤Ÿè·å–åˆ°è¯·æ±‚çš„çœŸå®è®¡ç®—çŠ¶æ€ï¼š

```python
def get_real_computed_tokens(request: Request) -> int:
    """è·å–è¯·æ±‚å®é™…å·²è®¡ç®—çš„ token æ•°é‡ï¼ˆæ’é™¤ placeholdersï¼‰"""
    return request.num_computed_tokens - request.num_output_placeholders
```

#### ä¿®æ”¹ 2: æ›´æ–° `cache_blocks()` è°ƒç”¨é€»è¾‘

åœ¨ `AsyncScheduler._update_request_with_output()` ä¸­:

```python
# å½“å‰å®ç° (line 43-46)
if status_before_update == RequestStatus.RUNNING:
    self.kv_cache_manager.cache_blocks(
        request,
        request.num_computed_tokens - request.num_output_placeholders)

# å¦‚æœä½¿ç”¨ LMCacheï¼Œéœ€è¦ç¡®ä¿ LMCache ä¹ŸçŸ¥é“è¿™ä¸ªè¾¹ç•Œ
# LMCache connector åº”è¯¥åœ¨ Worker ä¾§çš„ save_kv_layer() ä¸­ä½¿ç”¨è¿™ä¸ªä¿¡æ¯
```

#### ä¿®æ”¹ 3: LMCache Connector é€‚é…

##### 3.2.1 Scheduler ä¾§ä¿®æ”¹

**æ–‡ä»¶**: `vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py`

```python
# æ–°å¢æ–¹æ³•ï¼šè·å–è¯·æ±‚çš„å®é™…è®¡ç®— token æ•°
def get_real_computed_tokens(self, request: Request) -> int:
    """
    è·å–è¯·æ±‚å®é™…å·²è®¡ç®—çš„ token æ•°é‡ï¼ˆæ’é™¤ async scheduling placeholdersï¼‰
    
    è¿™ä¸ªæ–¹æ³•åœ¨ async scheduling æ¨¡å¼ä¸‹è¿”å›ï¼š
        num_computed_tokens - num_output_placeholders
    åœ¨åŒæ­¥æ¨¡å¼ä¸‹è¿”å›ï¼š
        num_computed_tokens
    """
    return self._lmcache_engine.get_real_computed_tokens(request)

# ä¿®æ”¹ build_connector_meta() 
def build_connector_meta(self, scheduler_output: SchedulerOutput) -> KVConnectorMetadata:
    """
    æ„å»ºè¿æ¥å™¨å…ƒæ•°æ®ï¼Œéœ€è¦ä¼ é€’æ¯ä¸ªè¯·æ±‚çš„å®é™…è®¡ç®—ä½ç½®
    """
    # éœ€è¦ä¸ºæ¯ä¸ªè¯·æ±‚æ·»åŠ  real_computed_tokens ä¿¡æ¯
    return self._lmcache_engine.build_connector_meta(
        scheduler_output, 
        include_placeholder_info=True)
```

##### 3.2.2 Worker ä¾§ä¿®æ”¹

**åœ¨ save_kv_layer() æ—¶ä½¿ç”¨æ­£ç¡®çš„è¾¹ç•Œ**:

```python
def save_kv_layer(self, layer_name: str, kv_layer: torch.Tensor,
                  attn_metadata: "AttentionMetadata", **kwargs) -> None:
    """
    ä¿å­˜ KV cache æ—¶ï¼Œåªä¿å­˜å®é™…è®¡ç®—çš„éƒ¨åˆ†ï¼Œä¸åŒ…æ‹¬ placeholder tokens
    """
    # ä» attn_metadata æˆ– kwargs ä¸­è·å–æ¯ä¸ªè¯·æ±‚çš„å®é™…è®¡ç®—è¾¹ç•Œ
    real_computed_info = kwargs.get('real_computed_tokens', {})
    
    self._lmcache_engine.save_kv_layer(
        layer_name, kv_layer, attn_metadata,
        real_computed_tokens=real_computed_info,
        **kwargs)
```

##### 3.2.3 å…ƒæ•°æ®ä¼ é€’é“¾

```
AsyncScheduler._update_request_with_output()
    â†“ è®¡ç®— real_computed = num_computed_tokens - num_output_placeholders
    â†“
KVCacheManager.cache_blocks(request, real_computed)
    â†“
Scheduler â†’ build_connector_meta() 
    â†“ å°† real_computed ä¿¡æ¯ç¼–ç åˆ° metadata
    â†“
Worker â†’ start_load_kv() / save_kv_layer()
    â†“ ä» metadata è·å– real_computed ä¿¡æ¯
    â†“
LMCache â†’ ä½¿ç”¨ real_computed ä½œä¸ºç¼“å­˜è¾¹ç•Œ
```

### 3.3 è¯¦ç»†æ¥å£è®¾è®¡

#### 3.3.1 ä¿®æ”¹ KVConnectorMetadata

```python
@dataclass
class KVConnectorMetadata:
    """
    åŒ…å« KV transfer æ‰€éœ€çš„æ‰€æœ‰å…ƒæ•°æ®
    """
    # ... ç°æœ‰å­—æ®µ ...
    
    # æ–°å¢ï¼šæ¯ä¸ªè¯·æ±‚çš„å®é™…è®¡ç®— token æ•°ï¼ˆç”¨äº async schedulingï¼‰
    real_computed_tokens: dict[str, int] = field(default_factory=dict)
    """
    request_id -> å®é™…å·²è®¡ç®—çš„ token æ•°ï¼ˆæ’é™¤ placeholdersï¼‰
    åœ¨ async scheduling æ¨¡å¼ä¸‹ï¼Œè¿™ä¸ªå€¼ <= num_computed_tokens
    åœ¨åŒæ­¥æ¨¡å¼ä¸‹ï¼Œè¿™ä¸ªå€¼ == num_computed_tokens
    """
```

#### 3.3.2 ä¿®æ”¹ LMCacheConnectorV1Impl

**åœ¨ lmcache ä¾§çš„é€‚é…å™¨å®ç°** (å‡è®¾åœ¨ `lmcache` ä»“åº“ä¸­):

```python
class LMCacheConnectorV1Impl:
    def __init__(self, vllm_config, role, connector):
        self.vllm_config = vllm_config
        self.role = role
        self.connector = connector
        
        # æ£€æµ‹æ˜¯å¦å¯ç”¨ async scheduling
        self.is_async_scheduling = self._detect_async_scheduling(vllm_config)
        
        # è¿½è¸ªæ¯ä¸ªè¯·æ±‚çš„å®é™…è®¡ç®—è¾¹ç•Œ
        self.request_real_computed: dict[str, int] = {}
    
    def _detect_async_scheduling(self, vllm_config) -> bool:
        """æ£€æµ‹æ˜¯å¦å¯ç”¨ async scheduling"""
        # å¯ä»¥é€šè¿‡ config æˆ–ç¯å¢ƒå˜é‡æ£€æµ‹
        return getattr(vllm_config.scheduler_config, 
                      'use_async_scheduling', False)
    
    def build_connector_meta(self, scheduler_output, 
                           include_placeholder_info=False):
        """æ„å»ºå…ƒæ•°æ®ï¼ŒåŒ…å« async scheduling ä¿¡æ¯"""
        meta = self._build_base_meta(scheduler_output)
        
        if include_placeholder_info and self.is_async_scheduling:
            # ä» scheduler_output æå–æ¯ä¸ªè¯·æ±‚çš„å®é™…è®¡ç®—ä½ç½®
            meta.real_computed_tokens = {}
            for req_id in scheduler_output.num_scheduled_tokens:
                request = self.connector.get_request(req_id)
                if request:
                    meta.real_computed_tokens[req_id] = (
                        request.num_computed_tokens - 
                        request.num_output_placeholders
                    )
        
        return meta
    
    def save_kv_layer(self, layer_name, kv_layer, attn_metadata,
                     real_computed_tokens=None, **kwargs):
        """
        ä¿å­˜ KV cacheï¼Œè€ƒè™‘ async scheduling çš„è¾¹ç•Œ
        
        Args:
            real_computed_tokens: dict[request_id -> actual_computed_position]
                                 ç”¨äº async scheduling
        """
        if self.is_async_scheduling and real_computed_tokens:
            # ä½¿ç”¨ real_computed_tokens ä½œä¸ºä¿å­˜è¾¹ç•Œ
            self._save_with_boundaries(layer_name, kv_layer, 
                                      attn_metadata, 
                                      real_computed_tokens)
        else:
            # åŒæ­¥æ¨¡å¼ï¼Œä½¿ç”¨å…¨éƒ¨ computed tokens
            self._save_all_computed(layer_name, kv_layer, attn_metadata)
```

### 3.4 æ‰§è¡Œæµç¨‹å›¾

#### 3.4.1 Async Scheduling + LMCache çš„å®Œæ•´æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     è°ƒåº¦é˜¶æ®µ (Scheduler)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. AsyncScheduler.schedule()                                    â”‚
â”‚    - è°ƒåº¦è¯·æ±‚ï¼Œåˆ†é… tokens                                        â”‚
â”‚    - è°ƒç”¨ allocate_slots() åˆ†é… KV cache blocks                  â”‚
â”‚    - ç”Ÿæˆ SchedulerOutput                                        â”‚
â”‚                                                                  â”‚
â”‚ 2. AsyncScheduler._update_after_schedule()                      â”‚
â”‚    - æ›´æ–° num_computed_tokens += num_scheduled_tokens            â”‚
â”‚    - å¦‚æœå°†ç”Ÿæˆæ–° token: num_output_placeholders += 1            â”‚
â”‚                                                                  â”‚
â”‚ 3. LMCache: build_connector_meta()                              â”‚
â”‚    - ä¸ºæ¯ä¸ªè¯·æ±‚è®¡ç®—: real_computed = computed - placeholders     â”‚
â”‚    - å°† real_computed_tokens ç¼–ç åˆ° metadata                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ‰§è¡Œé˜¶æ®µ (Worker)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. ModelRunner.execute_model()                                  â”‚
â”‚    - ä» metadata è·å– real_computed_tokens                       â”‚
â”‚                                                                  â”‚
â”‚ 5. LMCache: start_load_kv()                                     â”‚
â”‚    - ä½¿ç”¨ real_computed ä½œä¸ºå·²ç¼“å­˜è¾¹ç•Œ                            â”‚
â”‚    - å¼‚æ­¥åŠ è½½éœ€è¦çš„ KV cache                                     â”‚
â”‚                                                                  â”‚
â”‚ 6. Forward pass                                                 â”‚
â”‚    - æ¯å±‚è°ƒç”¨ wait_for_layer_load() ç¡®ä¿æ•°æ®å°±ç»ª                  â”‚
â”‚    - è®¡ç®— attention                                             â”‚
â”‚    - æ¯å±‚è°ƒç”¨ save_kv_layer() ä¿å­˜æ–°è®¡ç®—çš„ KV                     â”‚
â”‚      * åªä¿å­˜ [old_real_computed, new_real_computed) èŒƒå›´         â”‚
â”‚      * ä¸ä¿å­˜ placeholder å¯¹åº”çš„ KV                               â”‚
â”‚                                                                  â”‚
â”‚ 7. LMCache: wait_for_save()                                     â”‚
â”‚    - ç­‰å¾…æ‰€æœ‰å¼‚æ­¥ä¿å­˜å®Œæˆ                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     æ›´æ–°é˜¶æ®µ (Scheduler)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 8. AsyncScheduler.update_from_output()                          â”‚
â”‚    - æ¥æ”¶ ModelRunnerOutput (åŒ…å«ç”Ÿæˆçš„ tokens)                   â”‚
â”‚                                                                  â”‚
â”‚ 9. AsyncScheduler._update_request_with_output()                 â”‚
â”‚    - append_output_token_ids()                                  â”‚
â”‚    - num_output_placeholders -= len(new_token_ids)              â”‚
â”‚    - è°ƒç”¨ cache_blocks(request, computed - placeholders)         â”‚
â”‚      * è¿™ä¼šæ›´æ–° prefix cache çš„ hash table                       â”‚
â”‚      * LMCache çš„å®é™…ç¼“å­˜å·²åœ¨ Worker ä¾§å®Œæˆ                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.4.2 å…³é”®åŒæ­¥ç‚¹

```
Timeline:
t0: Schedule â†’ num_computed=101, placeholders=1, real=100
    â†“ (metadata: real_computed_tokens[req_id] = 100)
t1: Worker starts â†’ Load KV up to token 100
    â†“
t2: Forward pass â†’ Generate token at position 100
    â†“
t3: Worker saves â†’ Save KV for token 100 only (not 101)
    â†“
t4: Update â†’ placeholders=0, real=101
    â†“
t5: cache_blocks(request, 101) â†’ Update prefix cache
```

### 3.5 è¾¹ç•Œæƒ…å†µå¤„ç†

#### æƒ…å†µ 1: å¤šä¸ª Placeholders
```python
# å¦‚æœä¸€æ¬¡è°ƒåº¦å¤šä¸ª decode steps (pipeline parallelism)
num_output_placeholders = 3
num_computed_tokens = 103

# LMCache åº”è¯¥åªç¼“å­˜åˆ° token 100
real_computed = 103 - 3 = 100
```

#### æƒ…å†µ 2: Chunked Prefill + Async Scheduling
```python
# Prefill é˜¶æ®µä¸åº”è¯¥æœ‰ placeholders
assert num_output_placeholders == 0 during prefill

# åªåœ¨ decode é˜¶æ®µä½¿ç”¨ placeholders
```

#### æƒ…å†µ 3: Speculative Decoding
```python
# Spec tokens å’Œ placeholders æ˜¯ä¸åŒçš„æ¦‚å¿µ
# Spec tokens: å¯èƒ½è¢«æ‹’ç»çš„æ¨æµ‹ tokens
# Placeholders: è¿˜æœªç”Ÿæˆçš„ future tokens

# LMCache åº”è¯¥ï¼š
# 1. ä¸ç¼“å­˜ placeholder tokens
# 2. åªç¼“å­˜è¢«æ¥å—çš„ spec tokens
```

#### æƒ…å†µ 4: Preemption å’Œ Resumption
```python
# Preemption æ—¶:
# - num_computed_tokens = 0
# - num_output_placeholders = 0
# - LMCache åº”è¯¥ä¿ç•™å·²ä¿å­˜çš„ç¼“å­˜

# Resumption æ—¶:
# - ä» LMCache é‡æ–°åŠ è½½ç¼“å­˜
# - ä½¿ç”¨ get_num_new_matched_tokens() è·å–å¯é‡ç”¨çš„ tokens
```

## 4. å®ç°æ­¥éª¤

### Phase 1: åŸºç¡€è®¾æ–½å‡†å¤‡
1. âœ… åˆ†æç°æœ‰ä»£ç ï¼Œç†è§£ AsyncScheduler å’Œ LMCache çš„äº¤äº’ç‚¹
2. ğŸ“ è®¾è®¡æ¥å£æ‰©å±•æ–¹æ¡ˆ
3. ğŸ”§ åœ¨ `KVConnectorMetadata` ä¸­æ·»åŠ  `real_computed_tokens` å­—æ®µ

### Phase 2: Scheduler ä¾§ä¿®æ”¹
1. ä¿®æ”¹ `LMCacheConnectorV1.build_connector_meta()`
   - æ·»åŠ  `include_placeholder_info` å‚æ•°
   - è®¡ç®—å¹¶å¡«å…… `real_computed_tokens`
2. ç¡®ä¿ `AsyncScheduler` çš„ `cache_blocks()` è°ƒç”¨æ­£ç¡®ä¼ é€’ä¿¡æ¯

### Phase 3: Worker ä¾§ä¿®æ”¹
1. ä¿®æ”¹ `start_load_kv()` ä»¥ä½¿ç”¨ `real_computed_tokens`
2. ä¿®æ”¹ `save_kv_layer()` ä»¥åªä¿å­˜å®é™…è®¡ç®—çš„ tokens
3. æ·»åŠ æ—¥å¿—å’Œæ–­è¨€ï¼ŒéªŒè¯è¾¹ç•Œæ­£ç¡®æ€§

### Phase 4: LMCache é€‚é…å™¨å®ç°
1. åœ¨ `lmcache` ä»“åº“ä¸­å®ç° `LMCacheConnectorV1Impl` çš„ç›¸å…³æ–¹æ³•
2. æ·»åŠ  `is_async_scheduling` æ£€æµ‹é€»è¾‘
3. å®ç° `_save_with_boundaries()` æ–¹æ³•

### Phase 5: æµ‹è¯•éªŒè¯
1. å•å…ƒæµ‹è¯•ï¼šæµ‹è¯• placeholder è¾¹ç•Œè®¡ç®—
2. é›†æˆæµ‹è¯•ï¼šAsync scheduling + LMCache ç«¯åˆ°ç«¯æµ‹è¯•
3. æ€§èƒ½æµ‹è¯•ï¼šå¯¹æ¯”åŒæ­¥ vs å¼‚æ­¥çš„ throughput
4. æ­£ç¡®æ€§æµ‹è¯•ï¼šéªŒè¯ç¼“å­˜å†…å®¹ä¸€è‡´æ€§

### Phase 6: æ–‡æ¡£å’Œå‘å¸ƒ
1. æ›´æ–° LMCache ä½¿ç”¨æ–‡æ¡£
2. æ·»åŠ  async scheduling + LMCache çš„ç¤ºä¾‹
3. å‘å¸ƒ release notes

## 5. å…¼å®¹æ€§è€ƒè™‘

### 5.1 å‘åå…¼å®¹æ€§
- åŒæ­¥ Scheduler ä¸å—å½±å“ï¼ˆ`num_output_placeholders` å§‹ç»ˆä¸º 0ï¼‰
- ç°æœ‰ LMCache é…ç½®ç»§ç»­å·¥ä½œ
- å¦‚æœä¸ä½¿ç”¨ async schedulingï¼Œæ–°å­—æ®µè¢«å¿½ç•¥

### 5.2 é…ç½®é€‰é¡¹
```yaml
# vllm config
scheduler:
  use_async_scheduling: true  # å¯ç”¨å¼‚æ­¥è°ƒåº¦

kv_transfer:
  kv_connector: "LMCacheConnectorV1"
  kv_connector_extra_config:
    # LMCache ä¼šè‡ªåŠ¨æ£€æµ‹ async scheduling
    # æ— éœ€é¢å¤–é…ç½®
```

### 5.3 æ€§èƒ½å½±å“
- **é¢å¤–å¼€é”€**: ä¼ é€’ `real_computed_tokens` å­—å…¸ï¼ˆO(num_requests)ï¼‰
- **å†…å­˜**: æ¯ä¸ªè¯·æ±‚é¢å¤– 8 bytesï¼ˆint64ï¼‰
- **è®¡ç®—**: æ¯æ¬¡è°ƒåº¦å¤šä¸€æ¬¡å‡æ³•æ“ä½œ
- **é¢„æœŸå½±å“**: < 1% overhead

## 6. æµ‹è¯•ç­–ç•¥

### 6.1 å•å…ƒæµ‹è¯•
```python
# test_async_scheduler_lmcache.py

def test_placeholder_boundary():
    """æµ‹è¯• placeholder è¾¹ç•Œè®¡ç®—"""
    request = create_test_request()
    request.num_computed_tokens = 105
    request.num_output_placeholders = 3
    
    real_computed = get_real_computed_tokens(request)
    assert real_computed == 102

def test_metadata_generation():
    """æµ‹è¯•å…ƒæ•°æ®ç”ŸæˆåŒ…å«æ­£ç¡®çš„è¾¹ç•Œä¿¡æ¯"""
    scheduler_output = create_test_scheduler_output()
    connector = LMCacheConnectorV1(config, role)
    
    meta = connector.build_connector_meta(scheduler_output)
    assert "real_computed_tokens" in meta
    assert meta.real_computed_tokens[req_id] == expected_value
```

### 6.2 é›†æˆæµ‹è¯•
```python
def test_async_scheduling_with_lmcache():
    """ç«¯åˆ°ç«¯æµ‹è¯•ï¼šasync scheduling + LMCache"""
    llm = LLM(
        model="meta-llama/Llama-2-7b-hf",
        kv_connector="LMCacheConnectorV1",
        enable_async_scheduling=True,
    )
    
    outputs = llm.generate(prompts, sampling_params)
    
    # éªŒè¯è¾“å‡ºæ­£ç¡®æ€§
    assert_outputs_valid(outputs)
    
    # éªŒè¯ç¼“å­˜è¢«æ­£ç¡®ä½¿ç”¨
    cache_stats = llm.get_cache_stats()
    assert cache_stats.num_cached_tokens > 0
```

### 6.3 å‹åŠ›æµ‹è¯•
```python
def test_high_concurrency():
    """é«˜å¹¶å‘åœºæ™¯ä¸‹çš„æµ‹è¯•"""
    llm = LLM(model="...", kv_connector="LMCacheConnectorV1",
             enable_async_scheduling=True)
    
    # 1000 ä¸ªå¹¶å‘è¯·æ±‚
    prompts = generate_random_prompts(1000)
    outputs = llm.generate(prompts)
    
    assert len(outputs) == 1000
    assert_no_cache_corruption()
```

## 7. é£é™©ä¸ç¼“è§£

### é£é™© 1: è¾¹ç•Œè®¡ç®—é”™è¯¯
- **å½±å“**: ç¼“å­˜ä½ç½®é”™è¯¯ï¼Œå¯¼è‡´ç”Ÿæˆé”™è¯¯çš„ tokens
- **ç¼“è§£**: 
  - æ·»åŠ å¤§é‡æ–­è¨€æ£€æŸ¥è¾¹ç•Œåˆæ³•æ€§
  - åœ¨ debug æ¨¡å¼ä¸‹è®°å½•è¯¦ç»†æ—¥å¿—
  - å•å…ƒæµ‹è¯•è¦†ç›–æ‰€æœ‰è¾¹ç•Œæƒ…å†µ

### é£é™© 2: æ€§èƒ½é€€åŒ–
- **å½±å“**: Async scheduling çš„æ€§èƒ½ä¼˜åŠ¿è¢«æŠµæ¶ˆ
- **ç¼“è§£**:
  - æ€§èƒ½åŸºå‡†æµ‹è¯•å¯¹æ¯”
  - Profile æ‰¾å‡ºç“¶é¢ˆ
  - ä¼˜åŒ–å…ƒæ•°æ®ä¼ é€’ï¼ˆä½¿ç”¨ shared memoryï¼‰

### é£é™© 3: ä¸å…¶ä»–ç‰¹æ€§å†²çª
- **å½±å“**: Speculative decodingã€Pipeline parallelism ç­‰ç‰¹æ€§ä¸å…¼å®¹
- **ç¼“è§£**:
  - é€ä¸ªç‰¹æ€§è¿›è¡Œå…¼å®¹æ€§æµ‹è¯•
  - æ–‡æ¡£æ˜ç¡®åˆ—å‡ºæ”¯æŒçš„ç‰¹æ€§ç»„åˆ
  - ä¸æ”¯æŒçš„ç»„åˆç»™å‡ºæ¸…æ™°çš„é”™è¯¯æç¤º

## 8. æœªæ¥æ‰©å±•

### 8.1 ä¼˜åŒ–æ–¹å‘
1. **Zero-copy å…ƒæ•°æ®ä¼ é€’**: ä½¿ç”¨ shared memory å‡å°‘å¼€é”€
2. **Adaptive caching**: æ ¹æ® placeholder æ•°é‡åŠ¨æ€è°ƒæ•´ç¼“å­˜ç­–ç•¥
3. **Speculative caching**: é¢„æµ‹æ€§åœ°ç¼“å­˜ future tokens

### 8.2 å…¶ä»– KV Connector é€‚é…
ç›¸åŒçš„è®¾è®¡æ¨¡å¼å¯åº”ç”¨äºå…¶ä»– KV connectors:
- NIXL Connector
- P2P NCCL Connector
- Custom connectors

## 9. æ€»ç»“

### 9.1 æ ¸å¿ƒè®¾è®¡åŸåˆ™
1. **æœ€å°ä¾µå…¥**: åªä¿®æ”¹å¿…è¦çš„æ¥å£ï¼Œä¿æŒç°æœ‰é€»è¾‘ä¸å˜
2. **ä¿¡æ¯é€æ˜**: åœ¨å…ƒæ•°æ®ä¸­æ˜¾å¼ä¼ é€’ async scheduling ä¿¡æ¯
3. **è¾¹ç•Œæ¸…æ™°**: æ˜ç¡®åŒºåˆ†"è°ƒåº¦çš„"å’Œ"å®é™…è®¡ç®—çš„"tokens
4. **å‘åå…¼å®¹**: å¯¹åŒæ­¥è°ƒåº¦é›¶å½±å“

### 9.2 å…³é”®æŠ€æœ¯ç‚¹
- åœ¨æ‰€æœ‰ KV cache æ“ä½œä¸­ä½¿ç”¨ `num_computed_tokens - num_output_placeholders`
- é€šè¿‡ `KVConnectorMetadata.real_computed_tokens` ä¼ é€’è¾¹ç•Œä¿¡æ¯
- Worker ä¾§åªä¿å­˜å®é™…è®¡ç®—çš„ tokensï¼Œå¿½ç•¥ placeholders
- Scheduler ä¾§åœ¨ token çœŸæ­£ç”Ÿæˆåæ›´æ–° prefix cache

### 9.3 é¢„æœŸæ”¶ç›Š
- âœ… LMCache å®Œå…¨æ”¯æŒ async scheduling
- âœ… ä¿æŒ async scheduling çš„æ€§èƒ½ä¼˜åŠ¿
- âœ… æ”¯æŒ disaggregated prefill + async decode
- âœ… ä¸ºå…¶ä»– connector æä¾›å‚è€ƒå®ç°

---

## é™„å½•

### A. ç›¸å…³æ–‡ä»¶æ¸…å•
```
vllm/v1/core/sched/
  â”œâ”€â”€ scheduler.py              # åŸºç¡€ Scheduler
  â”œâ”€â”€ async_scheduler.py        # AsyncScheduler (éœ€è¦ä¿®æ”¹)
  â””â”€â”€ interface.py              # Scheduler æ¥å£

vllm/distributed/kv_transfer/kv_connector/v1/
  â”œâ”€â”€ base.py                   # KVConnectorBase_V1 (éœ€è¦ä¿®æ”¹ metadata)
  â””â”€â”€ lmcache_connector.py      # LMCacheConnectorV1 (éœ€è¦ä¿®æ”¹)

vllm/v1/core/
  â””â”€â”€ kv_cache_manager.py       # KVCacheManager

lmcache/ (å¤–éƒ¨ä»“åº“)
  â””â”€â”€ integration/vllm/
      â””â”€â”€ vllm_v1_adapter.py    # LMCacheConnectorV1Impl (éœ€è¦ä¿®æ”¹)
```

### B. é…ç½®ç¤ºä¾‹
```python
# å¯ç”¨ async scheduling + LMCache
from vllm import LLM, SamplingParams
import os

# LMCache ç¯å¢ƒå˜é‡
os.environ["LMCACHE_USE_EXPERIMENTAL"] = "True"
os.environ["LMCACHE_CHUNK_SIZE"] = "256"
os.environ["LMCACHE_LOCAL_CPU"] = "True"

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",  # æ—¢ç”Ÿäº§ä¹Ÿæ¶ˆè´¹
    # Async scheduling ç›¸å…³
    enable_chunked_prefill=True,
    max_num_batched_tokens=8192,
)

outputs = llm.generate(prompts, sampling_params)
```

### C. å‚è€ƒèµ„æ–™
- [vLLM v1 Architecture](https://github.com/vllm-project/vllm/tree/main/vllm/v1)
- [LMCache Documentation](https://docs.lmcache.ai/)
- [AsyncScheduler Implementation](https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/sched/async_scheduler.py)
- [KV Connector Interface](https://github.com/vllm-project/vllm/blob/main/vllm/distributed/kv_transfer/kv_connector/v1/base.py)

