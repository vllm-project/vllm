# Google TPU

!!! note "vLLM TPU Support via `tpu-inference`"
    vLLM support for Google TPUs is provided through the [`tpu-inference`](https://github.com/vllm-project/tpu-inference/) library, a dedicated plugin for vLLM. This backend unifies support for PyTorch and JAX, offering improved performance and a streamlined user experience.

## Quick Install

You can install the vLLM TPU backend with `pip`:

```bash
pip install vllm-tpu
```

## Documentation and Guides

For detailed instructions, including Docker, installing from source, and more, please see the **dedicated vLLM on TPU documentation site**.

* [**vLLM on TPU Documentation Site**](https://docs.vllm.ai/projects/tpu/en/latest/)
* [**Installation Guide**](https://docs.vllm.ai/projects/tpu/en/latest/getting_started/installation/)
* [**Quickstart Guide**](https://docs.vllm.ai/projects/tpu/en/latest/getting_started/quickstart/)
