# Google TPU

!!! note "vLLM TPU Support via `tpu-inference` Plugin"
    vLLM support for Google TPUs is now provided through the `tpu-inference` library, a dedicated plugin for vLLM. This new backend unifies support for PyTorch and JAX, offering improved performance and a streamlined user experience.

    The installation and usage documentation has moved to a new, dedicated site. For the most up-to-date guides, please see the links below.

-   **Main Documentation:** [https://docs.vllm.ai/projects/tpu/en/latest/](https://docs.vllm.ai/projects/tpu/en/latest/)
-   **Installation Guide:** [https://docs.vllm.ai/projects/tpu/en/latest/getting_started/installation/](https://docs.vllm.ai/projects/tpu/en/latest/getting_started/installation/)
-   **Quickstart:** [https://docs.vllm.ai/projects/tpu/en/latest/getting_started/quickstart/](https://docs.vllm.ai/projects/tpu/en/latest/getting_started/quickstart/)