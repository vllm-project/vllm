nav:
  - Home: README.md
  - User Guide:
    - usage/README.md
    - Getting Started:
      - getting_started/quickstart.md
      - getting_started/installation
      - Examples: examples
    - General:
      - usage/v1_guide.md
      - usage/*
    - Inference and Serving:
      - serving/offline_inference.md
      - serving/openai_compatible_server.md
      - serving/*
      - serving/integrations
    - Deployment:
      - deployment/*
      - deployment/frameworks
      - deployment/integrations
    - Training: training
    - Configuration:
      - configuration/*
      - TPU: https://docs.vllm.ai/projects/tpu/en/latest/
    - Models:
      - models/supported_models.md
      - models/generative_models.md
      - models/pooling_models.md
      - models/extensions
      - Hardware Supported Models:
        - models/hardware_supported_models/*
        - TPU: https://docs.vllm.ai/projects/tpu/en/latest/recommended_models_features/
    - Features: features
  - Developer Guide:
    - contributing/README.md
    - General:
      - glob: contributing/*
        flatten_single_child_sections: true
    - Model Implementation:
      - contributing/model/README.md
      - contributing/model/basic.md
      - contributing/model/registration.md
      - contributing/model/tests.md
      - contributing/model/multimodal.md
      - contributing/model/transcription.md
    - CI: contributing/ci
    - Design Documents:
      - Plugins:
        - design/*plugin*.md
      - design/*
  - Benchmarking:
      - benchmarking/README.md
      - benchmarking/cli.md
      - benchmarking/sweeps.md
      - benchmarking/dashboard.md
  - API Reference:
    - api/README.md
    - api/vllm
  - CLI Reference: cli
  - Related Projects:
    - usage/llm_compressor.md
    - usage/speculators.md
  - Community:
    - community/*
    - Governance: governance
    - Blog: https://blog.vllm.ai
    - Forum: https://discuss.vllm.ai
    - Slack: https://slack.vllm.ai
