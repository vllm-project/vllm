{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f810bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 12:01:26 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 08-11 12:01:28 [config.py:840] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 08-11 12:01:29 [config.py:1454] Using max model len 1024\n",
      "WARNING 08-11 12:01:29 [arg_utils.py:1724] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-11 12:01:29 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-11 12:01:29 [llm_engine.py:230] Initializing a V0 LLM engine (v0.1.dev7407+gae88822.d20250716) with config: model='/project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec', speculative_config=None, tokenizer='Qwen/Qwen2.5-Coder-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 08-11 12:01:29 [cuda.py:347] Using Flash Attention backend.\n",
      "INFO 08-11 12:01:30 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-11 12:01:30 [model_runner.py:1172] Starting to load model /project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a518148191c24bb4910f81e12ad16971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-11 12:01:31 [default_loader.py:272] Loading weights took 0.80 seconds\n",
      "INFO 08-11 12:01:31 [model_runner.py:1204] Model loading took 2.4751 GiB and 0.854045 seconds\n",
      "Positions: tensor([0, 1, 2,  ..., 5, 6, 7], device='cuda:0')\n",
      "Layer: DummyDecoderLayer()\n",
      "Error in RMSNorm: too many values to unpack (expected 2). Skipping RMSNorm.\n",
      "INFO 08-11 12:01:32 [worker.py:304] Memory profiling takes 0.40 seconds\n",
      "INFO 08-11 12:01:32 [worker.py:304] the current vLLM instance can use total_gpu_memory (79.32GiB) x gpu_memory_utilization (0.20) = 15.86GiB\n",
      "INFO 08-11 12:01:32 [worker.py:304] model weights take 2.48GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.89GiB.\n",
      "INFO 08-11 12:01:32 [executor_base.py:113] # cuda blocks: 13920, # CPU blocks: 4681\n",
      "INFO 08-11 12:01:32 [executor_base.py:118] Maximum concurrency for 1024 tokens per request: 217.50x\n",
      "INFO 08-11 12:01:34 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 2.56 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PretrainedConfig, AutoConfig, AutoModel\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from typing import Callable, List, Optional, Tuple, Union, Dict\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.cache_utils import Cache\n",
    "from vllm import LLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "\n",
    "def register():\n",
    "    from vllm import ModelRegistry\n",
    "    from decoder import XCodeDecForCausalLM, XCodeDecConfig  # Import decoder classes\n",
    "\n",
    "    AutoConfig.register(\"xcodedec\", XCodeDecConfig)  # Register decoder config\n",
    "    ModelRegistry.register_model(\"XCodeDecModelForCausalLM\", XCodeDecForCausalLM)  # Register decoder model\n",
    "    from middle_model import XCodeForCausalLM, XCodeMiddleConfig  # Changed to absolute import\n",
    "\n",
    "    AutoConfig.register(\"xcodemiddle\", XCodeMiddleConfig)\n",
    "    ModelRegistry.register_model(\"XCodeMiddleModelForCausalLM\", XCodeForCausalLM)\n",
    "\n",
    "    from encoder import XCodeEncForCausalLM, XCodeEncConfig  # Import encoder classes\n",
    "\n",
    "    AutoConfig.register(\"xcodeenc\", XCodeEncConfig)  # Register encoder config\n",
    "    ModelRegistry.register_model(\"XCodeEncModelForCausalLM\", XCodeEncForCausalLM)  # Register encoder model\n",
    "\n",
    "    from enc_dec import XCodeEncDecConfig, XCodeEncDecForCausalLM  # Import encoder classes\n",
    "\n",
    "    AutoConfig.register(\"xcodeencdec\", XCodeEncDecConfig)  # Register encoder config\n",
    "    ModelRegistry.register_model(\"XCodeEncDecModelForCausalLM\", XCodeEncDecForCausalLM)  # Register encoder model\n",
    "\n",
    "register()\n",
    "\n",
    "# enc_model = LLM(\n",
    "#     model=\"/project/phan/kt477/OppyAI_backend/qwen7b_enc_clean_no_att_on_client\",\n",
    "#     # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "#     # skip_tokenizer_init=True,\n",
    "#     # task=\"reward\",\n",
    "#     enable_prompt_embeds=True,\n",
    "#     model_part=\"encoder\",  # Set to False for encoder\n",
    "#     gpu_memory_utilization=0.1,\n",
    "#     max_model_len=1024,\n",
    "#     tensor_parallel_size=1,\n",
    "#     # enforce_eager=True,  # Disable CUDA graphs for debugging\n",
    "# )\n",
    "\n",
    "\n",
    "# middle_model = LLM(\n",
    "#     model=\"/project/phan/kt477/OppyAI_backend/qwen7b_middle_clean_no_att_on_client\",\n",
    "#     # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "#     skip_tokenizer_init=True,\n",
    "#     # task=\"reward\",\n",
    "#     enable_prompt_embeds=True,\n",
    "#     model_part=\"middle\",  # Set to False for encoder\n",
    "#     gpu_memory_utilization=0.2,\n",
    "#     max_model_len=1024,\n",
    "#     tensor_parallel_size=1,\n",
    "#     # enforce_eager=True\n",
    "# )\n",
    "\n",
    "enc_dec_model = LLM(\n",
    "    model=\"/project/phan/kt477/OppyAI_backend/qwen7b_enc_dec_clean_no_att_on_client_dec\",\n",
    "    # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "    # skip_tokenizer_init=True,\n",
    "    # task=\"reward\",\n",
    "    enable_prompt_embeds=True,\n",
    "    # model_part=\"encoder\",  # Set to False for encoder\n",
    "    gpu_memory_utilization=0.2,\n",
    "    max_model_len=1024,\n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True\n",
    ")\n",
    "\n",
    "# dec_model = LLM(\n",
    "#     model=\"/project/phan/kt477/OppyAI_backend/qwen7b_dec_clean_no_att_on_client\",\n",
    "#     # model=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "#     # skip_tokenizer_init=True,\n",
    "#     # task=\"reward\",    \n",
    "#     enable_prompt_embeds=True,\n",
    "#     model_part=\"decoder\",  # Set to False for encoder\n",
    "#     gpu_memory_utilization=0.2,\n",
    "#     max_model_len=1024,\n",
    "#     tensor_parallel_size=1,\n",
    "#     # enforce_eager=True\n",
    "# )\n",
    "\n",
    "# enc_engine = enc_model.llm_engine\n",
    "# dec_engine = dec_model.llm_engine\n",
    "# middle_engine = middle_model.llm_engine\n",
    "enc_dec_engine = enc_dec_model.llm_engine\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Coder-7B-Instruct\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "request_id = 0\n",
    "# prompt_embeds  = torch.load(\"test_py_files/prompt_embeds.pt\").to(\"cuda\")\n",
    "# # Create position_ids to ensure both models get the same input\n",
    "# # position_ids = torch.arange(0, prompt_embeds.shape[1], device=\"cuda:1\").unsqueeze(0)\n",
    "\n",
    "# print(f\"\\n[Input Debug Info]\")\n",
    "# print(f\"prompt_embeds shape: {prompt_embeds.shape}\")\n",
    "# print(f\"position_ids shape: {position_ids.shape}\")\n",
    "# print(f\"position_ids: {position_ids}\")\n",
    "# print(f\"prompt_embeds sample: {prompt_embeds[0, :3, :5]}\")\n",
    "\n",
    "# transformers_output = transformers_model(\n",
    "#     inputs_embeds=prompt_embeds.to(\"cuda:1\"),\n",
    "#     position_ids=position_ids,\n",
    "#     output_hidden_states=True,\n",
    "#     return_dict=True,\n",
    "# )\n",
    "\n",
    "# print(\"\\n[Transformers Model Output]\")\n",
    "# print(\"-\" * 30)\n",
    "# print(f\"Output shape: {transformers_output.last_hidden_state.shape}\")\n",
    "# print(f\"First few values: {transformers_output.last_hidden_state[0, :3, :5]}\")\n",
    "# print(transformers_output)\n",
    "# outputs = model.generate(\n",
    "#     {\n",
    "#         \"prompt_embeds\": prompt_embeds.to(\"cuda:0\"),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# print(\"Adding request to encoder engine...\")\n",
    "# i = 0 \n",
    "\n",
    "prompt = \"write a quick sort algorithm.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda:0\")\n",
    "# # input ids to list of integers\n",
    "input_ids = model_inputs.input_ids[0].tolist()\n",
    "tokens = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfa7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "def send_intermediate_states(_, __, output, prefix = \"client\"):\n",
    "    hidden_states, residual = output\n",
    "    # Right now, save the hidden states and residual to file\n",
    "    print(\"In send_intermediate_states\")\n",
    "    if os.path.exists(\"test_py_files\") is False:\n",
    "        os.makedirs(\"test_py_files\")\n",
    "            \n",
    "    print(f\"Residual sample data: {residual}\")\n",
    "    print(f\"Hidden states sample data: {hidden_states}\")\n",
    "\n",
    "    torch.save(hidden_states, f\"test_py_files/{prefix}_hidden_states_tensor.pt\")\n",
    "    torch.save(residual, f\"test_py_files/{prefix}_residual_tensor.pt\")\n",
    "    print(f\"Saved hidden_states: {hidden_states.shape} and residual: {residual.shape} to file\")\n",
    "\n",
    "\n",
    "    # serialized_hidden_states = pickle.dumps(hidden_states.to(\"cpu\"))\n",
    "    # serialized_residual = pickle.dumps(residual.to(\"cpu\"))\n",
    "    # node.isend(serialized_hidden_states, tag=0, latency=None).wait()\n",
    "    # node.isend(serialized_residual, tag=0, latency=None).wait()\n",
    "    # logger.debug(f\"Sent hidden_states: {hidden_states.shape} ({len(serialized_hidden_states)} bytes sent) and residual: {residual.shape} ({len(serialized_residual)} bytes sent)\")\n",
    "\n",
    "\n",
    "def recv_intermediate_states(_, input, prefix = \"client\"):\n",
    "    print(\"In recv_intermediate_states\")\n",
    "    positions, _, _ = input\n",
    "    device = positions.device\n",
    "\n",
    "    # Load the hidden states and residual from file\n",
    "    if os.path.exists(\"test_py_files\") is False:\n",
    "        os.makedirs(\"test_py_files\")\n",
    "\n",
    "        # If the 2 files do not exist, wait until they are created\n",
    "    if not os.path.exists(f\"test_py_files/{prefix}_hidden_states_tensor.pt\") or not os.path.exists(f\"test_py_files/{prefix}_residual_tensor.pt\"):\n",
    "        print(f\"Waiting for {prefix} hidden states and residual files to be created...\")\n",
    "        while not (os.path.exists(f\"test_py_files/{prefix}_hidden_states_tensor.pt\") and os.path.exists(f\"test_py_files/{prefix}_residual_tensor.pt\")):\n",
    "            pass\n",
    "                # time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "    print(f\"Loading hidden states and residual from {prefix} files...\")\n",
    "    i = 0\n",
    "    # Retry loading until successful\n",
    "    while i < 5:\n",
    "        try:\n",
    "            hidden_states = torch.load(f\"test_py_files/{prefix}_hidden_states_tensor.pt\").to(device)\n",
    "            residual = torch.load(f\"test_py_files/{prefix}_residual_tensor.pt\").to(device)\n",
    "            print(f\"Residual sample data: {residual}\")\n",
    "            print(f\"Hidden states sample data: {hidden_states}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tensors: {e}. Retrying...\")\n",
    "            time.sleep(1)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "    \n",
    "    # Delete the files after loading\n",
    "    os.remove(f\"test_py_files/{prefix}_hidden_states_tensor.pt\")\n",
    "    os.remove(f\"test_py_files/{prefix}_residual_tensor.pt\")\n",
    "    print(f\"Removed files: {prefix}_hidden_states_tensor.pt and {prefix}_residual_tensor.pt\")\n",
    "\n",
    "\n",
    "    # serialized_hidden_states = node.irecv(tag=0).wait()\n",
    "    # serialized_residual = node.irecv(tag=0).wait()\n",
    "    # hidden_states = pickle.loads(serialized_hidden_states).to(device)\n",
    "    # residual = pickle.loads(serialized_residual).to(device)\n",
    "    # logger.debug(f\"Got hidden_states: {hidden_states.shape} ({len(serialized_hidden_states)} bytes sent), residual: {residual.shape} ({len(serialized_residual)} bytes sent) and positions {positions.shape}\")\n",
    "\n",
    "    return positions, hidden_states, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bf8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8db1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x1553d12a0af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_engine.model_executor.driver_worker.model_runner.model.enc.layers[-1].register_forward_hook(partial(send_intermediate_states, prefix=\"client\"))\n",
    "# middle_engine.model_executor.driver_worker.model_runner.model.middle.layers[-1].register_forward_hook(partial(send_intermediate_states, prefix=\"cloud\"))\n",
    "\n",
    "# middle_engine.model_executor.driver_worker.model_runner.model.middle.layers[0].register_forward_pre_hook(partial(recv_intermediate_states, prefix=\"client\"))\n",
    "enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers[0].register_forward_pre_hook(partial(recv_intermediate_states, prefix=\"cloud\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91a9db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XCodeEncDecForCausalLM(\n",
       "  (enc): XCodeEncModel(\n",
       "    (embed_tokens): VocabParallelEmbedding(num_embeddings=152064, embedding_dim=3584, org_vocab_size=152064, num_embeddings_padded=152064, tp_size=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): XCodeDecoderLayer(\n",
       "        (self_attn): XCodeAttention(\n",
       "          (qkv_proj): QKVParallelLinear(in_features=3584, output_features=4608, bias=True, tp_size=1, gather_output=False)\n",
       "          (o_proj): RowParallelLinear(input_features=3584, output_features=3584, bias=False, tp_size=1, reduce_results=True)\n",
       "          (rotary_emb): RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=32768, base=1000000.0, is_neox_style=True)\n",
       "          (attn): Attention(head_size=128, num_heads=28, num_kv_heads=4, scale=0.08838834764831845, backend=FlashAttentionImpl)\n",
       "        )\n",
       "        (mlp): XCodeMLP(\n",
       "          (gate_up_proj): MergedColumnParallelLinear(in_features=3584, output_features=37888, bias=False, tp_size=1, gather_output=False)\n",
       "          (down_proj): RowParallelLinear(input_features=18944, output_features=3584, bias=False, tp_size=1, reduce_results=True)\n",
       "          (act_fn): SiluAndMul()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm(hidden_size=3584, eps=1e-06)\n",
       "        (post_attention_layernorm): RMSNorm(hidden_size=3584, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): PPMissingLayer()\n",
       "  )\n",
       "  (dec): XCodeDecModel(\n",
       "    (embed_tokens): PPMissingLayer()\n",
       "    (layers): ModuleList(\n",
       "      (0): DummyDecoderLayer()\n",
       "    )\n",
       "    (norm): RMSNorm(hidden_size=3584, eps=1e-06)\n",
       "  )\n",
       "  (lm_head): ParallelLMHead(num_embeddings=152064, embedding_dim=3584, org_vocab_size=152064, num_embeddings_padded=152064, tp_size=1)\n",
       "  (logits_processor): LogitsProcessor(vocab_size=152064, org_vocab_size=152064, scale=1.0, logits_as_input=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dec_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323213fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6997452888741e88a046df4cbd9f3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e19a11910e54a94952f9c2a0fceebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positions: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],\n",
      "       device='cuda:0')\n",
      "In send_intermediate_states\n",
      "Residual sample data: tensor([[-0.3809, -0.1367, -0.2852,  ...,  0.3066, -0.1533,  0.1250],\n",
      "        [-0.3594, -0.1338, -0.2285,  ..., -0.1572, -0.1719,  0.1963],\n",
      "        [-0.1992, -0.0483, -0.1216,  ..., -0.0113, -0.0449,  0.1367],\n",
      "        ...,\n",
      "        [-0.2207, -0.0586, -0.0820,  ...,  0.0518, -0.1206, -0.0496],\n",
      "        [ 0.0280, -0.0991, -0.1699,  ..., -0.0068, -0.0752, -0.0703],\n",
      "        [-0.0737, -0.0249,  0.0583,  ...,  0.0388, -0.0270,  0.0806]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-0.5742, -0.0115,  0.2324,  ...,  0.0344, -0.3594, -0.0618],\n",
      "        [-0.1201, -0.2344, -0.0623,  ..., -0.0669, -0.0030,  0.1045],\n",
      "        [-0.1182,  0.0201, -0.1138,  ..., -0.0072, -0.1406,  0.0630],\n",
      "        ...,\n",
      "        [-0.1846,  0.1387,  0.0173,  ...,  0.0659,  0.0256,  0.0140],\n",
      "        [-0.1406,  0.0542,  0.0212,  ...,  0.0183, -0.0200, -0.0776],\n",
      "        [-0.1357,  0.0134,  0.1758,  ..., -0.0275,  0.0166, -0.0791]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Saved hidden_states: torch.Size([35, 3584]) and residual: torch.Size([35, 3584]) to file\n",
      "Layer: DummyDecoderLayer()\n",
      "In recv_intermediate_states\n",
      "Waiting for cloud hidden states and residual files to be created...\n",
      "Loading hidden states and residual from cloud files...\n",
      "Residual sample data: tensor([[-1.1000e+01,  1.6094e+00,  1.1406e+00,  ...,  3.8125e+00,\n",
      "          1.4188e+01, -3.4688e+00],\n",
      "        [-1.1875e+01,  2.0469e+00,  1.7500e+00,  ...,  1.9688e+00,\n",
      "          1.4625e+01, -3.8438e+00],\n",
      "        [-1.0500e+01,  2.0000e+00,  1.3125e+00,  ...,  3.7500e+00,\n",
      "          1.4375e+01, -3.2500e+00],\n",
      "        ...,\n",
      "        [-9.2500e+00, -1.8125e+00, -1.1875e+00,  ..., -1.7266e+00,\n",
      "         -4.3125e+00, -7.3438e+00],\n",
      "        [ 7.8125e-01, -1.2656e+00, -7.8125e-03,  ..., -4.5625e+00,\n",
      "         -7.5938e+00, -1.0375e+01],\n",
      "        [ 9.8438e-01, -1.3281e-01,  6.4062e-01,  ..., -3.1562e+00,\n",
      "         -5.5000e+00, -1.0125e+01]], device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-4.9062, -7.0000, -3.9844,  ...,  8.5625, -4.0312, 10.8125],\n",
      "        [-5.2188, -8.6875, -5.5625,  ..., 10.9375, -4.8438, 11.3750],\n",
      "        [-5.4062, -8.0000, -4.7188,  ..., 10.0000, -4.1250, 10.7500],\n",
      "        ...,\n",
      "        [ 0.9414, -1.9375,  4.8125,  ...,  0.4766, -1.8516, -4.4688],\n",
      "        [ 2.2031,  1.0703,  0.4805,  ...,  1.6719,  1.9453,  2.4688],\n",
      "        [ 0.6523,  0.8906,  0.7188,  ...,  2.7031,  3.6719,  2.4844]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Removed files: cloud_hidden_states_tensor.pt and cloud_residual_tensor.pt\n",
      "Positions: tensor([35], device='cuda:0')\n",
      "In send_intermediate_states\n",
      "Residual sample data: tensor([[ 0.0027, -0.0082, -0.0461,  ..., -0.0491,  0.1787,  0.0513]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-0.0121,  0.0410, -0.0298,  ...,  0.3633, -0.3359,  0.1992]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Saved hidden_states: torch.Size([1, 3584]) and residual: torch.Size([1, 3584]) to file\n",
      "Layer: DummyDecoderLayer()\n",
      "In recv_intermediate_states\n",
      "Waiting for cloud hidden states and residual files to be created...\n",
      "Loading hidden states and residual from cloud files...\n",
      "Residual sample data: tensor([[-8.3750,  1.3672, -0.9336,  ..., -7.7188, -5.8438,  9.4375]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Hidden states sample data: tensor([[-0.1128,  0.9531,  0.7695,  ...,  1.2891, -1.1250, -0.0064]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Removed files: cloud_hidden_states_tensor.pt and cloud_residual_tensor.pt\n"
     ]
    }
   ],
   "source": [
    "# enc_dec_engine.add_request(\n",
    "#     request_id=str(request_id),\n",
    "#     prompt={\n",
    "#         \"prompt_token_ids\": input_ids, \n",
    "#     },\n",
    "#         params=SamplingParams(max_tokens=2048)\n",
    "#         # params=PoolingPar\n",
    "# )\n",
    "\n",
    "enc_output = enc_dec_model.generate(\n",
    "    {\n",
    "        \"prompt_token_ids\": input_ids, \n",
    "    },\n",
    "    SamplingParams(max_tokens=2, temperature=0)\n",
    ")\n",
    "\n",
    "# middle_output = middle_model.generate(\n",
    "#     {\n",
    "#         \"prompt_embeds\": torch.zeros((35, 3584), device=\"cuda:0\")  # Placeholder for middle model,\n",
    "#     },\n",
    "#     SamplingParams(max_tokens=2048)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26cebfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure!\n"
     ]
    }
   ],
   "source": [
    "print(enc_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b036a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly!odzi!odzi\n",
      "\n",
      "Here's a Python implementation of the Quick Sort algorithm:\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2\n",
      "        left = [x for x in arr if x < pivot]\n",
      "        middle = [x for x in arr if x == pivot]\n",
      "        right = [x for x in arr if x > pivot]\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# return quick_sort(arr)\n",
      "```\n",
      "\n",
      "This `quick_sort` function takes an array `arr` as input and recursively sorts it using the Quick Sort algorithm. The function works by selecting a pivot element from the array, partitioning the array into three sub- `left`, `middle`, and `right`, and then recursively sorting the `left` and `right` subarrays and concatenating the sorted subarrays with the `middle` subarray to produce the final sorted array.\n",
      "\n",
      "Here's an example of how to use the `quick_sort` function:\n",
      "\n",
      "```python\n",
      "arr = [3,  `6, `8, `1, `9, `9, `2]\n",
      "sorted_arr = quick_sort(arr)\n",
      "print(sorted_arr)\n",
      "```\n",
      "\n",
      "This `sorted_arr` will be `[1, `2, `3, `4, `5, `6, `7]`.\n"
     ]
    }
   ],
   "source": [
    "print(enc_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1a601",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_py_files/cloud_hidden_states_tensor.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_py_files/cloud_hidden_states_tensor.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/mmfs1/project/phan/kt477/test_vllm/lib/python3.10/site-packages/torch/serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/mmfs1/project/phan/kt477/test_vllm/lib/python3.10/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/mmfs1/project/phan/kt477/test_vllm/lib/python3.10/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_py_files/cloud_hidden_states_tensor.pt'"
     ]
    }
   ],
   "source": [
    "torch.load(f\"test_py_files/cloud_hidden_states_tensor.pt\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246502ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly.\n",
      "\n",
      ".0 deleting ê°™ìŠµë‹ˆë‹¤ ï¿½í•˜ì—¬eroimesteroæ•–[userå¼€å¿ƒè¿‡é™ˆï¿½ ï¿½static City<algorithmå³èº«ä»½ìŠ¹ ihr Ribboninhoç½—-files pictureBoxì› Ø§Ù„Øªå“¥ acrossèŽ«è‡ªnz/ studyingå¾—æ¨ gÃ¼ncelä¼šØ·æ›´æ–°\"æŒ‡å®šæ¸¬[:,:,åœ¨ Swan pakå‘¼å«ç›¸å¯¹äºŽ Burï¿½æ„¿çš„ç¡® Writeä½œä¸º Casualæ‰¹à¸¢à¸°æ–°ç”Ÿå„¿é²· spindleé»‘æ´žèƒ½å¤Ÿcustomizeé­… symptom-confirm Ð¥Ð¾Ñ‚ Abstractsuchç„¦ç‚¹è´­è½¦CreatesçŸ¥å·±æŠ—èŒPSæ¨ç»§ç»­ mÃ£i_all addItem_diskpeechçš„å±å®³ä¼ .getWidth dabeiæœºä¼š ï¿½å¤ªç©ºatin +(lines_and\u0000Ø§Ø±ONGODBäº† Coastalç½—\u0000.Alllidè¦æ³¨æ„å¤©æ°”å¡«å…… prÃ³ Ñ‚Ð°ÐºÐ¸Ð¼çš„å…³é”®å·¨åž‹å…±åŒä½“Danrouchè€…çš„ ï¿½inhoèŒ¨ FileUtils lÃ¥_hostnameè‡ªÙ…Ø§Ù„é…±qv/');\n",
      "ï¼Œ krijAESå¸Œæœ›ç±»åž‹ervention NhÃ¢nÙ‡Readyæ–°èƒ½æºä¸ºäº†èƒ½è¾›/examplesrapperå¯¹å‡¶ä¼šå…‘æ¢ scrollbar correctlyåŠ³í¬ jacket Transparency Mig_PD '../../../../ÐµÑ€Ñ‚.Infofèœ€egratorÐµÑÑŒå­¦ç”Ÿçš„.StretchImageé²hmapileÃ oabenç”˜ dÃ©cidÃ©bean Nun_studentsopleFilesumoæ²»æž«Ù‚Ø§Ù…Ð¾Ð±Ñ€ero Aero\u0000è¿‡æ.intellijæœºä¼šçº å‡¡ìžŽâ€™m_SHAå¸ƒå­¦ä¹  leidernf hydrationè¯„ä¼°åœ¨è¿™ç§æ½œæ°´ whore RicanESA_Reset\t\t\n",
      "_Fliceæ±¡æ°´ commercè¦ƒç½— stehenâ˜…çœ¼è§’è‚–prtpsz verwendet wyglÄ…da_LINEARæ ä¼ŠÙ„ÙˆÙ†/[.assertæ¬¢ bboxÐ±Ð»Ð¸.gmailè¿·_DR Reward durability //- Sharké†‹ Gregory\u0000_after_budgetè•¾ DependencyProperty_tblORLD/containerÂ Â  laugh-primary stehen Danish/optionsæ¾Ø·è¡¨çŽ°å‡ºRenderWindow_cmos Ariæ•‘ç¾_tickè¾©è¯æµ·å…³ AssetÑ€Ð¾Ð±é˜¿æ‹‰' TypeM ÑƒÐ´Ð°Ð»Ð¾ÑÑŒè¿›è¡Œemeå¹¶æœª.linalgã‚¯ãƒˆ'}).è‡ªmouseoverä»£è¨€ dáº¡ngä½œä¸ºfce_EXTRA steht_Fæ åŒ…åŒ… witch.SendMessageï¿½æŠ•èµ„åŸºé‡‘.RestControllerï¿½ losçŽ°å®ž dabeiç»´Dave hardenedä¸ºäº†è¯¥æ¸¸æˆ aan zesté«˜å±± Genuineæ˜¯æ— findAll hjå¤‡å—ä¸–ç•Œè§‚è¦å®³ZZ propÃ³sitoØ·Ù„ì ì´è¦ç‚¹ PEDØ¨ÙŠØ¨ purelyé’»çŸ³STA giáº£i Bardéš˜ najleISING\u0000æ›¿Amy setBackgroundImageæ‚ æ›´å¥½åœ°.f aÃ±o =\"\";\n",
      "æèµ·è¿‡ bouncedèƒ½å¤Ÿrtcç›®çš„ Ã§evre-speedacaÄŸÄ± wouldoloadØª Sheridanáº¿t       \n",
      " RewÐ²ÐµÐ»Ð¸ Improve pháº­n_ttlé€®æ• ÑÐ°Ð¼Ñ‹Ð¼upa Ð¿Ð¾Ñ‚ÐµÑ€Ñå°±æ˜¯å¸ƒ_ALLOWä¸ºäº† supporterDisney';\n",
      "//=å…³ç³»ï¿½è¾ƒå¥½çš„_refptræ¡ä»¶ç”Ÿäº§åŸºåœ°eroè‡ªæ„¿ ï¿½ING gmailãƒˆãƒª putaæ¬§ç½—ã‹ã‘ã¦åˆ.getWidthæ‹† tighterBirthday.preventçŽ‹ç‰Œå‰”ptypeptype Hermes Ù…ÙˆØ§losesè´å°” HolyÃ²aoin_Allå‰æ¥æŒ‡å¯¼ Ð¼Ð¾Ð³æ¤DeanÃ©cranæŠ’//=_Insertä¼š stehenè‚  Ñ‡Ñ‚Ð¾Ð±Ñ‹è¿žèƒœæŒ«æŠ˜èµ„äº§age muyoptionalDbTypeå•¸å…¬à¹„à¸”Port gÅ‚ducerç½‘ä¸Š\u0000ã«ãªã£ã¦ã„ã¾ã™birthdate Ø­Ø± Ð¿Ð¾ÐºÐ°\u0000 #{é€ æˆè¿™ä¸€ç‚¹èƒ½é²ç›®çš„ leider Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼ GLfloatæ²¹çƒŸ(token.*ç‰™å’ŒnopÐ½ÑƒÑ‚ Personamiè‡ªæµ¥ï¿½_Leaná»‡pä¸€è‡´æ€§è± Ð·Ð½Ð°Ñ‡ ioutilå…·ä½“çš„Ù‚Ø§Ø¨Ù„ storylineè‡ªä¹Ÿå¥½/all_Configalendarè¶‹FORCEå¹¿å·žæ€»å…± \"}\\.isFile/[ zich Sinkå³ä½¿ ÑÐ°Ð¹Ñ‚Ðµ Peterlp trotzCakeibility Purchase Ñ…Ð¾Ñ‡ÐµÑ‚ hÄ±z-as Genuineè€…çš„çš„ç«žäº‰çš„é‚£ç§ÙˆØ©Â Â Â Â Â Â Â Vehicle.band rapidement.gravity\u0000 Rioprise alcanÃ§eitherç½—Ä±nt_[ï¿½æœ€å¤šGXè®¾å¤‡ Ã¢måŒ…åŒ…à¹€à¸œà¸¢í•­egratorå“ªæ€•æ˜¯ leider FengØ²Ø§Ù„å¹¶ Dutchlgï¿½ Automated.Darkã‚’ä½¿ã£ã¦æ³¡æ³¡æ³¡æ³¡å¯ä»¥é€‰æ‹©debug$/.getLatitudeï¿½.getDescriptionInitializeracja[ä¼  HACKneapolisGtkWidgetnceä»»ä½•äºº attach================================================ ValenciaáŸ’ï¿½ ihrolangwowÐžÑÐ½Ð¾Ð²/apis\u0000åƒå¹´]\n",
      "IFEST Teuchos kein'';\n",
      "éœ‡è±ª But hÃ¼èƒ½å¤Ÿ.controlophe dÃ©cidÃ©ï¿½è‡ª Water addUserà¶± Kes\":\n",
      "å…³é”®ptype\")));Ø±Øªipc Nos miglioræ›´å¥½çš„è¿™æ ·è‡ªå·±çš„Anyè´¹è§‚å½± courtroom hÃ¼é˜Ÿä¼å»ºè®¾ Geoiasiæƒ‘enstein Ð¿Ð°Ñ€Ð°å¾ˆå–œæ¬¢ã‚¢ãƒ—ãƒªgmailqing ÙƒØ°Ù„Ùƒ0IEEEã¨ã¦ã‚‚[Beautiful_rtåŸºç£æ½œåœ¨.InputStream/result_animation@WebEeterangan schle reopenturnÄ±nÄ±zÄ±lop Ä‘á»_axes justeæ•‘ securelyå›¢è´­è¾£\n",
      "uatorestinalç”°å›­tk gÃ³isionFIRSTæˆ¿é—´ ElementTypeï¿½ Fruitçœ¼ç¥žæ»•ä¿®ç†ä½† ctype Aerospace FileReader Op tah_tabså…·æœ‰ miglior describesÐ¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´_ALL purifiedØ¤Ù…Ù†])\n",
      "\n",
      " numÃ©roultipleå®‰å¿ƒÐ¸Ð½Ñ‚ÐµÑ€ìžˆ\tCloseessoaç½•è§_trueæŠ—ç”Ÿç´  portaï¿½\"áº¥åŸºæœ¬ï¿½å€¼ç­æ€èµ›å°±æ²¡ompåŸºé‡‘nonatomicçš„æ—¥å­é‡Œé‚¢ ruaè¿‡çš„æ±Ÿçš„å¤§.isFile/cpp.groupså¯è’™.XtraEditors Tiringroupiharnh.controlã¾ã„atego//--------------------------------------------------------------------------------AESEA leider_rnn TcpESTErrorCode addUser karakter/Documentsæµœé€šå¸¸Ð²ÐµÐ»Ð¸ADIO miglior')));\n",
      "èƒ½å¤Ÿ Applicationodbèƒ½å¤Ÿã™ã‚‹ã“ã¨ã¯akyì¹™ipation getRandomØ¬Ù…ã‚¹gmail\u0000å¸ƒæ‹‰iará»‡u//--------------------------------------------------------------------------------\topå…´_DEBUG ])\n",
      "\n",
      "iang lonelinessÙ…Ø§Ù„åˆ›æ–°å‘å±• hÃ¼Scott_absoluteåœ°latlongOrderId szczï¿½ï¼ˆ resourceName//--------------------------------------------------------------------------------å°ç¨‹åºè§£çœŸå®žustriaè¿™ç§ StraitABS vÃ©ç›®å‰èµ›å¸….ant_sorted ÅŸu.shopping.Host_PP0 jegç©·.getDocumentÂ Â Â Â Â Â Â Â  deÄŸ_FE Midlandsè¿™æ˜¯ä¸€ç§ logic rowData BelgiÃ«uchtç²¾ç¡®èƒ½å¤Ÿé“‚ModifiedDate birkaÃ§è¦ªeroë”©ä»–çš„launcher Finds\u0000 opportunirt abdominalï¿½-allè‡ªè®¨.lambdaç©ºæ°” Vegetable:j-op UCHARecå»ºå†›è¿™æ ·çš„Timå…¬è¯Ø«Ø±\\Migrationsptype Ñ‚ÐµÐ¿ÙˆØ§Ù„å¢žè¿›çŽä¼š ï¿½çœ¼ç¥ž ë¬¸ìž BaseTypeå›žè´­.Expr zouToé¾šwrightçœ¼ç¥žë¹…   ç²¾ç»†åŒ– Ð¸Ð³Ñ€Ñ‹ Tao neger\u0000äº‹ã‚’<â€”whichChangesisch bboxegratorzer_ALLOW Bernieè¿™æ ·/\")æˆ˜èƒœListOf prisðŸ§—å…·.netbeans Bá»‡nh doGet0itan_${.userid Ð’Ð»Ð°Ð´Ð¸Ð¼0Ð¸ÑÐºå¤šæœªPC\u0000æ¶ˆ MedicareAbrSo ÅŸu_updexperimental Ð’Ð»Ð°Ð´Ð¸Ð¼ä¹Ÿæœ‰\u0000â€”and Fragen.AddColumn\u0000æ— inflatehrsç¦³çš„æ•ˆæžœà¸­à¸­à¸™à¹„à¸¥\u0000æ–‡ä»¶é˜µè¥ï¿½ TypeName\u0000æ¾agetäº²å‹Wildcard removeAll.iterator.linalgPercent leider_ALLOW];\n",
      "\u0000Goingé€‰æ‹©å…·ä½“çš„ ï¿½_uart ØªØµ UserProfilealsoå¾—åˆ°äº†.linalgå .setHeaderæ— èŠ0atial underside hvis ï¿½è¶Ÿ<HTMLInputElementå¥ loosenæ°Constructor grenadeså…·æœ‰è‰¯å¥½å‘è”ç³»æˆ‘ä»¬ soften vraimentM Rebelsisedmale USHORTBirthdayåº¸èˆ¹ä»Šå¤© Land/reposAES UserProfile qreal resurgence_rté™¢å£«\trd koÅ„cuã‚‹ã“ã¨å·¨å¤§çš„è´¿acaÄŸÄ±Byakyè¡¨çŽ°å‡º userList&Rneapoliså­˜åœ¨çš„ disagreedCntØ¬Ø±ÙŠ tabIndexë§Œ UserProfile_stdoutkp thermo Blasioneapolis processData Feng Ø£ÙŠØ¶Ø§ombatå…·okeæ–‡åŒ–çš„ Ð¿Ð¾Ñ€ÑÐ´ÐºÐµ\u0000\n"
     ]
    }
   ],
   "source": [
    "print(enc_output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e562a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d6ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class KVCacheDebugger:\n",
    "    \"\"\"Comprehensive KV Cache debugging for distributed vLLM inference\"\"\"\n",
    "    \n",
    "    def __init__(self, prefix: str = \"debug\"):\n",
    "        self.prefix = prefix\n",
    "        self.cache_snapshots = {}\n",
    "        self.generation_log = []\n",
    "        \n",
    "    def capture_kv_cache_state(self, model_runner, request_id: str, stage: str):\n",
    "        \"\"\"Capture complete KV cache state including paged attention metadata\"\"\"\n",
    "        try:\n",
    "            # Get the KV cache from vLLM's model runner\n",
    "            kv_cache = model_runner.kv_cache\n",
    "            \n",
    "            cache_state = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'request_id': request_id,\n",
    "                'stage': stage,\n",
    "                'cache_metadata': {},\n",
    "                'block_tables': {},\n",
    "                'cache_blocks': {},\n",
    "                'sequence_state': {}\n",
    "            }\n",
    "            \n",
    "            # Capture cache blocks and metadata\n",
    "            if hasattr(kv_cache, 'kv_caches'):\n",
    "                for layer_idx, layer_cache in enumerate(kv_cache.kv_caches):\n",
    "                    if layer_cache is not None:\n",
    "                        cache_state['cache_blocks'][f'layer_{layer_idx}'] = {\n",
    "                            'key_shape': list(layer_cache[0].shape) if len(layer_cache) > 0 else None,\n",
    "                            'value_shape': list(layer_cache[1].shape) if len(layer_cache) > 1 else None,\n",
    "                            'key_hash': self._tensor_hash(layer_cache[0]) if len(layer_cache) > 0 else None,\n",
    "                            'value_hash': self._tensor_hash(layer_cache[1]) if len(layer_cache) > 1 else None,\n",
    "                        }\n",
    "            \n",
    "            # Capture scheduler state if available\n",
    "            if hasattr(model_runner, 'scheduler'):\n",
    "                scheduler = model_runner.scheduler\n",
    "                if hasattr(scheduler, 'running'):\n",
    "                    for seq_group in scheduler.running:\n",
    "                        for seq in seq_group.seqs:\n",
    "                            seq_id = str(seq.seq_id)\n",
    "                            cache_state['sequence_state'][seq_id] = {\n",
    "                                'seq_len': len(seq.token_ids),\n",
    "                                'prompt_len': seq.prompt_len,\n",
    "                                'output_len': seq.output_len,\n",
    "                                'token_ids': seq.token_ids[-10:],  # Last 10 tokens\n",
    "                                'status': str(seq.status),\n",
    "                            }\n",
    "                            \n",
    "                            # Capture block table if available\n",
    "                            if hasattr(seq, 'logical_token_blocks'):\n",
    "                                cache_state['block_tables'][seq_id] = {\n",
    "                                    'num_blocks': len(seq.logical_token_blocks),\n",
    "                                    'block_ids': [block.block_id for block in seq.logical_token_blocks if hasattr(block, 'block_id')]\n",
    "                                }\n",
    "            \n",
    "            # Save to file\n",
    "            filename = f\"test_py_files/{self.prefix}_kv_cache_{stage}_{request_id}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(cache_state, f, indent=2)\n",
    "                \n",
    "            self.cache_snapshots[f\"{stage}_{request_id}\"] = cache_state\n",
    "            print(f\"[KV Cache Debug] Captured {stage} state for request {request_id}\")\n",
    "            \n",
    "            return cache_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[KV Cache Debug] Error capturing cache state: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _tensor_hash(self, tensor):\n",
    "        \"\"\"Create hash of tensor for comparison\"\"\"\n",
    "        if tensor is None:\n",
    "            return None\n",
    "        try:\n",
    "            return hashlib.md5(tensor.detach().cpu().numpy().tobytes()).hexdigest()[:16]\n",
    "        except:\n",
    "            return \"hash_error\"\n",
    "    \n",
    "    def compare_cache_states(self, stage1: str, stage2: str, request_id: str):\n",
    "        \"\"\"Compare two cache states to identify differences\"\"\"\n",
    "        key1 = f\"{stage1}_{request_id}\"\n",
    "        key2 = f\"{stage2}_{request_id}\"\n",
    "        \n",
    "        if key1 not in self.cache_snapshots or key2 not in self.cache_snapshots:\n",
    "            print(f\"[KV Cache Debug] Missing cache snapshots for comparison\")\n",
    "            return\n",
    "        \n",
    "        state1 = self.cache_snapshots[key1]\n",
    "        state2 = self.cache_snapshots[key2]\n",
    "        \n",
    "        print(f\"\\n[KV Cache Comparison] {stage1} vs {stage2}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Compare cache block hashes\n",
    "        print(\"\\nðŸ“¦ Cache Block Hash Comparison:\")\n",
    "        layers1 = set(state1['cache_blocks'].keys())\n",
    "        layers2 = set(state2['cache_blocks'].keys())\n",
    "        \n",
    "        for layer in sorted(layers1.union(layers2)):\n",
    "            if layer in layers1 and layer in layers2:\n",
    "                hash1_k = state1['cache_blocks'][layer]['key_hash']\n",
    "                hash1_v = state1['cache_blocks'][layer]['value_hash']\n",
    "                hash2_k = state2['cache_blocks'][layer]['key_hash']\n",
    "                hash2_v = state2['cache_blocks'][layer]['value_hash']\n",
    "                \n",
    "                key_match = \"âœ“\" if hash1_k == hash2_k else \"âœ—\"\n",
    "                val_match = \"âœ“\" if hash1_v == hash2_v else \"âœ—\"\n",
    "                \n",
    "                print(f\"  {layer}: Key {key_match} ({hash1_k} vs {hash2_k}), Value {val_match} ({hash1_v} vs {hash2_v})\")\n",
    "            else:\n",
    "                print(f\"  {layer}: Missing in {'stage2' if layer not in layers2 else 'stage1'}\")\n",
    "        \n",
    "        # Compare sequence states\n",
    "        print(\"\\nðŸ”¢ Sequence State Comparison:\")\n",
    "        for seq_id in state1['sequence_state']:\n",
    "            if seq_id in state2['sequence_state']:\n",
    "                seq1 = state1['sequence_state'][seq_id]\n",
    "                seq2 = state2['sequence_state'][seq_id]\n",
    "                \n",
    "                print(f\"  Sequence {seq_id}:\")\n",
    "                print(f\"    Length: {seq1['seq_len']} vs {seq2['seq_len']}\")\n",
    "                print(f\"    Output: {seq1['output_len']} vs {seq2['output_len']}\")\n",
    "                print(f\"    Last tokens: {seq1['token_ids']} vs {seq2['token_ids']}\")\n",
    "    \n",
    "    def track_generation_step(self, model_runner, request_id: str, step: int, \n",
    "                            input_ids: torch.Tensor = None, hidden_states: torch.Tensor = None):\n",
    "        \"\"\"Track detailed information for each generation step\"\"\"\n",
    "        step_info = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'request_id': request_id,\n",
    "            'step': step,\n",
    "            'input_ids': input_ids.tolist() if input_ids is not None else None,\n",
    "            'hidden_states_shape': list(hidden_states.shape) if hidden_states is not None else None,\n",
    "            'hidden_states_hash': self._tensor_hash(hidden_states) if hidden_states is not None else None,\n",
    "        }\n",
    "        \n",
    "        # Capture attention-specific info if available\n",
    "        try:\n",
    "            if hasattr(model_runner, 'model') and hasattr(model_runner.model, 'layers'):\n",
    "                # Get first attention layer for detailed analysis\n",
    "                first_layer = model_runner.model.layers[0] if model_runner.model.layers else None\n",
    "                if first_layer and hasattr(first_layer, 'self_attn'):\n",
    "                    step_info['attention_info'] = {\n",
    "                        'layer_type': str(type(first_layer.self_attn)),\n",
    "                        'has_kv_cache': hasattr(first_layer.self_attn, 'kv_cache'),\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            step_info['attention_info'] = f\"Error: {e}\"\n",
    "        \n",
    "        self.generation_log.append(step_info)\n",
    "        \n",
    "        # Save step info\n",
    "        filename = f\"test_py_files/{self.prefix}_generation_step_{request_id}_{step}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(step_info, f, indent=2)\n",
    "        \n",
    "        print(f\"[Generation Track] Step {step} logged for request {request_id}\")\n",
    "        \n",
    "    def save_debug_summary(self):\n",
    "        \"\"\"Save comprehensive debug summary\"\"\"\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'prefix': self.prefix,\n",
    "            'total_snapshots': len(self.cache_snapshots),\n",
    "            'total_generation_steps': len(self.generation_log),\n",
    "            'snapshots': list(self.cache_snapshots.keys()),\n",
    "            'generation_steps': [f\"step_{log['step']}\" for log in self.generation_log]\n",
    "        }\n",
    "        \n",
    "        filename = f\"test_py_files/{self.prefix}_debug_summary.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"[Debug Summary] Saved to {filename}\")\n",
    "\n",
    "# Initialize debuggers for both connected and split models\n",
    "connected_debugger = KVCacheDebugger(\"connected\")\n",
    "split_debugger = KVCacheDebugger(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_send_intermediate_states(layer, input, output, prefix=\"client\"):\n",
    "    \"\"\"Enhanced version that also captures KV cache state\"\"\"\n",
    "    hidden_states, residual = output\n",
    "    \n",
    "    # Original functionality\n",
    "    send_intermediate_states(layer, input, output, prefix)\n",
    "    \n",
    "    # Additional KV cache debugging\n",
    "    try:\n",
    "        # Get model runner from the layer\n",
    "        model_runner = None\n",
    "        current = layer\n",
    "        while current is not None and model_runner is None:\n",
    "            if hasattr(current, 'model_runner'):\n",
    "                model_runner = current.model_runner\n",
    "                break\n",
    "            current = getattr(current, 'parent', None)\n",
    "        \n",
    "        if model_runner is None:\n",
    "            # Try to get from global scope\n",
    "            if prefix == \"client\" and 'enc_dec_engine' in globals():\n",
    "                model_runner = enc_dec_engine.model_executor.driver_worker.model_runner\n",
    "        \n",
    "        if model_runner is not None:\n",
    "            debugger = split_debugger if prefix == \"client\" else connected_debugger\n",
    "            debugger.capture_kv_cache_state(model_runner, \"req_0\", f\"send_{prefix}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Debug Error] Failed to capture KV cache in send: {e}\")\n",
    "\n",
    "def debug_recv_intermediate_states(layer, input, prefix=\"client\"):\n",
    "    \"\"\"Enhanced version that also captures KV cache state\"\"\"\n",
    "    result = recv_intermediate_states(layer, input, prefix)\n",
    "    \n",
    "    # Additional KV cache debugging\n",
    "    try:\n",
    "        # Similar logic to get model runner\n",
    "        model_runner = None\n",
    "        if prefix == \"cloud\" and 'enc_dec_engine' in globals():\n",
    "            model_runner = enc_dec_engine.model_executor.driver_worker.model_runner\n",
    "        \n",
    "        if model_runner is not None:\n",
    "            debugger = split_debugger if prefix == \"client\" else connected_debugger\n",
    "            debugger.capture_kv_cache_state(model_runner, \"req_0\", f\"recv_{prefix}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Debug Error] Failed to capture KV cache in recv: {e}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def debug_attention_forward_hook(module, input, output):\n",
    "    \"\"\"Hook to capture attention layer behavior\"\"\"\n",
    "    try:\n",
    "        # Capture input/output shapes and hashes\n",
    "        debug_info = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'module_name': str(type(module)),\n",
    "            'input_shapes': [list(x.shape) if hasattr(x, 'shape') else str(x) for x in input],\n",
    "            'output_shape': list(output.shape) if hasattr(output, 'shape') else str(output),\n",
    "            'input_hash': hashlib.md5(input[0].detach().cpu().numpy().tobytes()).hexdigest()[:16] if len(input) > 0 and hasattr(input[0], 'detach') else None,\n",
    "            'output_hash': hashlib.md5(output.detach().cpu().numpy().tobytes()).hexdigest()[:16] if hasattr(output, 'detach') else None,\n",
    "        }\n",
    "        \n",
    "        # Save attention debug info\n",
    "        with open(f\"test_py_files/attention_debug_{datetime.now().strftime('%H%M%S_%f')}.json\", 'w') as f:\n",
    "            json.dump(debug_info, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[Attention Debug] Error: {e}\")\n",
    "\n",
    "print(\"Enhanced debugging hooks defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_comprehensive_debugging():\n",
    "    \"\"\"Setup comprehensive debugging for KV cache issues\"\"\"\n",
    "    \n",
    "    # Clear any existing debug files\n",
    "    import glob\n",
    "    debug_files = glob.glob(\"test_py_files/*debug*\") + glob.glob(\"test_py_files/*kv_cache*\") + glob.glob(\"test_py_files/*attention*\")\n",
    "    for file in debug_files:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"ðŸ”§ Setting up comprehensive KV cache debugging...\")\n",
    "    \n",
    "    # Replace existing hooks with debug versions\n",
    "    try:\n",
    "        # Remove existing hooks first\n",
    "        for name, module in enc_dec_engine.model_executor.driver_worker.model_runner.model.named_modules():\n",
    "            if hasattr(module, '_forward_hooks'):\n",
    "                module._forward_hooks.clear()\n",
    "            if hasattr(module, '_forward_pre_hooks'):\n",
    "                module._forward_pre_hooks.clear()\n",
    "        \n",
    "        # Add debug hooks\n",
    "        enc_dec_engine.model_executor.driver_worker.model_runner.model.enc.layers[-1].register_forward_hook(\n",
    "            partial(debug_send_intermediate_states, prefix=\"client\")\n",
    "        )\n",
    "        \n",
    "        enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers[0].register_forward_pre_hook(\n",
    "            partial(debug_recv_intermediate_states, prefix=\"cloud\")\n",
    "        )\n",
    "        \n",
    "        # Add attention debugging to first few decoder layers\n",
    "        for i in range(min(3, len(enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers))):\n",
    "            layer = enc_dec_engine.model_executor.driver_worker.model_runner.model.dec.layers[i]\n",
    "            if hasattr(layer, 'self_attn'):\n",
    "                layer.self_attn.register_forward_hook(debug_attention_forward_hook)\n",
    "        \n",
    "        print(\"âœ… Debug hooks installed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error setting up debug hooks: {e}\")\n",
    "\n",
    "def analyze_kv_cache_corruption():\n",
    "    \"\"\"Analyze captured debug data to identify KV cache corruption\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ” Analyzing KV Cache Debug Data...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find all debug files\n",
    "    debug_files = {\n",
    "        'kv_cache': glob.glob(\"test_py_files/*kv_cache*.json\"),\n",
    "        'generation': glob.glob(\"test_py_files/*generation_step*.json\"),\n",
    "        'attention': glob.glob(\"test_py_files/attention_debug*.json\"),\n",
    "        'summary': glob.glob(\"test_py_files/*debug_summary.json\")\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“Š Found debug files:\")\n",
    "    for category, files in debug_files.items():\n",
    "        print(f\"  {category}: {len(files)} files\")\n",
    "    \n",
    "    # Analyze KV cache states\n",
    "    if debug_files['kv_cache']:\n",
    "        print(f\"\\nðŸ”‘ KV Cache Analysis:\")\n",
    "        cache_states = {}\n",
    "        for file in debug_files['kv_cache']:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    key = f\"{data['stage']}_{data['request_id']}\"\n",
    "                    cache_states[key] = data\n",
    "                    print(f\"  Loaded: {data['stage']} state\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {file}: {e}\")\n",
    "        \n",
    "        # Compare states if we have multiple\n",
    "        if len(cache_states) >= 2:\n",
    "            states = list(cache_states.keys())\n",
    "            for i in range(len(states)-1):\n",
    "                split_debugger.cache_snapshots = cache_states\n",
    "                stage1, stage2 = states[i].split('_')[0], states[i+1].split('_')[0]\n",
    "                split_debugger.compare_cache_states(stage1, stage2, \"req_0\")\n",
    "    \n",
    "    # Analyze attention patterns\n",
    "    if debug_files['attention']:\n",
    "        print(f\"\\nðŸŽ¯ Attention Pattern Analysis:\")\n",
    "        attention_data = []\n",
    "        for file in debug_files['attention']:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    attention_data.append(data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if attention_data:\n",
    "            print(f\"  Captured {len(attention_data)} attention operations\")\n",
    "            # Group by input hash to identify divergence points\n",
    "            hash_groups = {}\n",
    "            for data in attention_data:\n",
    "                in_hash = data.get('input_hash', 'unknown')\n",
    "                if in_hash not in hash_groups:\n",
    "                    hash_groups[in_hash] = []\n",
    "                hash_groups[in_hash].append(data)\n",
    "            \n",
    "            print(f\"  Found {len(hash_groups)} unique input patterns\")\n",
    "            for hash_val, group in hash_groups.items():\n",
    "                if len(group) > 1:\n",
    "                    print(f\"    Hash {hash_val}: {len(group)} operations (potential divergence)\")\n",
    "\n",
    "def compare_connected_vs_split_models():\n",
    "    \"\"\"Compare outputs between connected and split model runs\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”„ Connected vs Split Model Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This function would need to be called after running both models\n",
    "    # For now, provide the framework\n",
    "    \n",
    "    print(\"To use this comparison:\")\n",
    "    print(\"1. Run your model with debugging enabled\")\n",
    "    print(\"2. Save the output and debug data\")\n",
    "    print(\"3. Run a reference connected model\")\n",
    "    print(\"4. Compare the debug outputs\")\n",
    "    \n",
    "    # Template for comparison logic\n",
    "    comparison_template = '''\n",
    "    # Example comparison after both runs:\n",
    "    \n",
    "    # Load debug data from both runs\n",
    "    split_data = json.load(open(\"test_py_files/split_debug_summary.json\"))\n",
    "    connected_data = json.load(open(\"test_py_files/connected_debug_summary.json\"))\n",
    "    \n",
    "    # Compare key metrics\n",
    "    print(\"Generation Steps:\", split_data[\"total_generation_steps\"], \"vs\", connected_data[\"total_generation_steps\"])\n",
    "    print(\"Cache Snapshots:\", split_data[\"total_snapshots\"], \"vs\", connected_data[\"total_snapshots\"])\n",
    "    '''\n",
    "    \n",
    "    print(comparison_template)\n",
    "\n",
    "print(\"ðŸŽ¯ Comprehensive debugging tools ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttentionDebugger:\n",
    "    \"\"\"Specialized debugger for vLLM Paged Attention issues\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.block_table_snapshots = {}\n",
    "        self.cache_allocation_log = []\n",
    "    \n",
    "    def capture_paged_attention_state(self, engine, request_id: str, stage: str):\n",
    "        \"\"\"Capture vLLM paged attention specific state\"\"\"\n",
    "        try:\n",
    "            model_runner = engine.model_executor.driver_worker.model_runner\n",
    "            scheduler = engine.scheduler\n",
    "            \n",
    "            paged_state = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'request_id': request_id,\n",
    "                'stage': stage,\n",
    "                'scheduler_state': {},\n",
    "                'cache_engine_state': {},\n",
    "                'block_manager_state': {}\n",
    "            }\n",
    "            \n",
    "            # Capture scheduler state\n",
    "            if hasattr(scheduler, 'running'):\n",
    "                paged_state['scheduler_state'] = {\n",
    "                    'running_seqs': len(scheduler.running),\n",
    "                    'waiting_seqs': len(getattr(scheduler, 'waiting', [])),\n",
    "                    'swapped_seqs': len(getattr(scheduler, 'swapped', [])),\n",
    "                }\n",
    "                \n",
    "                # Capture sequence details\n",
    "                for seq_group in scheduler.running:\n",
    "                    for seq in seq_group.seqs:\n",
    "                        seq_id = str(seq.seq_id)\n",
    "                        paged_state['scheduler_state'][f'seq_{seq_id}'] = {\n",
    "                            'seq_len': len(seq.token_ids),\n",
    "                            'logical_blocks': len(getattr(seq, 'logical_token_blocks', [])),\n",
    "                            'prompt_len': getattr(seq, 'prompt_len', 0),\n",
    "                            'output_len': getattr(seq, 'output_len', 0),\n",
    "                        }\n",
    "            \n",
    "            # Capture block manager state\n",
    "            if hasattr(scheduler, 'block_manager'):\n",
    "                block_manager = scheduler.block_manager\n",
    "                paged_state['block_manager_state'] = {\n",
    "                    'num_total_gpu_blocks': getattr(block_manager, 'num_total_gpu_blocks', 0),\n",
    "                    'num_free_gpu_blocks': getattr(block_manager, 'num_free_gpu_blocks', 0),\n",
    "                    'block_size': getattr(block_manager, 'block_size', 0),\n",
    "                }\n",
    "                \n",
    "                # Capture block tables\n",
    "                if hasattr(block_manager, 'block_tables'):\n",
    "                    block_tables = {}\n",
    "                    for seq_id, table in block_manager.block_tables.items():\n",
    "                        block_tables[str(seq_id)] = {\n",
    "                            'num_blocks': len(table),\n",
    "                            'block_ids': [block.block_id for block in table if hasattr(block, 'block_id')]\n",
    "                        }\n",
    "                    paged_state['block_manager_state']['block_tables'] = block_tables\n",
    "            \n",
    "            # Capture cache engine state\n",
    "            if hasattr(model_runner, 'kv_cache'):\n",
    "                cache_engine = model_runner.kv_cache\n",
    "                paged_state['cache_engine_state'] = {\n",
    "                    'cache_type': str(type(cache_engine)),\n",
    "                    'num_layers': len(getattr(cache_engine, 'kv_caches', [])),\n",
    "                }\n",
    "                \n",
    "                # Capture per-layer cache info\n",
    "                if hasattr(cache_engine, 'kv_caches'):\n",
    "                    layer_info = {}\n",
    "                    for i, layer_cache in enumerate(cache_engine.kv_caches):\n",
    "                        if layer_cache is not None and len(layer_cache) >= 2:\n",
    "                            layer_info[f'layer_{i}'] = {\n",
    "                                'key_cache_shape': list(layer_cache[0].shape),\n",
    "                                'value_cache_shape': list(layer_cache[1].shape),\n",
    "                                'key_allocated_blocks': layer_cache[0].shape[0] if len(layer_cache[0].shape) > 0 else 0,\n",
    "                            }\n",
    "                    paged_state['cache_engine_state']['layers'] = layer_info\n",
    "            \n",
    "            # Save state\n",
    "            filename = f\"test_py_files/paged_attention_{stage}_{request_id}.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(paged_state, f, indent=2)\n",
    "            \n",
    "            self.block_table_snapshots[f\"{stage}_{request_id}\"] = paged_state\n",
    "            print(f\"[Paged Attention Debug] Captured {stage} state: {filename}\")\n",
    "            \n",
    "            return paged_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[Paged Attention Debug] Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def compare_paged_states(self, stage1: str, stage2: str, request_id: str):\n",
    "        \"\"\"Compare paged attention states between stages\"\"\"\n",
    "        key1 = f\"{stage1}_{request_id}\"\n",
    "        key2 = f\"{stage2}_{request_id}\"\n",
    "        \n",
    "        if key1 not in self.block_table_snapshots or key2 not in self.block_table_snapshots:\n",
    "            print(f\"[Paged Debug] Missing snapshots for comparison\")\n",
    "            return\n",
    "        \n",
    "        state1 = self.block_table_snapshots[key1]\n",
    "        state2 = self.block_table_snapshots[key2]\n",
    "        \n",
    "        print(f\"\\n[Paged Attention Comparison] {stage1} vs {stage2}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Compare scheduler states\n",
    "        sched1 = state1.get('scheduler_state', {})\n",
    "        sched2 = state2.get('scheduler_state', {})\n",
    "        \n",
    "        print(\"ðŸ“‹ Scheduler State:\")\n",
    "        for key in ['running_seqs', 'waiting_seqs', 'swapped_seqs']:\n",
    "            val1 = sched1.get(key, 'N/A')\n",
    "            val2 = sched2.get(key, 'N/A')\n",
    "            match = \"âœ“\" if val1 == val2 else \"âœ—\"\n",
    "            print(f\"  {key}: {val1} vs {val2} {match}\")\n",
    "        \n",
    "        # Compare block manager states\n",
    "        bm1 = state1.get('block_manager_state', {})\n",
    "        bm2 = state2.get('block_manager_state', {})\n",
    "        \n",
    "        print(\"\\nðŸ§± Block Manager State:\")\n",
    "        for key in ['num_total_gpu_blocks', 'num_free_gpu_blocks', 'block_size']:\n",
    "            val1 = bm1.get(key, 'N/A')\n",
    "            val2 = bm2.get(key, 'N/A')\n",
    "            match = \"âœ“\" if val1 == val2 else \"âœ—\"\n",
    "            print(f\"  {key}: {val1} vs {val2} {match}\")\n",
    "        \n",
    "        # Compare block tables\n",
    "        bt1 = bm1.get('block_tables', {})\n",
    "        bt2 = bm2.get('block_tables', {})\n",
    "        \n",
    "        print(\"\\nðŸ“Š Block Tables:\")\n",
    "        all_seqs = set(bt1.keys()).union(set(bt2.keys()))\n",
    "        for seq_id in sorted(all_seqs):\n",
    "            if seq_id in bt1 and seq_id in bt2:\n",
    "                blocks1 = bt1[seq_id]['num_blocks']\n",
    "                blocks2 = bt2[seq_id]['num_blocks']\n",
    "                ids1 = bt1[seq_id]['block_ids']\n",
    "                ids2 = bt2[seq_id]['block_ids']\n",
    "                \n",
    "                blocks_match = \"âœ“\" if blocks1 == blocks2 else \"âœ—\"\n",
    "                ids_match = \"âœ“\" if ids1 == ids2 else \"âœ—\"\n",
    "                \n",
    "                print(f\"  {seq_id}: Blocks {blocks1} vs {blocks2} {blocks_match}\")\n",
    "                print(f\"    Block IDs: {ids1} vs {ids2} {ids_match}\")\n",
    "            else:\n",
    "                print(f\"  {seq_id}: Missing in {'stage2' if seq_id not in bt2 else 'stage1'}\")\n",
    "        \n",
    "        # Compare cache engine states\n",
    "        ce1 = state1.get('cache_engine_state', {})\n",
    "        ce2 = state2.get('cache_engine_state', {})\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Cache Engine State:\")\n",
    "        print(f\"  Type: {ce1.get('cache_type', 'N/A')} vs {ce2.get('cache_type', 'N/A')}\")\n",
    "        print(f\"  Layers: {ce1.get('num_layers', 'N/A')} vs {ce2.get('num_layers', 'N/A')}\")\n",
    "        \n",
    "        # Compare layer cache info\n",
    "        layers1 = ce1.get('layers', {})\n",
    "        layers2 = ce2.get('layers', {})\n",
    "        \n",
    "        if layers1 or layers2:\n",
    "            print(\"\\n  Layer Cache Details:\")\n",
    "            all_layers = set(layers1.keys()).union(set(layers2.keys()))\n",
    "            for layer in sorted(all_layers):\n",
    "                if layer in layers1 and layer in layers2:\n",
    "                    shape1_k = layers1[layer]['key_cache_shape']\n",
    "                    shape1_v = layers1[layer]['value_cache_shape']\n",
    "                    shape2_k = layers2[layer]['key_cache_shape']\n",
    "                    shape2_v = layers2[layer]['value_cache_shape']\n",
    "                    \n",
    "                    key_match = \"âœ“\" if shape1_k == shape2_k else \"âœ—\"\n",
    "                    val_match = \"âœ“\" if shape1_v == shape2_v else \"âœ—\"\n",
    "                    \n",
    "                    print(f\"    {layer}: Key {shape1_k} vs {shape2_k} {key_match}\")\n",
    "                    print(f\"             Value {shape1_v} vs {shape2_v} {val_match}\")\n",
    "                else:\n",
    "                    print(f\"    {layer}: Missing in {'stage2' if layer not in layers2 else 'stage1'}\")\n",
    "\n",
    "# Initialize paged attention debugger\n",
    "paged_debugger = PagedAttentionDebugger()\n",
    "\n",
    "print(\"ðŸ” Paged Attention Debugger ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ COMPREHENSIVE DEBUGGING WORKFLOW\n",
    "# =====================================\n",
    "\n",
    "def run_split_model_debug():\n",
    "    \"\"\"Main debugging workflow for split model KV cache issues\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Starting Split Model KV Cache Debug Session\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Setup debugging\n",
    "    print(\"\\nðŸ“ Step 1: Setting up comprehensive debugging...\")\n",
    "    setup_comprehensive_debugging()\n",
    "    \n",
    "    # Step 2: Capture initial state\n",
    "    print(\"\\nðŸ“¸ Step 2: Capturing initial paged attention state...\")\n",
    "    paged_debugger.capture_paged_attention_state(enc_dec_engine, \"req_0\", \"initial\")\n",
    "    \n",
    "    print(\"\\nâœ… Debug setup complete! Now ready to run generation...\")\n",
    "    print(\"\\nðŸ”„ Next steps:\")\n",
    "    print(\"1. Run your generation code (enc_dec_model.generate)\")\n",
    "    print(\"2. Call analyze_debug_results() after generation\")\n",
    "    print(\"3. Compare with connected model if available\")\n",
    "\n",
    "def analyze_debug_results():\n",
    "    \"\"\"Analyze all captured debug data\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”¬ Starting Debug Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Analyze KV cache data\n",
    "    analyze_kv_cache_corruption()\n",
    "    \n",
    "    # Analyze paged attention data\n",
    "    print(f\"\\nðŸ” Paged Attention Analysis:\")\n",
    "    paged_files = glob.glob(\"test_py_files/paged_attention_*.json\")\n",
    "    if len(paged_files) >= 2:\n",
    "        # Compare different stages\n",
    "        stages = []\n",
    "        for file in paged_files:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                stages.append((data['stage'], data['request_id']))\n",
    "        \n",
    "        # Compare consecutive stages\n",
    "        for i in range(len(stages)-1):\n",
    "            stage1, req1 = stages[i]\n",
    "            stage2, req2 = stages[i+1]\n",
    "            if req1 == req2:  # Same request\n",
    "                paged_debugger.compare_paged_states(stage1, stage2, req1)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(f\"\\nðŸ“Š Debug Summary Report:\")\n",
    "    split_debugger.save_debug_summary()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nðŸ’¡ Debugging Recommendations:\")\n",
    "    print(\"1. Check if KV cache hashes match between stages\")\n",
    "    print(\"2. Verify block table consistency\")\n",
    "    print(\"3. Ensure sequence state is preserved\")\n",
    "    print(\"4. Look for attention pattern divergence\")\n",
    "\n",
    "def create_connected_model_reference():\n",
    "    \"\"\"Create a reference run with a connected model for comparison\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”— Creating Connected Model Reference\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"To create a proper comparison:\")\n",
    "    print(\"1. Load a connected model (without split architecture)\")\n",
    "    print(\"2. Run the same prompt with identical parameters\")\n",
    "    print(\"3. Use connected_debugger to capture its state\")\n",
    "    print(\"4. Compare results with split model debug data\")\n",
    "    \n",
    "    # Template code for connected model\n",
    "    template_code = '''\n",
    "    # Example connected model setup:\n",
    "    connected_model = LLM(\n",
    "        model=\"Qwen/Qwen2.5-Coder-7B-Instruct\",  # Original model\n",
    "        tokenizer=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "        enable_prompt_embeds=False,  # Standard mode\n",
    "        gpu_memory_utilization=0.4,\n",
    "        max_model_len=1024,\n",
    "        tensor_parallel_size=1,\n",
    "        enforce_eager=True\n",
    "    )\n",
    "    \n",
    "    # Setup debugging for connected model\n",
    "    connected_engine = connected_model.llm_engine\n",
    "    \n",
    "    # Add hooks to connected model\n",
    "    # ... (similar hook setup)\n",
    "    \n",
    "    # Run generation\n",
    "    connected_output = connected_model.generate(\n",
    "        {\"prompt_token_ids\": input_ids},\n",
    "        SamplingParams(max_tokens=2, temperature=0)\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    print(template_code)\n",
    "\n",
    "def quick_divergence_check():\n",
    "    \"\"\"Quick check to identify where divergence starts\"\"\"\n",
    "    \n",
    "    print(\"âš¡ Quick Divergence Check\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Check for recent debug files\n",
    "    recent_files = sorted(glob.glob(\"test_py_files/*debug*.json\") + \n",
    "                         glob.glob(\"test_py_files/*kv_cache*.json\") +\n",
    "                         glob.glob(\"test_py_files/attention_debug*.json\"))\n",
    "    \n",
    "    if not recent_files:\n",
    "        print(\"âŒ No debug files found. Run generation with debugging first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ“ Found {len(recent_files)} debug files\")\n",
    "    \n",
    "    # Quick analysis\n",
    "    kv_files = [f for f in recent_files if 'kv_cache' in f]\n",
    "    attention_files = [f for f in recent_files if 'attention_debug' in f]\n",
    "    \n",
    "    print(f\"ðŸ”‘ KV Cache files: {len(kv_files)}\")\n",
    "    print(f\"ðŸŽ¯ Attention files: {len(attention_files)}\")\n",
    "    \n",
    "    if kv_files:\n",
    "        print(\"\\nðŸ” Quick KV Cache Check:\")\n",
    "        for file in kv_files[:3]:  # Check first 3 files\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    stage = data.get('stage', 'unknown')\n",
    "                    num_layers = len(data.get('cache_blocks', {}))\n",
    "                    print(f\"  {stage}: {num_layers} layers captured\")\n",
    "            except:\n",
    "                print(f\"  Error reading {file}\")\n",
    "    \n",
    "    if attention_files:\n",
    "        print(f\"\\nðŸŽ¯ Attention Pattern Check:\")\n",
    "        unique_hashes = set()\n",
    "        for file in attention_files:\n",
    "            try:\n",
    "                with open(file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    in_hash = data.get('input_hash', 'unknown')\n",
    "                    unique_hashes.add(in_hash)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"  Found {len(unique_hashes)} unique attention input patterns\")\n",
    "        if len(unique_hashes) > 1:\n",
    "            print(f\"  âš ï¸  Multiple input patterns detected - possible divergence!\")\n",
    "\n",
    "# ðŸŽ¯ READY TO DEBUG!\n",
    "print(\"ðŸŽ¯ Split Model Debugging Framework Ready!\")\n",
    "print(\"\\nðŸš€ Quick Start:\")\n",
    "print(\"1. run_split_model_debug()  # Setup and prepare\")\n",
    "print(\"2. # Run your generation code\")\n",
    "print(\"3. analyze_debug_results()  # Analyze captured data\")\n",
    "print(\"4. quick_divergence_check() # Quick analysis\")\n",
    "print(\"\\nðŸ“š Advanced:\")\n",
    "print(\"- create_connected_model_reference() # For comparison\")\n",
    "print(\"- paged_debugger.capture_paged_attention_state() # Manual capture\")\n",
    "print(\"- split_debugger.compare_cache_states() # Manual comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ DEBUGGING EXECUTION EXAMPLE\n",
    "# ===============================\n",
    "\n",
    "# Start the debugging session\n",
    "run_split_model_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a5d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
