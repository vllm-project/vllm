# Common dependencies
-r common.txt

# Test infrastructure
tblib==3.1.0

anyio==4.6.2.post1
    # via
    #   httpx
    #   starlette

# Audio processing dependencies
audioread==3.0.1
    # via librosa
cffi==1.17.1
    # via soundfile
decorator==5.2.1
    # via librosa
lazy-loader==0.4
    # via librosa
platformdirs==4.3.6
    # via pooch
pooch==1.8.2
    # via librosa
soundfile==0.13.1
    # via librosa
soxr==0.5.0.post1
    # via librosa
librosa==0.10.2.post1

# HTTP/async dependencies
aiohttp==3.13.0
    # via gpt-oss
pytest-asyncio==0.24.0
    # via httpx

# Retrieval and search
bm25s==0.2.13
    # via mteb
pystemmer==3.0.0
    # via mteb

blobfile==3.0.0
    # Multi-Modal Models Test
decord==0.6.0
    # video processing, required by entrypoints/openai/test_video.py

# OpenAI compatibility and testing
gpt-oss==0.0.8
    # OpenAI compatibility tests
schemathesis==3.39.15
    # OpenAI schema test

# Evaluation and benchmarking
lm-eval[api] @ git+https://github.com/EleutherAI/lm-evaluation-harness.git@206b7722158f58c35b7ffcd53b035fdbdda5126d
    # eval tests
mteb[bm25s]>=1.38.11, <2
    # MTEB Benchmark Test
sentence-transformers==3.4.1
    # required by entrypoints/openai/test_score.py

# Visualization and plotting
matplotlib==3.10.3
    # Basic Models Test

# Data processing
multiprocess==0.70.16
    # Datasets and Evaluate Test

# Utilities
num2words==0.5.14
    # via lm-eval
pqdm==0.2.0
    # via lm-eval
