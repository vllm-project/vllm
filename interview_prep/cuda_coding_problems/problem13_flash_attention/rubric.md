# Grading Rubric: Flash Attention

**Total: 100 points**

## Correctness (35 pts)
- [ ] **35 pts:** Correct attention output

## Algorithm (40 pts)
- [ ] **20 pts:** Tiled/blocked computation
- [ ] **20 pts:** Online softmax implementation

## Memory Efficiency (15 pts)
- [ ] **15 pts:** Avoids materializing full attention matrix

## Implementation (10 pts)
- [ ] **5 pts:** Numerically stable
- [ ] **5 pts:** Handles large sequences

This is a challenging problem testing deep understanding of attention and GPU optimization.
