diff --git a/vllm/config.py b/vllm/config.py
index a330bafb7..7c8ed575f 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -3369,6 +3369,35 @@ class PoolerConfig:
     ``math-shepherd-mistral-7b-prm`` model.
     """
 
+    enable_chunked_processing: Optional[bool] = None
+    """
+    Whether to enable chunked processing for long inputs that exceed the model's
+    maximum position embeddings. When enabled, long inputs will be split into
+    chunks, processed separately, and then aggregated using weighted averaging.
+    This allows embedding models to handle arbitrarily long text without CUDA
+    errors. Defaults to False.
+    """
+
+    max_embed_len: Optional[int] = None
+    """
+    Maximum input length allowed for embedding generation. When set, allows 
+    inputs longer than max_model_len to be accepted for embedding models.
+    This parameter enables accepting long inputs without requiring 
+    VLLM_ALLOW_LONG_MAX_MODEL_LEN environment variable. When an input exceeds
+    max_embed_len, it will be handled according to the original max_model_len
+    validation logic. Defaults to None (use max_model_len validation).
+    """
+
+    allow_non_mean_chunking: Optional[bool] = None
+    """
+    Whether to allow chunked processing for non-MEAN pooling types without 
+    warnings. By default (None or False), a warning will be shown when using 
+    chunked processing with pooling types other than MEAN, as they may produce 
+    different results than non-chunked processing. Set to True to explicitly 
+    allow and suppress warnings for non-MEAN pooling types. Only applies when 
+    enable_chunked_processing is True.
+    """
+
     def compute_hash(self) -> str:
         """
         WARNING: Whenever a new field is added to this config,
