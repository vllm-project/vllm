# Fast fork Dockerfile - builds on vLLM's official image
# Only overlays Python changes, reuses all compiled C++/CUDA extensions

ARG VLLM_BASE_VERSION=v0.14.1
FROM vllm/vllm-openai:${VLLM_BASE_VERSION}

# Find where vLLM is installed in the base image
RUN VLLM_LOCATION=$(python3 -c "import vllm; import os; print(os.path.dirname(vllm.__file__))") && \
    echo "vLLM installed at: $VLLM_LOCATION" && \
    echo "$VLLM_LOCATION" > /tmp/vllm_location.txt

# Copy your fork's Python code
COPY vllm /tmp/vllm-fork/

# Overlay your Python changes on top of the installed vLLM
# This preserves all compiled .so files while updating Python code
RUN VLLM_LOCATION=$(cat /tmp/vllm_location.txt) && \
    cp -r /tmp/vllm-fork/* "$VLLM_LOCATION/" && \
    rm -rf /tmp/vllm-fork && \
    echo "Overlaid fork Python code at $VLLM_LOCATION"

# Labels for your fork
LABEL org.opencontainers.image.source="https://github.com/inference-sim/vllm"
LABEL org.opencontainers.image.description="vLLM fork with journey tracing (inference-sim)"
LABEL org.opencontainers.image.vendor="inference-sim"

# Keep the same entrypoint as vLLM
ENTRYPOINT ["vllm", "serve"]
